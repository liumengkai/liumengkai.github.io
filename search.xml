<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Clickhouse 入门与实践</title>
    <url>/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)。</p>
<span id="more"></span>

<h1 id="OLAP和OLTP区别"><a href="#OLAP和OLTP区别" class="headerlink" title="OLAP和OLTP区别"></a>OLAP和OLTP区别</h1><p>上面那句话是来自ClickHouse中文官网</p>
<p>其中OLAP这个词熟悉又陌生，还有一个对应的叫做OLTP，我们简单介绍一下两者区别。</p>
<p>OLTP（on-line transaction processing）翻译为联机事务处理</p>
<p>OLAP（On-Line Analytical Processing）翻译为联机分析处理</p>
<p>OLTP主要对数据做增删改操作</p>
<p>OLAP主要对数据做查操作</p>
<p>OLTP用于在业务系统中，直接产生存储业务数据，如电子商务，银行，证券，一般数据量较小，要求操作实时性高</p>
<p>OLAP主要集中业务数据，进行统一的综合分析，分析的数据量一般巨大，不能做到很好地实时性</p>
<h2 id="OLAP分类"><a href="#OLAP分类" class="headerlink" title="OLAP分类"></a>OLAP分类</h2><p>OLAP还可以分为ROLAP（关系型联机分析处理）以及MOLAP（多维联机分析处理）</p>
<h3 id="ROLAP"><a href="#ROLAP" class="headerlink" title="ROLAP"></a>ROLAP</h3><p>完全基于关系模型进行存储，只是根据分析的需求对模型的结构和组织进行了优化，代表有MPP分布式数据库，以及基于Hadoop的Spark，Hive</p>
<p>因为ROLAP是实时来需求，实时进行计算，所以在数据量庞大的情况下速度会变得不能接受，而传统数据库无法支持大规模集群和PB级别数据量，所以这时候出现了MMP（大规模并行）数据库，这种数据库解决了一部分可扩展性问题，在支持的数据体量上有了很大的提升，但是在节点数量很大的时候就算再提升集群规模性能也不会有很大提升</p>
<p>基于Hadoop的Spark对硬件要求很低，但是计算量级达到一定程度无法秒级响应，并且因为是基于内存计算容易出现内存溢出等问题</p>
<h3 id="MOLAP"><a href="#MOLAP" class="headerlink" title="MOLAP"></a>MOLAP</h3><p>MOLAP理念就是既然计算能力不够能秒级返回结果，那我就按照一些维度先算好数据，到时候直接返回，优点就是同等资源下支持的数据体量更大，并发更多，但是当表的维度越多越复杂，需要的磁盘存储空间越大，构建cube也越复杂。常见的MOLAP服务器有SSAS，Kylin</p>
<p>Kylin则是目前技术较为先进的一款成熟产品，基于Hadoop框架，Cube以分片的形式存储在不同节点上，Cube大小不受服务器配置限制，所以具备很好的可扩展性和对服务器要求很低，在扩容成本上就非常低廉。另外为了控制整体Cube的大小，Kylin给客户提供了建模的能力，即用户可以根据自身需要，对模型种的维度以及维度组合进行预先的构建，把一些不需要的维度和组合筛选掉，从而达到降低维度的目的，减少磁盘空间的占用。</p>
<p>从可扩展性上看：</p>
<p>Kylin=Impala/Spark&gt;MPP数据库&gt;传统数据库；</p>
<p>从对硬件要求上看：</p>
<p>传统数据库&gt;MPP数据库&gt;Impala/Spark&gt;=Kylin；</p>
<p>从响应效率上来看：</p>
<p>不同的数据量、并发数，响应效率差别不一，但可以确定的是，要计算的数据量越大，并发的用户数越多，同等资源情况下预计算的响应效率会越发明显。</p>
<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/shujuku.jpg" class="" title="中二是最后的热血">

<h1 id="ClickHouse"><a href="#ClickHouse" class="headerlink" title="ClickHouse"></a>ClickHouse</h1><p>ClickHouse是一个用于联机分析(OLAP)的列式数据库管理系统(DBMS)</p>
<h2 id="安装与部署"><a href="#安装与部署" class="headerlink" title="安装与部署"></a>安装与部署</h2><p>系统是m1 Mac，目前还不能原生的支持ClickHouse，找了个centos 7的服务器，下面我们先来安装部署一下ClickHouse。</p>
<p>首先检查我们的系统是否支持</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">grep -q sse4_2 /proc/cpuinfo &amp;&amp; echo &quot;SSE 4.2 supported&quot; || echo &quot;SSE 4.2 not supported&quot;</span><br></pre></td></tr></table></figure>

<p>输出前面的结果：SSE 4.2 supported  即为支持</p>
<p>centos安装命令如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">首先我们添加安装源</span></span><br><span class="line">sudo yum install yum-utils</span><br><span class="line">sudo rpm --import https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG</span><br><span class="line">sudo yum-config-manager --add-repo https://repo.clickhouse.tech/rpm/stable/x86_64</span><br></pre></td></tr></table></figure>

<p>如果想使用最新的版本，请用<code>testing</code>替代<code>stable</code></p>
<p>运行命令安装：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo yum install clickhouse-server clickhouse-client</span><br></pre></td></tr></table></figure>

<p>启动（默认是./config.xml所以需要在相应目录下启动）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">sudo service clickhouse-server start</span><br><span class="line">或者使用</span><br><span class="line">systemctl start clickhouse-serve</span><br></pre></td></tr></table></figure>

<p>服务端日志文件路径:/var/log/clickhouse-server/</p>
<p>默认配置文件在：/etc/clickhouse-server/config.xml</p>
<p>运行的时候，配置文件会自动合并<code>/etc/clickhouse-server/config.d</code>目录下的xml文件</p>
<p>在控制台启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">实际上还有config.d目录下的xml配置文件，需要注意下</span></span><br><span class="line">clickhouse-server --config-file=/etc/clickhouse-server/config.xml</span><br></pre></td></tr></table></figure>

<h2 id="导入示例数据集"><a href="#导入示例数据集" class="headerlink" title="导入示例数据集"></a>导入示例数据集</h2><p>下面我们做一个官网的一个数据集测试，了解一下基础的如何导入数据以及查询</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">curl https://datasets.clickhouse.tech/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` &gt; hits_v1.tsv</span><br><span class="line">curl https://datasets.clickhouse.tech/visits/tsv/visits_v1.tsv.xz | unxz --threads=`nproc` &gt; visits_v1.tsv</span><br></pre></td></tr></table></figure>

<p>这两个文件数据量有10G左右，我们通过head命令提取部分数据即可</p>
<p>head -n 100000 hits_v1.tsv &gt; hits_v2.tsv</p>
<p>head visits_v1.tsv -n 50000 &gt; visits_v2.tsv</p>
<h3 id="准备库表"><a href="#准备库表" class="headerlink" title="准备库表"></a>准备库表</h3><p>登录clickhouse客户端</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        clickhouse-client快速提示
    </div>
    <div class='spoiler-content'>
        <p>交互模式:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">clickhouse-client</span><br><span class="line">clickhouse-client --host=... --port=... --user=... --password=...</span><br></pre></td></tr></table></figure>

<p>启用多行查询:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">clickhouse-client -m</span><br><span class="line">clickhouse-client --multiline</span><br></pre></td></tr></table></figure>

<p>以批处理模式运行查询:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">clickhouse-client --query=&#x27;SELECT 1&#x27;</span><br><span class="line">echo &#x27;SELECT 1&#x27; | clickhouse-client</span><br><span class="line">clickhouse-client &lt;&lt;&lt; &#x27;SELECT 1&#x27;</span><br></pre></td></tr></table></figure>

<p>从指定格式的文件中插入数据:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">clickhouse-client --query=&#x27;INSERT INTO table VALUES&#x27; &lt; data.txt</span><br><span class="line">clickhouse-client --query=&#x27;INSERT INTO table FORMAT TabSeparated&#x27; &lt; data.tsv </span><br></pre></td></tr></table></figure>

    </div>
</div>

<p>这里我们建立一个库名叫做adtiming</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">clickhouse<span class="operator">-</span>client <span class="comment">--query &quot;CREATE DATABASE IF NOT EXISTS adtiming&quot;</span></span><br></pre></td></tr></table></figure>

<p>建表语法要复杂的多，<a class="link"   href="https://clickhouse.tech/docs/zh/sql-reference/statements/create/" >建表文档<i class="fas fa-external-link-alt"></i></a></p>
<p>分别创建hits表以及visits表</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        建表语句
    </div>
    <div class='spoiler-content'>
        <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> adtiming.hits_v1</span><br><span class="line">(</span><br><span class="line">    `WatchID` UInt64,</span><br><span class="line">    `JavaEnable` UInt8,</span><br><span class="line">    `Title` String,</span><br><span class="line">    `GoodEvent` Int16,</span><br><span class="line">    `EventTime` DateTime,</span><br><span class="line">    `EventDate` <span class="type">Date</span>,</span><br><span class="line">    `CounterID` UInt32,</span><br><span class="line">    `ClientIP` UInt32,</span><br><span class="line">    `ClientIP6` FixedString(<span class="number">16</span>),</span><br><span class="line">    `RegionID` UInt32,</span><br><span class="line">    `UserID` UInt64,</span><br><span class="line">    `CounterClass` Int8,</span><br><span class="line">    `OS` UInt8,</span><br><span class="line">    `UserAgent` UInt8,</span><br><span class="line">    `URL` String,</span><br><span class="line">    `Referer` String,</span><br><span class="line">    `URLDomain` String,</span><br><span class="line">    `RefererDomain` String,</span><br><span class="line">    `Refresh` UInt8,</span><br><span class="line">    `IsRobot` UInt8,</span><br><span class="line">    `RefererCategories` <span class="keyword">Array</span>(UInt16),</span><br><span class="line">    `URLCategories` <span class="keyword">Array</span>(UInt16),</span><br><span class="line">    `URLRegions` <span class="keyword">Array</span>(UInt32),</span><br><span class="line">    `RefererRegions` <span class="keyword">Array</span>(UInt32),</span><br><span class="line">    `ResolutionWidth` UInt16,</span><br><span class="line">    `ResolutionHeight` UInt16,</span><br><span class="line">    `ResolutionDepth` UInt8,</span><br><span class="line">    `FlashMajor` UInt8,</span><br><span class="line">    `FlashMinor` UInt8,</span><br><span class="line">    `FlashMinor2` String,</span><br><span class="line">    `NetMajor` UInt8,</span><br><span class="line">    `NetMinor` UInt8,</span><br><span class="line">    `UserAgentMajor` UInt16,</span><br><span class="line">    `UserAgentMinor` FixedString(<span class="number">2</span>),</span><br><span class="line">    `CookieEnable` UInt8,</span><br><span class="line">    `JavascriptEnable` UInt8,</span><br><span class="line">    `IsMobile` UInt8,</span><br><span class="line">    `MobilePhone` UInt8,</span><br><span class="line">    `MobilePhoneModel` String,</span><br><span class="line">    `Params` String,</span><br><span class="line">    `IPNetworkID` UInt32,</span><br><span class="line">    `TraficSourceID` Int8,</span><br><span class="line">    `SearchEngineID` UInt16,</span><br><span class="line">    `SearchPhrase` String,</span><br><span class="line">    `AdvEngineID` UInt8,</span><br><span class="line">    `IsArtifical` UInt8,</span><br><span class="line">    `WindowClientWidth` UInt16,</span><br><span class="line">    `WindowClientHeight` UInt16,</span><br><span class="line">    `ClientTimeZone` Int16,</span><br><span class="line">    `ClientEventTime` DateTime,</span><br><span class="line">    `SilverlightVersion1` UInt8,</span><br><span class="line">    `SilverlightVersion2` UInt8,</span><br><span class="line">    `SilverlightVersion3` UInt32,</span><br><span class="line">    `SilverlightVersion4` UInt16,</span><br><span class="line">    `PageCharset` String,</span><br><span class="line">    `CodeVersion` UInt32,</span><br><span class="line">    `IsLink` UInt8,</span><br><span class="line">    `IsDownload` UInt8,</span><br><span class="line">    `IsNotBounce` UInt8,</span><br><span class="line">    `FUniqID` UInt64,</span><br><span class="line">    `HID` UInt32,</span><br><span class="line">    `IsOldCounter` UInt8,</span><br><span class="line">    `IsEvent` UInt8,</span><br><span class="line">    `IsParameter` UInt8,</span><br><span class="line">    `DontCountHits` UInt8,</span><br><span class="line">    `WithHash` UInt8,</span><br><span class="line">    `HitColor` FixedString(<span class="number">1</span>),</span><br><span class="line">    `UTCEventTime` DateTime,</span><br><span class="line">    `Age` UInt8,</span><br><span class="line">    `Sex` UInt8,</span><br><span class="line">    `Income` UInt8,</span><br><span class="line">    `Interests` UInt16,</span><br><span class="line">    `Robotness` UInt8,</span><br><span class="line">    `GeneralInterests` <span class="keyword">Array</span>(UInt16),</span><br><span class="line">    `RemoteIP` UInt32,</span><br><span class="line">    `RemoteIP6` FixedString(<span class="number">16</span>),</span><br><span class="line">    `WindowName` Int32,</span><br><span class="line">    `OpenerName` Int32,</span><br><span class="line">    `HistoryLength` Int16,</span><br><span class="line">    `BrowserLanguage` FixedString(<span class="number">2</span>),</span><br><span class="line">    `BrowserCountry` FixedString(<span class="number">2</span>),</span><br><span class="line">    `SocialNetwork` String,</span><br><span class="line">    `SocialAction` String,</span><br><span class="line">    `HTTPError` UInt16,</span><br><span class="line">    `SendTiming` Int32,</span><br><span class="line">    `DNSTiming` Int32,</span><br><span class="line">    `ConnectTiming` Int32,</span><br><span class="line">    `ResponseStartTiming` Int32,</span><br><span class="line">    `ResponseEndTiming` Int32,</span><br><span class="line">    `FetchTiming` Int32,</span><br><span class="line">    `RedirectTiming` Int32,</span><br><span class="line">    `DOMInteractiveTiming` Int32,</span><br><span class="line">    `DOMContentLoadedTiming` Int32,</span><br><span class="line">    `DOMCompleteTiming` Int32,</span><br><span class="line">    `LoadEventStartTiming` Int32,</span><br><span class="line">    `LoadEventEndTiming` Int32,</span><br><span class="line">    `NSToDOMContentLoadedTiming` Int32,</span><br><span class="line">    `FirstPaintTiming` Int32,</span><br><span class="line">    `RedirectCount` Int8,</span><br><span class="line">    `SocialSourceNetworkID` UInt8,</span><br><span class="line">    `SocialSourcePage` String,</span><br><span class="line">    `ParamPrice` Int64,</span><br><span class="line">    `ParamOrderID` String,</span><br><span class="line">    `ParamCurrency` FixedString(<span class="number">3</span>),</span><br><span class="line">    `ParamCurrencyID` UInt16,</span><br><span class="line">    `GoalsReached` <span class="keyword">Array</span>(UInt32),</span><br><span class="line">    `OpenstatServiceName` String,</span><br><span class="line">    `OpenstatCampaignID` String,</span><br><span class="line">    `OpenstatAdID` String,</span><br><span class="line">    `OpenstatSourceID` String,</span><br><span class="line">    `UTMSource` String,</span><br><span class="line">    `UTMMedium` String,</span><br><span class="line">    `UTMCampaign` String,</span><br><span class="line">    `UTMContent` String,</span><br><span class="line">    `UTMTerm` String,</span><br><span class="line">    `FromTag` String,</span><br><span class="line">    `HasGCLID` UInt8,</span><br><span class="line">    `RefererHash` UInt64,</span><br><span class="line">    `URLHash` UInt64,</span><br><span class="line">    `CLID` UInt32,</span><br><span class="line">    `YCLID` UInt64,</span><br><span class="line">    `ShareService` String,</span><br><span class="line">    `ShareURL` String,</span><br><span class="line">    `ShareTitle` String,</span><br><span class="line">    `ParsedParams` Nested(</span><br><span class="line">        Key1 String,</span><br><span class="line">        Key2 String,</span><br><span class="line">        Key3 String,</span><br><span class="line">        Key4 String,</span><br><span class="line">        Key5 String,</span><br><span class="line">        ValueDouble Float64),</span><br><span class="line">    `IslandID` FixedString(<span class="number">16</span>),</span><br><span class="line">    `RequestNum` UInt32,</span><br><span class="line">    `RequestTry` UInt8</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree()</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(EventDate)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (CounterID, EventDate, intHash32(UserID))</span><br><span class="line">SAMPLE <span class="keyword">BY</span> intHash32(UserID)</span><br></pre></td></tr></table></figure>



<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> adtiming.visits_v1</span><br><span class="line">(</span><br><span class="line">    `CounterID` UInt32,</span><br><span class="line">    `StartDate` <span class="type">Date</span>,</span><br><span class="line">    `Sign` Int8,</span><br><span class="line">    `IsNew` UInt8,</span><br><span class="line">    `VisitID` UInt64,</span><br><span class="line">    `UserID` UInt64,</span><br><span class="line">    `StartTime` DateTime,</span><br><span class="line">    `Duration` UInt32,</span><br><span class="line">    `UTCStartTime` DateTime,</span><br><span class="line">    `PageViews` Int32,</span><br><span class="line">    `Hits` Int32,</span><br><span class="line">    `IsBounce` UInt8,</span><br><span class="line">    `Referer` String,</span><br><span class="line">    `StartURL` String,</span><br><span class="line">    `RefererDomain` String,</span><br><span class="line">    `StartURLDomain` String,</span><br><span class="line">    `EndURL` String,</span><br><span class="line">    `LinkURL` String,</span><br><span class="line">    `IsDownload` UInt8,</span><br><span class="line">    `TraficSourceID` Int8,</span><br><span class="line">    `SearchEngineID` UInt16,</span><br><span class="line">    `SearchPhrase` String,</span><br><span class="line">    `AdvEngineID` UInt8,</span><br><span class="line">    `PlaceID` Int32,</span><br><span class="line">    `RefererCategories` <span class="keyword">Array</span>(UInt16),</span><br><span class="line">    `URLCategories` <span class="keyword">Array</span>(UInt16),</span><br><span class="line">    `URLRegions` <span class="keyword">Array</span>(UInt32),</span><br><span class="line">    `RefererRegions` <span class="keyword">Array</span>(UInt32),</span><br><span class="line">    `IsYandex` UInt8,</span><br><span class="line">    `GoalReachesDepth` Int32,</span><br><span class="line">    `GoalReachesURL` Int32,</span><br><span class="line">    `GoalReachesAny` Int32,</span><br><span class="line">    `SocialSourceNetworkID` UInt8,</span><br><span class="line">    `SocialSourcePage` String,</span><br><span class="line">    `MobilePhoneModel` String,</span><br><span class="line">    `ClientEventTime` DateTime,</span><br><span class="line">    `RegionID` UInt32,</span><br><span class="line">    `ClientIP` UInt32,</span><br><span class="line">    `ClientIP6` FixedString(<span class="number">16</span>),</span><br><span class="line">    `RemoteIP` UInt32,</span><br><span class="line">    `RemoteIP6` FixedString(<span class="number">16</span>),</span><br><span class="line">    `IPNetworkID` UInt32,</span><br><span class="line">    `SilverlightVersion3` UInt32,</span><br><span class="line">    `CodeVersion` UInt32,</span><br><span class="line">    `ResolutionWidth` UInt16,</span><br><span class="line">    `ResolutionHeight` UInt16,</span><br><span class="line">    `UserAgentMajor` UInt16,</span><br><span class="line">    `UserAgentMinor` UInt16,</span><br><span class="line">    `WindowClientWidth` UInt16,</span><br><span class="line">    `WindowClientHeight` UInt16,</span><br><span class="line">    `SilverlightVersion2` UInt8,</span><br><span class="line">    `SilverlightVersion4` UInt16,</span><br><span class="line">    `FlashVersion3` UInt16,</span><br><span class="line">    `FlashVersion4` UInt16,</span><br><span class="line">    `ClientTimeZone` Int16,</span><br><span class="line">    `OS` UInt8,</span><br><span class="line">    `UserAgent` UInt8,</span><br><span class="line">    `ResolutionDepth` UInt8,</span><br><span class="line">    `FlashMajor` UInt8,</span><br><span class="line">    `FlashMinor` UInt8,</span><br><span class="line">    `NetMajor` UInt8,</span><br><span class="line">    `NetMinor` UInt8,</span><br><span class="line">    `MobilePhone` UInt8,</span><br><span class="line">    `SilverlightVersion1` UInt8,</span><br><span class="line">    `Age` UInt8,</span><br><span class="line">    `Sex` UInt8,</span><br><span class="line">    `Income` UInt8,</span><br><span class="line">    `JavaEnable` UInt8,</span><br><span class="line">    `CookieEnable` UInt8,</span><br><span class="line">    `JavascriptEnable` UInt8,</span><br><span class="line">    `IsMobile` UInt8,</span><br><span class="line">    `BrowserLanguage` UInt16,</span><br><span class="line">    `BrowserCountry` UInt16,</span><br><span class="line">    `Interests` UInt16,</span><br><span class="line">    `Robotness` UInt8,</span><br><span class="line">    `GeneralInterests` <span class="keyword">Array</span>(UInt16),</span><br><span class="line">    `Params` <span class="keyword">Array</span>(String),</span><br><span class="line">    `Goals` Nested(</span><br><span class="line">        ID UInt32,</span><br><span class="line">        Serial UInt32,</span><br><span class="line">        EventTime DateTime,</span><br><span class="line">        Price Int64,</span><br><span class="line">        OrderID String,</span><br><span class="line">        CurrencyID UInt32),</span><br><span class="line">    `WatchIDs` <span class="keyword">Array</span>(UInt64),</span><br><span class="line">    `ParamSumPrice` Int64,</span><br><span class="line">    `ParamCurrency` FixedString(<span class="number">3</span>),</span><br><span class="line">    `ParamCurrencyID` UInt16,</span><br><span class="line">    `ClickLogID` UInt64,</span><br><span class="line">    `ClickEventID` Int32,</span><br><span class="line">    `ClickGoodEvent` Int32,</span><br><span class="line">    `ClickEventTime` DateTime,</span><br><span class="line">    `ClickPriorityID` Int32,</span><br><span class="line">    `ClickPhraseID` Int32,</span><br><span class="line">    `ClickPageID` Int32,</span><br><span class="line">    `ClickPlaceID` Int32,</span><br><span class="line">    `ClickTypeID` Int32,</span><br><span class="line">    `ClickResourceID` Int32,</span><br><span class="line">    `ClickCost` UInt32,</span><br><span class="line">    `ClickClientIP` UInt32,</span><br><span class="line">    `ClickDomainID` UInt32,</span><br><span class="line">    `ClickURL` String,</span><br><span class="line">    `ClickAttempt` UInt8,</span><br><span class="line">    `ClickOrderID` UInt32,</span><br><span class="line">    `ClickBannerID` UInt32,</span><br><span class="line">    `ClickMarketCategoryID` UInt32,</span><br><span class="line">    `ClickMarketPP` UInt32,</span><br><span class="line">    `ClickMarketCategoryName` String,</span><br><span class="line">    `ClickMarketPPName` String,</span><br><span class="line">    `ClickAWAPSCampaignName` String,</span><br><span class="line">    `ClickPageName` String,</span><br><span class="line">    `ClickTargetType` UInt16,</span><br><span class="line">    `ClickTargetPhraseID` UInt64,</span><br><span class="line">    `ClickContextType` UInt8,</span><br><span class="line">    `ClickSelectType` Int8,</span><br><span class="line">    `ClickOptions` String,</span><br><span class="line">    `ClickGroupBannerID` Int32,</span><br><span class="line">    `OpenstatServiceName` String,</span><br><span class="line">    `OpenstatCampaignID` String,</span><br><span class="line">    `OpenstatAdID` String,</span><br><span class="line">    `OpenstatSourceID` String,</span><br><span class="line">    `UTMSource` String,</span><br><span class="line">    `UTMMedium` String,</span><br><span class="line">    `UTMCampaign` String,</span><br><span class="line">    `UTMContent` String,</span><br><span class="line">    `UTMTerm` String,</span><br><span class="line">    `FromTag` String,</span><br><span class="line">    `HasGCLID` UInt8,</span><br><span class="line">    `FirstVisit` DateTime,</span><br><span class="line">    `PredLastVisit` <span class="type">Date</span>,</span><br><span class="line">    `LastVisit` <span class="type">Date</span>,</span><br><span class="line">    `TotalVisits` UInt32,</span><br><span class="line">    `TraficSource` Nested(</span><br><span class="line">        ID Int8,</span><br><span class="line">        SearchEngineID UInt16,</span><br><span class="line">        AdvEngineID UInt8,</span><br><span class="line">        PlaceID UInt16,</span><br><span class="line">        SocialSourceNetworkID UInt8,</span><br><span class="line">        Domain String,</span><br><span class="line">        SearchPhrase String,</span><br><span class="line">        SocialSourcePage String),</span><br><span class="line">    `Attendance` FixedString(<span class="number">16</span>),</span><br><span class="line">    `CLID` UInt32,</span><br><span class="line">    `YCLID` UInt64,</span><br><span class="line">    `NormalizedRefererHash` UInt64,</span><br><span class="line">    `SearchPhraseHash` UInt64,</span><br><span class="line">    `RefererDomainHash` UInt64,</span><br><span class="line">    `NormalizedStartURLHash` UInt64,</span><br><span class="line">    `StartURLDomainHash` UInt64,</span><br><span class="line">    `NormalizedEndURLHash` UInt64,</span><br><span class="line">    `TopLevelDomain` UInt64,</span><br><span class="line">    `URLScheme` UInt64,</span><br><span class="line">    `OpenstatServiceNameHash` UInt64,</span><br><span class="line">    `OpenstatCampaignIDHash` UInt64,</span><br><span class="line">    `OpenstatAdIDHash` UInt64,</span><br><span class="line">    `OpenstatSourceIDHash` UInt64,</span><br><span class="line">    `UTMSourceHash` UInt64,</span><br><span class="line">    `UTMMediumHash` UInt64,</span><br><span class="line">    `UTMCampaignHash` UInt64,</span><br><span class="line">    `UTMContentHash` UInt64,</span><br><span class="line">    `UTMTermHash` UInt64,</span><br><span class="line">    `FromHash` UInt64,</span><br><span class="line">    `WebVisorEnabled` UInt8,</span><br><span class="line">    `WebVisorActivity` UInt32,</span><br><span class="line">    `ParsedParams` Nested(</span><br><span class="line">        Key1 String,</span><br><span class="line">        Key2 String,</span><br><span class="line">        Key3 String,</span><br><span class="line">        Key4 String,</span><br><span class="line">        Key5 String,</span><br><span class="line">        ValueDouble Float64),</span><br><span class="line">    `Market` Nested(</span><br><span class="line">        Type UInt8,</span><br><span class="line">        GoalID UInt32,</span><br><span class="line">        OrderID String,</span><br><span class="line">        OrderPrice Int64,</span><br><span class="line">        PP UInt32,</span><br><span class="line">        DirectPlaceID UInt32,</span><br><span class="line">        DirectOrderID UInt32,</span><br><span class="line">        DirectBannerID UInt32,</span><br><span class="line">        GoodID String,</span><br><span class="line">        GoodName String,</span><br><span class="line">        GoodQuantity Int32,</span><br><span class="line">        GoodPrice Int64),</span><br><span class="line">    `IslandID` FixedString(<span class="number">16</span>)</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> CollapsingMergeTree(Sign)</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(StartDate)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> (CounterID, StartDate, intHash32(UserID), VisitID)</span><br><span class="line">SAMPLE <span class="keyword">BY</span> intHash32(UserID)</span><br></pre></td></tr></table></figure>

    </div>
</div>

<p>我们可以看到两张表分别使用了不同的表引擎，其中 <code>hits_v1</code>使用 <a class="link"   href="https://clickhouse.tech/docs/zh/engines/table-engines/mergetree-family/mergetree/" >MergeTree引擎<i class="fas fa-external-link-alt"></i></a>，而<code>visits_v1</code>使用 <a class="link"   href="https://clickhouse.tech/docs/zh/engines/table-engines/mergetree-family/collapsingmergetree/" >Collapsing<i class="fas fa-external-link-alt"></i></a>引擎</p>
<p>对于我们需要不同功能的表或者不同使用途径的表Clickhouse具有不同的表引擎来应对，<a class="link"   href="https://developer.aliyun.com/article/762461" >表引擎文档<i class="fas fa-external-link-alt"></i></a> <a class="link"   href="https://clickhouse.tech/docs/zh/engines/table-engines/mergetree-family/mergetree/" >表引擎官方文档<i class="fas fa-external-link-alt"></i></a></p>
<p><code>MergeTree</code> 系列的引擎被设计用于插入极大量的数据到一张表当中。数据可以以数据片段的形式一个接着一个的快速写入，数据片段在后台按照一定的规则进行合并。相比在插入时不断修改（重写）已存储的数据，这种策略会高效很多。所以相当于这种表结构适用于插入操作较多，并且一次插入数据较大的情况。</p>
<p><code>CollapsingMergeTree</code> 会异步的删除（折叠） <code>Sign</code> 有 <code>1</code> 和 <code>-1</code> 且其余所有字段的值都相等的成对的行。没有成对的行会被保留。</p>
<p>每个引擎实际上就代表一种需求，比如CollapsingMergeTree就是去重作用。</p>
<h3 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">clickhouse-client --query &quot;INSERT INTO adtiming.hits_v1 FORMAT TSV&quot; --max_insert_block_size=100000 &lt; hits_v2.tsv</span><br><span class="line">clickhouse-client --query &quot;INSERT INTO adtiming.visits_v1 FORMAT TSV&quot; --max_insert_block_size=100000 &lt; visits_v2.tsv</span><br></pre></td></tr></table></figure>

<p>检验插入是否成功(如果直接用大数据量文件可能会导致插入失败)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">clickhouse-client --query &quot;SELECT COUNT(*) FROM adtiming.hits_v1&quot;</span><br><span class="line">clickhouse-client --query &quot;SELECT COUNT(*) FROM adtiming.visits_v1&quot;</span><br></pre></td></tr></table></figure>

<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/jiancha.png" class="" title="中二是最后的热血">

<h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><p>下面对数据进行简单的查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    StartURL <span class="keyword">AS</span> URL,</span><br><span class="line">    <span class="built_in">AVG</span>(Duration) <span class="keyword">AS</span> AvgDuration</span><br><span class="line"><span class="keyword">FROM</span> tutorial.visits_v1</span><br><span class="line"><span class="keyword">WHERE</span> StartDate <span class="keyword">BETWEEN</span> <span class="string">&#x27;2014-03-23&#x27;</span> <span class="keyword">AND</span> <span class="string">&#x27;2014-03-30&#x27;</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> URL</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> AvgDuration <span class="keyword">DESC</span></span><br><span class="line">LIMIT <span class="number">10</span></span><br></pre></td></tr></table></figure>



<p>查询结果如下图所示</p>
<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/chaxunjieguo.png" class="" title="中二是最后的热血">

<h2 id="简单思考"><a href="#简单思考" class="headerlink" title="简单思考"></a>简单思考</h2><p>官网实例这个sql第一眼看上去感觉有点怪，因为group by后面的字段是别名字段URL，这个在Hive或者Mysql中应该都是不可以的，group by后面直接使用别名是会报错的，我们可以从SQL的执行顺序出发来看这个问题。</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        执行顺序
    </div>
    <div class='spoiler-content'>
        <p>Mysql的实行顺序：</p>
<p>(1)    FROM <left_table></p>
<p>(2)    ON <join_condition></p>
<p>(3)    <join_type> JOIN <right_table></p>
<p>(4)    WHERE <where_condition></p>
<p>(5)    GROUP BY <group_by_list></p>
<p>(6)    HAVING <having_condition></p>
<p>(7)    SELECT</p>
<p>(8)    DISTINCT <select_list></p>
<p>(9)    ORDER BY <order_by_condition></p>
<p>(10)   LIMIT <limit_number> </p>
<p>Hive执行顺序</p>
<p><code>from … on … join … where … group by … having …</code></p>
<p><code>select … distinct … order by … limit</code></p>
<p>上面展示了Hive以及Mysql中都是group by在select后面，所以group by不能使用别名来操作，但是order by是可以直接使用别名的，说明ClickHouse的SQL执行和常见的数据库不太一样，具体差别都有什么我们后面学习到了再来总结。</p>

    </div>
</div>

<h2 id="ClickHouse表引擎"><a href="#ClickHouse表引擎" class="headerlink" title="ClickHouse表引擎"></a>ClickHouse表引擎</h2><p>首先我们了解到ClickHouse的核心引擎是MergeTree系列引擎，因为MT系列引擎支持了像数据按照主键排序，指定分区键实现分区，数据副本，数据采样等功能，这是其他像Log，Integration引擎所不具备的功能，所以我们先学习MergeTree系列引擎，也是最难的。</p>
<p>MergeTree系列引擎种类繁多，有以下MergeTree、ReplacingMergeTree、SummingMergeTree、AggregatingMergeTree、CollapsingMergeTree、VersionedCollapsingMergeTree、GraphiteMergeTree 等 7 种不同类型的 MergeTree 引擎，以及其对应支持副本的Replicated*系列引擎</p>
<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/clickhouse_yinqing.png" class="" title="中二是最后的热血">

<p>下面我们介绍每一种引擎的业务场景</p>
<p><strong>ReplacingMergeTree</strong>： 在后台数据合并期间，对同一个分区内具有相同 <code>排序键</code> 的数据进行去重操作，<code>ReplacingMergeTree</code> 适用于在后台清除重复的数据以节省空间，但是它只能保证同一个分区内没有重复的数据出现</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr2],</span><br><span class="line">    ...</span><br><span class="line">) ENGINE <span class="operator">=</span> ReplacingMergeTree([ver])</span><br><span class="line">[<span class="keyword">PARTITION</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[SAMPLE <span class="keyword">BY</span> expr]</span><br><span class="line">[SETTINGS name<span class="operator">=</span><span class="keyword">value</span>, ...]</span><br></pre></td></tr></table></figure>

<p>ENGINE = ReplacingMergeTree([ver])，其中ver为选填参数，它需要指定一个UInt8/UInt16、Date或DateTime类型的字段，它决定了数据去重时所用的算法，如果没有设置该参数，合并时保留分组内的最后一条数据；如果指定了该参数，则保留ver字段取值最大的那一行。</p>
<p><strong>SummingMergeTree</strong>： 当合并数据时，会把同一个分区内的具有相同主键的记录合并为一条记录。根据聚合字段设置，该字段的值为聚合后的汇总值，非聚合字段使用第一条记录的值，聚合字段类型必须为数值类型。如果没有指定聚合字段，则会按照<code>非主键的数值类型字段</code>进行聚合</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr2],</span><br><span class="line">    ...</span><br><span class="line">) ENGINE <span class="operator">=</span> SummingMergeTree([columns])</span><br><span class="line">[<span class="keyword">PARTITION</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[SAMPLE <span class="keyword">BY</span> expr]</span><br><span class="line">[SETTINGS name<span class="operator">=</span><span class="keyword">value</span>, ...]</span><br></pre></td></tr></table></figure>

<p>SummingMergeTree表引擎依据ORDER BY指定的字段进行聚合，PRIMARY KEY指定主键，但是ORDER BY可以指代主键，一般只声明ORDER BY即可，表数据会按照orderby的维度进行排序，而索引数据会按照主键进行排序生成，索引的顺序必须和表数据对应上，那么含义就是order by的维度中主键必须是最左前缀，下面举一个例子。</p>
<p>假设我们有col1,col2,col3三列，业务上我们只需要对col1进行查询过滤，就是我们只需要col1当做主键，然后根据col1数据以及index_granularity（索引间隔）参数生成我们的索引文件，为了保证数据也是索引的顺序，那么我们应该写order by(col1,col2).</p>
<p>这样带来的好处还有就是主键和维度分开，我们以后可以根据业务情况修改维度</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> test_summing MODIFY <span class="keyword">ORDER</span> <span class="keyword">BY</span> (col1,col2,col3);</span><br></pre></td></tr></table></figure>

<p><strong>AggregatingMergeTree</strong>： 在同一数据分区下，可以将具有相同主键的数据进行聚合。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr2],</span><br><span class="line">    ...</span><br><span class="line">) ENGINE <span class="operator">=</span> AggregatingMergeTree()</span><br><span class="line">[<span class="keyword">PARTITION</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[SAMPLE <span class="keyword">BY</span> expr]</span><br><span class="line">[TTL expr]</span><br><span class="line">[SETTINGS name<span class="operator">=</span><span class="keyword">value</span>, ...]</span><br></pre></td></tr></table></figure>

<p>与<code>SummingMergeTree</code>的区别在于：<code>SummingMergeTree</code>对非主键列进行<code>sum</code>聚合，而<code>AggregatingMergeTree</code>则可以指定各种聚合函数。对于<code>AggregateFunction</code>类型的列字段，在进行数据的写入和查询时与其他的表引擎有很大区别，在写入数据时，需使用带有 -State- 聚合函数的 INSERT SELECT语句；而在查询数据时，则需要调用相应的-Merge函数。</p>
<p>一般AggregatingMergeTree都是需要和普通的MergeTree进行合用，MergeTree作为底表，防止因为聚合函数错误导致数据丢失，而AggregatingMergeTree作为<code>物化视图</code>，相当于在底表上进行聚合查询</p>
<p><strong>CollapsingMergeTree</strong>： 在同一数据分区下，对具有相同主键的数据进行折叠合并。</p>
<p>建表语法：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr2],</span><br><span class="line">    ...</span><br><span class="line">) ENGINE <span class="operator">=</span> VersionedCollapsingMergeTree(sign, version)</span><br><span class="line">[<span class="keyword">PARTITION</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[SAMPLE <span class="keyword">BY</span> expr]</span><br><span class="line">[SETTINGS name<span class="operator">=</span><span class="keyword">value</span>, ...]</span><br></pre></td></tr></table></figure>

<p>CollapsingMergeTree同样也根据order by字段进行唯一性判定，实际上这种引擎是根据以赠代删的思路，支持行级别修改和删除，通过定义一个sign标记为字段，记录数据行状态，如果sign标记为1，则表示这是一行有效数据，如果sign为-1表示这行数据需要被删除，当分区合并的时候，同一个分区内sign标记为1和-1的一组数据会被抵消删除掉。</p>
<p>分区数据合并折叠不是实时操作，需要在后台实行Compaction操作，用户可以进行手动操作，但是在生产环境中不适合使用效率很低，一般建议group by完毕之和进行having count。eg:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> emp_id,name,<span class="built_in">sum</span>(salary <span class="operator">*</span> sign)<span class="keyword">FROM</span> emp_collapsingmergetree <span class="keyword">GROUP</span> <span class="keyword">BY</span> emp_id, name <span class="keyword">HAVING</span> <span class="built_in">sum</span>(sign) <span class="operator">&gt;</span> <span class="number">0</span>;</span><br><span class="line">┌─emp_id─┬─name─┬─<span class="built_in">sum</span>(multiply(salary, sign))─┐</span><br><span class="line">│      <span class="number">1</span> │ tom  │                    <span class="number">30000.00</span> │</span><br><span class="line">└────────┴──────┴─────────────────────────────┘</span><br></pre></td></tr></table></figure>

<p>上面我们知道了CollapsingMergeTree的合并原理，不难想象，这种合并对写入顺序有着严格要求，否则就会导致数据无法正常折叠，比如我们先写入的是sign=-1的数据，然后再写入sign=1的数据，这时候我们合并分区之和再进行排查依旧是会出现两条数据。</p>
<p>所以这种表结构只适合单线程写入，可以很好地控制顺序写入，但是如果是多线程的状态就不能很好地控制数据写入顺序了，这时候就需要另一种表结构了，就是下面我们要提到的VersionedCollapsingMergeTree</p>
<p><strong>VersionedCollapsingMergeTree</strong>：</p>
<p>基于 CollapsingMergeTree 引擎，增添了数据版本信息字段(version)配置选项。在数据依据 ORDER BY 设置对数据进行排序的基础上，如果数据的版本信息列不在排序字段中，那么版本信息会被隐式的作为 ORDER BY 的最后一列从而影响数据排序。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db.]table_name [<span class="keyword">ON</span> CLUSTER cluster]</span><br><span class="line">(</span><br><span class="line">    name1 [type1] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr1],</span><br><span class="line">    name2 [type2] [<span class="keyword">DEFAULT</span><span class="operator">|</span>MATERIALIZED<span class="operator">|</span>ALIAS expr2],</span><br><span class="line">    ...</span><br><span class="line">) ENGINE <span class="operator">=</span> VersionedCollapsingMergeTree(sign, version)</span><br><span class="line">[<span class="keyword">PARTITION</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[<span class="keyword">ORDER</span> <span class="keyword">BY</span> expr]</span><br><span class="line">[SAMPLE <span class="keyword">BY</span> expr]</span><br><span class="line">[SETTINGS name<span class="operator">=</span><span class="keyword">value</span>, ...]</span><br></pre></td></tr></table></figure>

<p>version字段会自动的添加到order by末尾，我们在插入数据的之和只要保证同一条数据的version大小，就相当于可以保证它的顺序关系。</p>
<p><strong>GraphiteMergeTree</strong>： 用来存储时序数据库 Graphites 的数据。</p>
<p>ClickHouse表引擎种类繁多，其中每一种适合的业务场景都不相同，详细可以参考<a class="link"   href="https://clickhouse.tech/docs/zh/engines/table-engines/mergetree-family/collapsingmergetree/" >官方文档<i class="fas fa-external-link-alt"></i></a> 或者一些<a class="link"   href="https://developer.aliyun.com/article/762461" >三方文档<i class="fas fa-external-link-alt"></i></a>，</p>
<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/yinqing.png" class="" title="中二是最后的热血">



<h2 id="搭建ClickHouse集群"><a href="#搭建ClickHouse集群" class="headerlink" title="搭建ClickHouse集群"></a>搭建ClickHouse集群</h2><p>搭建ClickHouse需要依赖Zookeeper集群，这里我们只在一台机器上搭建Zookeeper，给出Zookeeper，ClickHouse，Kafka版本作参考</p>
<table>
<thead>
<tr>
<th>Zookeeper</th>
<th>ClickHouse</th>
</tr>
</thead>
<tbody><tr>
<td>zookeeper-3.4.10</td>
<td>21.8.4.51(21.8.4.51)</td>
</tr>
</tbody></table>
<p>启动Zookeeper并查看状态</p>
<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/zookeeper.png" class="">

<p>接下来需要在ClickHouse中配置集群信息，在配置之前我们需要明白以下几个关键信息</p>
<p>1.主要读取的配置文件是/etc/clickhouse-server/config.xml</p>
<p>2.ClickHouse会将/etc/clickhouse-server/conf.d/metrika.xml配置文件和上面提到的配置文件合并，但是为了保险，建议还是执行第二条</p>
<p>3.通用的配置信息可以写到metrika.xml配置文件中，如果没有的话需要创建一个，然后在config.xml中引入</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!--引入方法--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">include_from</span>&gt;</span>/etc/clickhouse-server/conf.d/metrika.xml<span class="tag">&lt;/<span class="name">include_from</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>4.如果上面的方法没有生效，直接修改config.xml文件</p>
<p>5.要清楚你在修改配置文件的哪些地方，我们不管是直接修改config.xml还是写一个metrika.xml文件都是为了修改<remote_servers>,<zookeeper>等几个关键信息，要确保我们的修改成功生效</p>
<p>了解上面几点之和我们开始配置ClickHouse集群：</p>
<ul>
<li><p>修改/etc/clickhouse-server/config.xml</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 如果禁用了ipv6，使用下面配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">listen_host</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">listen_host</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 如果没有禁用ipv6，使用下面配置，我使用的下面的配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">listen_host</span>&gt;</span>::<span class="tag">&lt;/<span class="name">listen_host</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>这个配置是为了我们能正常的登录clickhouse-client，服务器使用了ipv6但是我不小心用了上面的<listen_host>，登录的时候报错 9000 connection refused，最终定位到了这里。</p>
</li>
<li><p>创建/etc/clickhouse-server/conf.d/metrika.xml文件</p>
<p>根据集群信息，将下面的xml改写成自己集群使用的配置文件，放到对应目录下，需要注意一下几点：</p>
<p>（1）下面xml信息中的集群名称叫做‘doit_ch_cluster1’，可以随意修改，但是要记住名称</p>
<p>（2）<shard>代表分片，<replica>代表副本，一个分片可能有多个副本，集群表也可以有多个分片</p>
<p>（3）下面配置文件集群信息是<clickhouse_remote_servers>标签，注意config.xml中的集群信息叫做什么，一般需要在config.xml中进行一行配置 <code>&lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt;</code>，意思相当于<remote_servers>标签内容取自<clickhouse_remote_servers>，其他标签也是一样的，虽然听起来多此一举，但是可以将通用的配置信息摘取出来，之和直接scp给各个服务器就行，但是我两台机器中的一台这样配置一直没有生效。。。没错两台中的一台没有生效，另一台生效了，就很无语，只能将其中的配置直接修改到config.xml中，也算是解决了</p>
<p>（4）macros配置，这个配置创建数据副本表的时候路径可以直接使用宏替换替换，因为我们创建集群表的时候需要保证不同的表以及不同副本在Zookeeper中的路径不同，其中<code>&lt;shard&gt;</code>标签代表的是我们想让哪几台机器组成一个分片集合，同一个分片的分片名称是一样的比如都是01，或者02，而<code>&lt;replica&gt;</code>标签代表的是本台机器的创建的副本的名称标识，副本是即使是同一个分片的副本名称也不能一样，所以这个副本名称一般采用主机名或者ip，这是需要注意的地方，这块理解起来需要结合官方文档](<a class="link"   href="https://clickhouse.tech/docs/zh/engines/table-engines/mergetree-family/replication/" >https://clickhouse.tech/docs/zh/engines/table-engines/mergetree-family/replication/<i class="fas fa-external-link-alt"></i></a>)</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">yandex</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- /etc/clickhouse-server/config.xml 中配置的remote_servers的incl属性值，--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 集群名称，可以修改 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">doit_ch_cluster1</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 配置三个分片，每个分片对应一台机器，为每个分片配置一个副本 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">             <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>master<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>slaver1<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shard</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">internal_replication</span>&gt;</span>true<span class="tag">&lt;/<span class="name">internal_replication</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">replica</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">host</span>&gt;</span>slaver2<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">port</span>&gt;</span>9000<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">shard</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">doit_ch_cluster1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_remote_servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- zookeeper相关配置 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 该标签与config.xml的&lt;zookeeper incl=&quot;zookeeper-servers&quot; optional=&quot;true&quot; /&gt; 保持一致 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;1&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>master<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;2&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>slaver1<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">node</span> <span class="attr">index</span>=<span class="string">&quot;3&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">host</span>&gt;</span>slaver2<span class="tag">&lt;/<span class="name">host</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">port</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">port</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">node</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">zookeeper-servers</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 分片和副本标识，shard标签配置分片编号，&lt;replica&gt;配置分片副本主机名，需要修改对应主机上的配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">macros</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">replica</span>&gt;</span>doit01<span class="tag">&lt;/<span class="name">replica</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">macros</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">networks</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">ip</span>&gt;</span>::/0<span class="tag">&lt;/<span class="name">ip</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">networks</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">case</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">min_part_size</span>&gt;</span>10000000000<span class="tag">&lt;/<span class="name">min_part_size</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">min_part_size_ratio</span>&gt;</span>0.01<span class="tag">&lt;/<span class="name">min_part_size_ratio</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">method</span>&gt;</span>lz4<span class="tag">&lt;/<span class="name">method</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">clickhouse_compression</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">yandex</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>启动ClickHouse集群</p>
<p>我们在ClickHouse机器上都配置完成之后，确保我们的Zookeeper是开启状态，然后重启ClickHouse集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl restart clickhouse-server</span><br></pre></td></tr></table></figure>

<p>确保所有的Clickhouse状态都正常，如果出现问题就去查看一下/var/log/clickhouse/中的err日志</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">systemctl status clickhouse-server</span><br></pre></td></tr></table></figure>

<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/clickhouse.png" class="">

<p>确认所有正常之后我们登录任意一台clickhouse客户端，查看clickhouse集群以及Zookeeper信息，需要注意的是，system.zookeeper表只有在关于Zookeeper配置生效之后才会出现</p>
<img src="/2021/08/26/Clickhouse-%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/jiqunxinxi.png" class=""></li>
</ul>
<h2 id="创建Distribute-Local表"><a href="#创建Distribute-Local表" class="headerlink" title="创建Distribute+Local表"></a>创建Distribute+Local表</h2><p>Distribute是Clickhouse比较特殊的一种表引擎，是ClickHouse分布式表的代言词，distribute表引擎创建的表不存储数据，它起到一个路由的作用，将数据路由到集群机器上的本地表。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Distributed(cluster_name, database_name, table_name[, sharding_key])</span><br></pre></td></tr></table></figure>

<p>cluster_name：集群名称，与集群配置中的自定义名称相对应。<br>database_name：数据库名称。<br>table_name：表名称，需要集群中的机器上都存在这个表<br>sharding_key：可选的，用于分片的key值，在数据写入的过程中，分布式表会依据分片key的规则，将数据分布到各个节点的本地表，这个参数可选</p>
<ul>
<li><p>建立distribute表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> user_cluster <span class="keyword">ON</span> CLUSTER doit_ch_cluster1</span><br><span class="line">(</span><br><span class="line">    id Int32,</span><br><span class="line">    name String</span><br><span class="line">)ENGINE <span class="operator">=</span> Distributed(doit_ch_cluster1, <span class="keyword">default</span>, user_local,id);</span><br><span class="line"></span><br><span class="line"><span class="comment">--上面建表语句代表了数据是要分发到doit_ch_cluster1集群中default.user_local表，根据id决定分发到哪台机器上</span></span><br></pre></td></tr></table></figure>

<p>这里我们为了简单，就只设计两个表字段，其中id是我们用来将数据分到集群机器上的sharding_key，我们也可以用rand()函数让ck随机的决定将数据发送到哪台机器</p>
</li>
<li><p>建立本地表</p>
<p>得益于我们的集群配置，我们可以使用<code>ON CLUSTER doit_ch_cluster1</code>语法自动在集群中建表，这里我们使用正常的查询引擎MergeTree()</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> user_local <span class="keyword">ON</span> CLUSTER doit_ch_cluster1</span><br><span class="line">(</span><br><span class="line">    `id` Int32,</span><br><span class="line">    `name` String</span><br><span class="line">)</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree()</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> id</span><br><span class="line"><span class="keyword">PRIMARY</span> KEY id</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> id</span><br></pre></td></tr></table></figure></li>
</ul>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>go语言笔记</title>
    <url>/2022/06/02/Go%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>go语言学习笔记</p>
<span id="more"></span>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>因为我是Mac本，所以在安装go以及beego上吃了不少亏，网上大多都是针对Windows或者Linux来安装，这里总结一下教训，如果你非Mac可以忽略这一点。</p>
<p><strong>注意：</strong></p>
<ol>
<li><p>Mac本安装go的时候可以用<a class="link"   href="https://go.dev/dl/" >官网下载<i class="fas fa-external-link-alt"></i></a>安装包的方式，注意m1的mac需要选arm架构的下载包，但是这种方式我在安装beego的时候出现问题，执行完下面的安装命令之后我的$GOPATH/bin下面<strong>不会出现</strong>bee可执行文件，相当于显示安装成功但是没有结果，经过许久探索无果，遂决定更换homebrew安装go &amp; beego。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> install beego &amp;&amp; bee</span></span><br><span class="line">go get github.com/astaxie/beego</span><br><span class="line">go get github.com/beego/bee</span><br></pre></td></tr></table></figure></li>
<li><p>改用homebrew安装go</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> install go with home brew</span></span><br><span class="line">brew install go</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到go安装路径</span></span><br><span class="line">brew list go</span><br><span class="line"></span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/bin/go</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/bin/gofmt</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/api/ (21 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/bin/ (2 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/doc/ (4 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/lib/ (3 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/misc/ (371 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/pkg/ (669 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/src/ (7192 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/test/ (2539 files)</span><br><span class="line">/opt/homebrew/Cellar/go/1.17.2/libexec/ (6 files)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置go相关的环境变量 修改你所使用的bash</span></span><br><span class="line">vim ~/.zshrc</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在末尾添加go环境变量 注意把路径修改为你的go安装路径</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> GOROOT</span></span><br><span class="line">export GOROOT=/opt/homebrew/Cellar/go/1.17.2/libexec</span><br><span class="line"><span class="meta">#</span><span class="bash"> GOPATH</span></span><br><span class="line">export GOPATH=$HOME/go</span><br><span class="line"><span class="meta">#</span><span class="bash"> Bin</span></span><br><span class="line">export PATH=$&#123;PATH&#125;:$GOPATH/bin</span><br></pre></td></tr></table></figure>

<p>其中的 Bin(export PATH=${PATH}:$GOPATH/bin) 就是我们安装beego会在下面生成bee可执行文件</p>
</li>
<li><p>安装beego</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 安装完成之后bee可执行文件生成在<span class="variable">$GOPATH</span>/bin下</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果执行失败试试先执行 go env -w GO111MODULE=on 打开go module在安装</span></span><br><span class="line">go get github.com/beego/bee</span><br></pre></td></tr></table></figure>

<p>这次安装完成之后会在GOPATH/bin下出现bee可执行命令，并且终端直接输入bee也会输出bee相关信息</p>
<img src="/2022/06/02/Go%E8%AF%AD%E8%A8%80%E7%AC%94%E8%AE%B0/bee.png" class="" title="中二是最后的热血"></li>
<li><p>目前go推荐使用go module这个自带的go依赖管理库，配置GO111MODULE=true打开，默认就是打开，关于go module的详细资料可以<a class="link"   href="https://www.cnblogs.com/wongbingming/p/12941021.html" >看这里<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">go env -w GO111MODULE=on</span><br></pre></td></tr></table></figure></li>
<li><p>创建项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建项目</span></span><br><span class="line">bee new hello</span><br><span class="line">cd $GOPATH/src/hello/</span><br><span class="line">go get hello</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 运行项目</span></span><br><span class="line">cd $GOPATH/src/hello/</span><br><span class="line">bee run</span><br></pre></td></tr></table></figure></li>
</ol>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop提交任务到Yarn流程源码学习分析</title>
    <url>/2022/10/02/Hadoop%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%88%B0Yarn%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>Talk is cheap show me the code!</p>
<span id="more"></span>

<h1 id="Hadoop-向-yarn-提交任务整体流程"><a href="#Hadoop-向-yarn-提交任务整体流程" class="headerlink" title="Hadoop 向 yarn 提交任务整体流程"></a>Hadoop 向 yarn 提交任务整体流程</h1><p>总体流程：</p>
<p>1）作业提交</p>
<p>第 1 步：Client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业。</p>
<p>第 2 步：Client 向 RM 申请一个作业 id。</p>
<p>第 3 步：RM 给 Client 返回该 job 资源的提交路径和作业 id。</p>
<p>第 4 步：Client 提交 jar 包、切片信息和配置文件到指定的资源提交路径。</p>
<p>第 5 步：Client 提交完资源后，向 RM 申请运行 MrAppMaster。</p>
<p>2）作业初始化</p>
<p>第 6 步：当 RM 收到 Client 的请求后，将该 job 添加到容量调度器中。</p>
<p>第 7 步：某一个空闲的 NM 领取到该 Job。</p>
<p>第 8 步：该 NM 创建 Container，并产生 MRAppmaster。</p>
<p>第 9 步：下载Client 提交的资源到本地。</p>
<p>3）任务分配</p>
<p>第 10 步：MrAppMaster 向 RM 申请运行多个 MapTask 任务资源。</p>
<p>第 11 步：RM 将运行 MapTask 任务分配给另外两个NodeManager，另两个 NodeManager分别领取任务并</p>
<p>创建容器。</p>
<p>4）任务运行</p>
<p>第 12 步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动</p>
<p>MapTask，MapTask 对数据分区排序。</p>
<p>第13 步：MrAppMaster 等待所有MapTask 运行完毕后，向RM 申请容器，运行ReduceTask。</p>
<p>第 14 步：ReduceTask 向 MapTask 获取相应分区的数据。</p>
<p>第 15 步：程序运行完毕后，MR 会向 RM 申请注销自己。</p>
<h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><p>代码中的整体流程:</p>
<p>下面涉及到的类/接口之间关系</p>
<img src="/2022/10/02/Hadoop%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%88%B0Yarn%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/class.png" class="" title="中二是最后的热血">

<p>Job.waitForCompletion() -&gt; Job.submit()  -&gt; Job.connect() -&gt; Cluster.Cluster() -&gt; Cluster.initialize() -&gt; YarnClientProtocolProvider.create() -&gt; JobSubmitter.sbumitJobInternal() -&gt; YARNRunner.submitJob() -&gt; ResourceMgrDelegate.submitApplication() -&gt; YarnClientImpl.submitApplication() -&gt; ApplicationClientProtocolPBClientImpl.submitApplication() -&gt; ApplicationClientProtocolPBServiceImpl.submitApplication() -&gt; ClientRMService.submitApplication() -&gt; RMAppManager.submitApplication()</p>
<p>(1)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 首先调用waitForCompletion启动任务</span></span><br><span class="line">JobClient.java:</span><br><span class="line"></span><br><span class="line">JobClient.waitForCompletion()</span><br></pre></td></tr></table></figure>

<p>(2)</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// waitForCompletion方法中调用submit</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">waitForCompletion</span><span class="params">(<span class="keyword">boolean</span> verbose</span></span></span><br><span class="line"><span class="params"><span class="function">                                   )</span> <span class="keyword">throws</span> IOException, InterruptedException,</span></span><br><span class="line"><span class="function">                                            ClassNotFoundException </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (state == JobState.DEFINE) &#123;</span><br><span class="line">      submit();</span><br><span class="line">...</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Submit the job to the cluster and return immediately.</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">submit</span><span class="params">()</span> </span></span><br><span class="line"><span class="function">         <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException </span>&#123;</span><br><span class="line">    </span><br><span class="line">		ensureState(JobState.DEFINE);</span><br><span class="line">    setUseNewAPI();</span><br><span class="line"><span class="comment">// 这里的connector会 确认 存在cluster对象，而cluster对象的主要作用是:Provides a way to access information about the map/reduce cluster.是我们访问map/reduce集群的主要途径</span></span><br><span class="line"><span class="comment">// 在Cluster.initialze()方法中根据providerList我们创建对应的cluster对象，mapreduce.framework.name参数来确定你的Job采用哪类应用，有Local和Yarn两种模式，Yarn任务对应的YarnClientProtocolProvider，各自的应用只需要实现相应的接口就可以把自己的Job运行在Yarn上，在YarnClientProtocolProvider.create()方法中创建了resMgrDelegate(ResourceMgrDelegate) 在创建resMgrDelegate的时候会创建其类变量client也就是YarnClientImpl，而我们最终提交任务就是通过YarnClientImpl.submitApplication()方法提交。</span></span><br><span class="line">    connect();</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建submitter对象 然后由submitClient对象调用submitJobInternal</span></span><br><span class="line">    <span class="keyword">final</span> JobSubmitter submitter = </span><br><span class="line">        getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">    status = ugi.doAs(<span class="keyword">new</span> PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">      </span><br><span class="line">      <span class="function"><span class="keyword">public</span> JobStatus <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException,ClassNotFoundException </span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">          <span class="comment">//submitJobInternal这个方法需要注意</span></span><br><span class="line">        <span class="keyword">return</span> submitter.submitJobInternal(Job.<span class="keyword">this</span>, cluster);</span><br><span class="line">      &#125;</span><br><span class="line">      </span><br><span class="line">    &#125;);</span><br><span class="line">    state = JobState.RUNNING;</span><br><span class="line">    LOG.info(<span class="string">&quot;The url to track the job: &quot;</span> + getTrackingURL());</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// submitJobInternal方法中:</span></span><br><span class="line"><span class="comment">// 设置jobid,client应该上传资源的路径，队列等信息</span></span><br><span class="line"><span class="comment">// copyAndConfigureFiles方法 将jar包上传到集群中 </span></span><br><span class="line"><span class="comment">// writeSplits 将切片信息上传</span></span><br><span class="line"><span class="comment">// writeConf 将配置文件上传</span></span><br><span class="line"><span class="comment">// submitClient.submitJob 申请向resourcemanager中提交一个任务，这是一个接口方法，具体由实现类中重写的方法来实现，</span></span><br><span class="line"><span class="comment">// 我们向Yarn提交任务所以用的是YarnRunner中的submitJob 除此之外还有LocalJobRunner</span></span><br><span class="line"><span class="function">JobStatus <span class="title">submitJobInternal</span><span class="params">(Job job, Cluster cluster)</span>  <span class="keyword">throws</span> ClassNotFoundException, </span></span><br><span class="line"><span class="function">InterruptedException, IOException </span>&#123;</span><br><span class="line">...</span><br><span class="line">  <span class="comment">//获取jobid</span></span><br><span class="line">  JobID jobId = submitClient.getNewJobID();</span><br><span class="line">  job.setJobID(jobId);</span><br><span class="line">  Path submitJobDir = <span class="keyword">new</span> Path(jobStagingArea, jobId.toString());</span><br><span class="line">  JobStatus status = <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">// 获取job资源应该提交的路径</span></span><br><span class="line">    conf.set(MRJobConfig.USER_NAME,</span><br><span class="line">        UserGroupInformation.getCurrentUser().getShortUserName());</span><br><span class="line">    conf.set(<span class="string">&quot;hadoop.http.filter.initializers&quot;</span>, </span><br><span class="line">        <span class="string">&quot;org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer&quot;</span>);</span><br><span class="line">    conf.set(MRJobConfig.MAPREDUCE_JOB_DIR, submitJobDir.toString());</span><br><span class="line">    LOG.debug(<span class="string">&quot;Configuring job &quot;</span> + jobId + <span class="string">&quot; with &quot;</span> + submitJobDir </span><br><span class="line">        + <span class="string">&quot; as the submit dir&quot;</span>);</span><br><span class="line">...</span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="comment">// Create the splits for the job</span></span><br><span class="line">LOG.debug(<span class="string">&quot;Creating splits at &quot;</span> + jtFs.makeQualified(submitJobDir));</span><br><span class="line"><span class="keyword">int</span> maps = writeSplits(job, submitJobDir);</span><br><span class="line">conf.setInt(MRJobConfig.NUM_MAPS, maps);</span><br><span class="line">LOG.info(<span class="string">&quot;number of splits:&quot;</span> + maps);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">// 提交任务 这里submitClient是ClientProtocol接口中的方法,ClientProtocol接口类实现有LocalJobRunner和YARNRunner，毫无疑问我们这里调用的是YARNRunner中的submitJob方法</span></span><br><span class="line"></span><br><span class="line">status = submitClient.submitJob(</span><br><span class="line">      jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line">  <span class="keyword">if</span> (status != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Could not launch job&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">  <span class="keyword">if</span> (status == <span class="keyword">null</span>) &#123;</span><br><span class="line">    LOG.info(<span class="string">&quot;Cleaning up the staging area &quot;</span> + submitJobDir);</span><br><span class="line">    <span class="keyword">if</span> (jtFs != <span class="keyword">null</span> &amp;&amp; submitJobDir != <span class="keyword">null</span>)</span><br><span class="line">      jtFs.delete(submitJobDir, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Cluster.java中 初始化方法</span></span><br><span class="line"><span class="comment">// 这里初始化集群方法用到的ClientProtocolProvider接口中的实现类YarnClientProtocolProvider中的create方法</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InetSocketAddress jobTrackAddr, Configuration conf)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  initProviderList();</span><br><span class="line">  <span class="keyword">final</span> IOException initEx = <span class="keyword">new</span> IOException(</span><br><span class="line">      <span class="string">&quot;Cannot initialize Cluster. Please check your configuration for &quot;</span></span><br><span class="line">          + MRConfig.FRAMEWORK_NAME</span><br><span class="line">          + <span class="string">&quot; and the correspond server addresses.&quot;</span>);</span><br><span class="line">  <span class="keyword">if</span> (jobTrackAddr != <span class="keyword">null</span>) &#123;</span><br><span class="line">    LOG.info(</span><br><span class="line">        <span class="string">&quot;Initializing cluster for Job Tracker=&quot;</span> + jobTrackAddr.toString());</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// 接口 ClientProtocolProvider ClientProtocol是客户端通信协议提供者 这里我们是和Yarn通信所以使用的是YarnClientProtocolProvider</span></span><br><span class="line">  <span class="keyword">for</span> (ClientProtocolProvider provider : providerList) &#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Trying ClientProtocolProvider : &quot;</span></span><br><span class="line">        + provider.getClass().getName());</span><br><span class="line">    ClientProtocol clientProtocol = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (jobTrackAddr == <span class="keyword">null</span>) &#123;</span><br><span class="line"><span class="comment">// 这里是YarnClientProtocolProvider中的create方法，注意create方法中创建了ResourceMgrDelegate，而ResourceMgrDelegate是抽象类YarnClient的实现类，也是我们与yarn RM的主要通信途径实现类。</span></span><br><span class="line"><span class="comment">// 其中ResourceMgrDelegate的类变量client是YarnClientImpl，后面提交任务还是会用到其中的submitApplication方法</span></span><br><span class="line"><span class="comment">// YARNRunner：This class enables the current JobClient (0.22 hadoop) to run on YARN.</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">public ResourceMgrDelegate(YarnConfiguration conf) &#123;</span></span><br><span class="line"><span class="comment">    super(ResourceMgrDelegate.class.getName());</span></span><br><span class="line"><span class="comment">    this.conf = conf;</span></span><br><span class="line"><span class="comment">    this.client = YarnClient.createYarnClient(); //这里的clinet是YarnClientImpl</span></span><br><span class="line"><span class="comment">    init(conf);</span></span><br><span class="line"><span class="comment">    start();</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* YarnClientProtocolProvider.create()方法</span></span><br><span class="line"><span class="comment">public ClientProtocol create(Configuration conf) throws IOException &#123;</span></span><br><span class="line"><span class="comment">    if (MRConfig.YARN_FRAMEWORK_NAME.equals(conf.get(MRConfig.FRAMEWORK_NAME))) &#123;</span></span><br><span class="line"><span class="comment">      return new YARNRunner(conf);</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">    return null;*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">YARNRunner.YARNRunner()构造方法</span></span><br><span class="line"><span class="comment">public YARNRunner(Configuration conf, ResourceMgrDelegate resMgrDelegate,</span></span><br><span class="line"><span class="comment">      ClientCache clientCache) &#123;</span></span><br><span class="line"><span class="comment">    this.conf = conf;</span></span><br><span class="line"><span class="comment">    try &#123;</span></span><br><span class="line"><span class="comment">      this.resMgrDelegate = resMgrDelegate; // resMgrDelegate 是RM通信主要途径</span></span><br><span class="line"><span class="comment">      this.clientCache = clientCache;</span></span><br><span class="line"><span class="comment">      this.defaultFileContext = FileContext.getFileContext(this.conf);</span></span><br><span class="line"><span class="comment">    &#125; catch (UnsupportedFileSystemException ufe) &#123;</span></span><br><span class="line"><span class="comment">      throw new RuntimeException(&quot;Error in instantiating YarnClient&quot;, ufe);</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">  &#125;*/</span></span><br><span class="line">        clientProtocol = provider.create(conf);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        clientProtocol = provider.create(jobTrackAddr, conf);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (clientProtocol != <span class="keyword">null</span>) &#123;</span><br><span class="line">        clientProtocolProvider = provider;</span><br><span class="line">        client = clientProtocol;</span><br><span class="line">        LOG.debug(<span class="string">&quot;Picked &quot;</span> + provider.getClass().getName()</span><br><span class="line">            + <span class="string">&quot; as the ClientProtocolProvider&quot;</span>);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        LOG.debug(<span class="string">&quot;Cannot pick &quot;</span> + provider.getClass().getName()</span><br><span class="line">            + <span class="string">&quot; as the ClientProtocolProvider - returned null protocol&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">      <span class="keyword">final</span> String errMsg = <span class="string">&quot;Failed to use &quot;</span> + provider.getClass().getName()</span><br><span class="line">          + <span class="string">&quot; due to error: &quot;</span>;</span><br><span class="line">      initEx.addSuppressed(<span class="keyword">new</span> IOException(errMsg, e));</span><br><span class="line">      LOG.info(errMsg, e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">null</span> == clientProtocolProvider || <span class="keyword">null</span> == client) &#123;</span><br><span class="line">    <span class="keyword">throw</span> initEx;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Client提交完资源之后需要向RM申请MRAppMaster 这里是YarnRunner中submitJob方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> JobStatus <span class="title">submitJob</span><span class="params">(JobID jobId, String jobSubmitDir, Credentials ts)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">  </span><br><span class="line">  addHistoryToken(ts);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// createApplicationSubmissionContext 准备好所有创建一个MR AM所需的配置等信息，eg:创建applicationId，Setup ContainerLaunchContext for AM container等操作</span></span><br><span class="line">  ApplicationSubmissionContext appContext =</span><br><span class="line">    createApplicationSubmissionContext(conf, jobSubmitDir, ts);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Submit to ResourceManager</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">//resMgrDelegate是ResourceMgrDelegate类对象 他负责和RM服务通信，继续调用他submitApplication方法，传递前面配置好的任务上下文信息，submitApplication这个方法底层调用的是client.submitApplication(appContext)方法，client是YarnClient抽象类，提交任务到Yarn会用到它的实现类YarnClientImpl.submitApplication()方法</span></span><br><span class="line">    ApplicationId applicationId =</span><br><span class="line">        resMgrDelegate.submitApplication(appContext);</span><br><span class="line"></span><br><span class="line">    ApplicationReport appMaster = resMgrDelegate</span><br><span class="line">        .getApplicationReport(applicationId);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// YarnClientImpl.java:submitApplication():</span></span><br><span class="line"><span class="comment">// request.setApplicationSubmissionContext(appContext) 将任务上下文信息传入到SubmitApplicationRequest request()中</span></span><br><span class="line"><span class="comment">// rmClient.submitApplication(request) 提交任务</span></span><br><span class="line"><span class="comment">// rmClient是接口ApplicationClientProtocol，submitApplication是该接口中的方法，ApplicationClientProtocol协议负责的就是Client和ResourceManager的交互逻辑。主要功能是submit/abort jobs（提交/终止任务）和get information from applications（获取应用信息）以及get cluster metrics（获取集群指标）</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ApplicationId</span></span><br><span class="line"><span class="function">    <span class="title">submitApplication</span><span class="params">(ApplicationSubmissionContext appContext)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> YarnException, IOException </span>&#123;</span><br><span class="line">...</span><br><span class="line">  SubmitApplicationRequest request =</span><br><span class="line">      Records.newRecord(SubmitApplicationRequest.class);</span><br><span class="line"></span><br><span class="line">  request.setApplicationSubmissionContext(appContext);</span><br><span class="line">...</span><br><span class="line">  <span class="comment">//<span class="doctag">TODO:</span> YARN-1763:Handle RM failovers during the submitApplication call.</span></span><br><span class="line">  rmClient.submitApplication(request);</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">return</span> applicationId;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>后续涉及到客户端和服务端和服务端通信，没有了解过，所以先到此为止。</p>
<p>参考资料:</p>
<p><a class="link"   href="https://cloud.tencent.com/developer/article/1889678" >https://cloud.tencent.com/developer/article/1889678<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://www.cnblogs.com/zsql/p/11276160.html" >https://www.cnblogs.com/zsql/p/11276160.html<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://max.book118.com/html/2021/1111/7110106022004041.shtm" >https://max.book118.com/html/2021/1111/7110106022004041.shtm<i class="fas fa-external-link-alt"></i></a></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>Hadoop 源码</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive笔记</title>
    <url>/2022/06/09/Hive/</url>
    <content><![CDATA[<p>Hive相关笔记</p>
<span id="more"></span>

<h2 id="CTE-amp-Temporary-tables-amp-View-amp-Materialized-view"><a href="#CTE-amp-Temporary-tables-amp-View-amp-Materialized-view" class="headerlink" title="CTE &amp; Temporary tables &amp; View &amp; Materialized view"></a>CTE &amp; Temporary tables &amp; View &amp; Materialized view</h2><h3 id="CTE"><a href="#CTE" class="headerlink" title="CTE"></a>CTE</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> table_name <span class="keyword">as</span> (...)</span><br><span class="line">selece <span class="operator">*</span> <span class="keyword">from</span> table_name</span><br></pre></td></tr></table></figure>



<p>Hive中的CTE全称叫做Common Table Expression 公共表达式，意思就是大家公共的，都能用的结果，一般用在SQL中可以简化我们的逻辑，提高可读性，CTE主要有以下几个优点。</p>
<ol>
<li>复用公共代码块，减少表的 读取次数，降低IO 提高性能。达到一次查询（读），多次使用，目的是减少读的次数（需要配置参数来物化CTE结果）</li>
<li>提高代码的可读性：使代码更简洁，便于维护。如将子查询抽出来以后，放到with语句，可方便定位，维护代码，代码的可读性增强。</li>
<li>做<strong>递归查询</strong>，进行迭代计算。</li>
</ol>
<p><strong>注意：</strong>CTE默认在每次调用的时候都会重新计算一遍，设置hive.optimize.cte.materialize.threshold指标来强制物化CTE结果，当CTE被使用的次数大于该指标值的时候将被存储到磁盘中。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 下面<span class="keyword">SQL</span>可以检测是否物化到磁盘，设置hive.optimize.cte.materialize.threshold <span class="operator">=</span> <span class="number">1</span> </span><br><span class="line"># 如果两个随机数是相等的，证明被物化了，如果是不等的，证明计算了两次</span><br><span class="line"><span class="keyword">with</span> temp <span class="keyword">as</span> (<span class="keyword">select</span> rand())</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> temp </span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> temp</span><br></pre></td></tr></table></figure>

<h3 id="View"><a href="#View" class="headerlink" title="View"></a>View</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 视图是保存了逻辑结构，没有真正的存储数据，所以如果逻辑复杂 需要重复计算多次 不建议使用视图</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] view_name [(column_name [COMMENT column_comment], ...) ]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> ...</span><br><span class="line"></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> view_name</span><br></pre></td></tr></table></figure>

<p>视图作用：</p>
<ol>
<li>简化SQL，使逻辑更加清晰</li>
<li>不对外公开所有的字段，例如对不同的部门建立不同的视图表</li>
<li>降低数据冗余</li>
</ol>
<p><strong>注意：</strong></p>
<ol>
<li>视图定义在创建时冻结，因此，如果视图定义为 select * from t，其中 t 是具有两列 a 和 b 的表，则稍后请求选择*从视图中看，即使稍后将新列 c 添加到表中，也应仅返回 a 和 b 列</li>
<li>视图是只读的，仅能查询，不能进行数据插入和修改</li>
<li>hive优先解析视图，比如，如果使用视图的查询语句和视图均包含limit子句，那么用户最终获取的数据条数将首先考虑视图中限制的输出记录数</li>
</ol>
<h3 id="Partitioned-View"><a href="#Partitioned-View" class="headerlink" title="Partitioned View"></a>Partitioned View</h3><p>Hive还可以建立分区视图，有人可能疑惑分区视图这个业务场景在哪里？</p>
<p>需要对每天的分区表中的数据进行一些<strong>预处理 比如去重</strong> 之后再按照天数据供查询，我们就需要分区视图，总不能针对每一个分区都建立一张视图。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 先创建表</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> if <span class="keyword">not</span> <span class="keyword">exists</span> kantlin (</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">name string,</span><br><span class="line">age <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (date_id string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br><span class="line">#插入两条相同的数据</span><br><span class="line"> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> kantlin <span class="keyword">partition</span>(date_id<span class="operator">=</span><span class="string">&#x27;20210618&#x27;</span>)   <span class="keyword">values</span> (<span class="number">1</span>,<span class="string">&#x27;kantlin&#x27;</span>,<span class="number">3</span>);</span><br><span class="line"> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> kantlin <span class="keyword">partition</span>(date_id<span class="operator">=</span><span class="string">&#x27;20210618&#x27;</span>)   <span class="keyword">values</span> (<span class="number">1</span>,<span class="string">&#x27;kantlin&#x27;</span>,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">#创建分区视图</span><br><span class="line">#注意 PARTITIONED <span class="keyword">ON</span>后面跟的是分区字段，而且这个字段必须是<span class="keyword">select</span>和<span class="keyword">group</span> <span class="keyword">by</span>的最后一个元素！</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> kantlin PARTITIONED <span class="keyword">ON</span>(date_id) <span class="keyword">AS</span> <span class="keyword">SELECT</span> id,name,age,<span class="type">time</span>,date_id  <span class="keyword">from</span> kantlin <span class="keyword">GROUP</span> <span class="keyword">BY</span> id,name,age,<span class="type">time</span>,date_id ;</span><br></pre></td></tr></table></figure>

<h3 id="Materialized-views"><a href="#Materialized-views" class="headerlink" title="Materialized views"></a>Materialized views</h3><p>Hive3.0.0引入了对物化视图的支持，需要注意的是<strong>引入物化视图的目的是为了优化数据访问的效率</strong>，物化视图相比视图来说修改要更加贴近底层，他会重新我们的查询SQL，自动优化我们的查询流程，可能我们两张表join的情况，存在满足我们结果的物化视图，那么他会直接从物化视图表拿数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> MATERIALIZED <span class="keyword">VIEW</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] [db_name.]materialized_view_name</span><br><span class="line">  [DISABLE REWRITE]</span><br><span class="line">  [COMMENT materialized_view_comment]</span><br><span class="line">  [PARTITIONED <span class="keyword">ON</span> (col_name, ...)]</span><br><span class="line">  [CLUSTERED <span class="keyword">ON</span> (col_name, ...) <span class="operator">|</span> DISTRIBUTED <span class="keyword">ON</span> (col_name, ...) SORTED <span class="keyword">ON</span> (col_name, ...)]</span><br><span class="line">  [</span><br><span class="line">    [<span class="type">ROW</span> FORMAT row_format]</span><br><span class="line">    [STORED <span class="keyword">AS</span> file_format]</span><br><span class="line">      <span class="operator">|</span> STORED <span class="keyword">BY</span> <span class="string">&#x27;storage.handler.class.name&#x27;</span> [<span class="keyword">WITH</span> SERDEPROPERTIES (...)]</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="operator">&lt;</span>query<span class="operator">&gt;</span>;</span><br></pre></td></tr></table></figure>

<p>在实例化视图创建语句中未指定的 SerDe 和存储格式的默认值(它们是可选的)分别使用配置属性<code>hive.materializedview.serde</code>和<code>hive.materializedview.fileformat</code>指定。</p>
<p>实例化视图可以使用自定义存储处理程序存储在外部系统中，例如<a class="link"   href="https://www.docs4dev.com/docs/zh/apache-hive/3.1.1/reference/Druid_Integration.html" >Druid<i class="fas fa-external-link-alt"></i></a>。例如，以下语句创建存储在 Druid 中的实例化视图：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> MATERIALIZED <span class="keyword">VIEW</span> druid_wiki_mv</span><br><span class="line">STORED <span class="keyword">AS</span> <span class="string">&#x27;org.apache.hadoop.hive.druid.DruidStorageHandler&#x27;</span></span><br><span class="line"><span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> __time, page, <span class="keyword">user</span>, c_added, c_removed</span><br><span class="line"><span class="keyword">FROM</span> src;</span><br></pre></td></tr></table></figure>

<p>当实例化视图使用的源表中的数据发生更改时，例如，插入新数据或修改现有数据时，我们将需要刷新实例化视图的内容，以使其与这些更改保持最新。当前，物化视图的重建操作需要由用户触发。特别是，用户应执行以下语句：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> MATERIALIZED <span class="keyword">VIEW</span> [db_name.]materialized_view_name REBUILD;</span><br></pre></td></tr></table></figure>

<p>Hive 支持增量视图维护，即仅刷新受原始源表中的更改影响的数据。增量视图维护将减少重建步骤的执行时间。此外，它将为物化视图中的现有数据保留 LLAP 缓存。</p>
<p>默认情况下，Hive 将尝试以增量方式重建实例化视图，如果不可能的话，将退回到完全重建。当前实现仅在源表上执行<code>INSERT</code>操作时支持增量重建，而<code>UPDATE</code>和<code>DELETE</code>操作将强制对物化视图进行完全重建。</p>
<h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><h3 id="How-to-write-SQL"><a href="#How-to-write-SQL" class="headerlink" title="How to write SQL?"></a>How to write SQL?</h3><p>本章节记录写在开发过程中遇到一些SQL书写方法或者技巧。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Mac安装grafana+Grafana连接ClickHouse</title>
    <url>/2021/10/13/Mac%E5%AE%89%E8%A3%85grafana-ClickHouse%E9%93%BE%E6%8E%A5Grafana/</url>
    <content><![CDATA[<p>Mac安装Grafana &amp; 使用第三方插件Grafana连接ClickHouse</p>
<span id="more"></span>

<h2 id="Mac安装grafana"><a href="#Mac安装grafana" class="headerlink" title="Mac安装grafana"></a>Mac安装grafana</h2><p>使用brew安装grafana</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">1.安装grafana</span><br><span class="line">brew install grafana</span><br><span class="line">2.已安装grafana，需要升级</span><br><span class="line">brew reinstall grafana</span><br></pre></td></tr></table></figure>

<p>安装完成后我们用命令检查一下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">brew list | grep grafana</span><br></pre></td></tr></table></figure>

<img src="/2021/10/13/Mac%E5%AE%89%E8%A3%85grafana-ClickHouse%E9%93%BE%E6%8E%A5Grafana/%E6%A3%80%E6%9F%A5grafana%E5%AE%89%E8%A3%85.png" class="" title="中二是最后的热血">

<p>使用brew启动grafana服务</p>
<img src="/2021/10/13/Mac%E5%AE%89%E8%A3%85grafana-ClickHouse%E9%93%BE%E6%8E%A5Grafana/%E5%90%AF%E5%8A%A8grafana.png" class="" title="中二是最后的热血">

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">停止grafana服务:</span><br><span class="line">brew service stop grafana</span><br><span class="line">重启grafana服务：</span><br><span class="line">brew service restart grafana</span><br></pre></td></tr></table></figure>

<p>启动成功之后我们可以在本地访问grafana页面，输入<code>localhost:3000</code></p>
<img src="/2021/10/13/Mac%E5%AE%89%E8%A3%85grafana-ClickHouse%E9%93%BE%E6%8E%A5Grafana/grafana.png" class="" title="中二是最后的热血">

<h2 id="安装grafana-clickhouse插件"><a href="#安装grafana-clickhouse插件" class="headerlink" title="安装grafana-clickhouse插件"></a>安装grafana-clickhouse插件</h2><p>参考grafana官网上安装教程<a class="link"   href="https://grafana.com/grafana/plugins/vertamedia-clickhouse-datasource/?tab=installation" >https://grafana.com/grafana/plugins/vertamedia-clickhouse-datasource/?tab=installation<i class="fas fa-external-link-alt"></i></a> 有两种安装方式。</p>
<p>1.通过grafana cloud安装，应该是创建grafana账号连接grafana的在线服务器下载安装，这个有兴趣的可以研究一下，我这里没有用到</p>
<p>2.<code>grafana-cli plugins install vertamedia-clickhouse-datasource</code> 使用grafana-cli安装，然后重启grafana服务，但是页面的plugin tab或者datasource中依然是没有clickhouse选项的，接下来开始排查问题出在哪里。</p>
<p>排查过程：<br>（1）卸载brew安装的grafana，使用zip压缩包重新安装grafana，依然存在这个问题<br>（2）卸载<code>grafana-clickhouse</code>插件，使用zip压缩包重新安装一次，没有解决<br>（3）使用find命令查找使用grafana-cli安装的<code>grafana-clickhouse</code>插件位置，发现是在<code>/usr/local/var/lib/grafana/plugins/vertamedia-clickhouse-datasource</code>文件夹下，并且发现在<code>/Users/liu/grafana-8.1.0/public/app/plugins/datasource/</code>中有默认已经存在的插件的文件夹，手动将<code>vertamedia-clickhouse-datasource</code>目录移到默认的文件夹中，重启grafana服务，发现plugin页面以及datasource上出现了clickhouse选项，但是点击报错<code>Fetch error404</code>:</p>
<img src="/2021/10/13/Mac%E5%AE%89%E8%A3%85grafana-ClickHouse%E9%93%BE%E6%8E%A5Grafana/%E6%8A%A5%E9%94%99.png" class="" title="中二是最后的热血">
<p>（4）查看grafana配置文件default.ini，发现其中有这么一行<br><code># Directory where grafana will automatically scan and look for plugins plugins = data/plugins</code><br>将插件目录手动移到对应目录下面，重启grafana服务，everything is fine!</p>
<img src="/2021/10/13/Mac%E5%AE%89%E8%A3%85grafana-ClickHouse%E9%93%BE%E6%8E%A5Grafana/%E6%88%90%E5%8A%9F.png" class="" title="中二是最后的热血">

<p>我也在插件的github提出了问题issue，感兴趣的可以看一下插件开发者解释，我们都初步认为是grafana的bug<br><a class="link"   href="https://github.com/Vertamedia/clickhouse-grafana/issues?q=is:issue+author:@me+is:closed" >https://github.com/Vertamedia/clickhouse-grafana/issues?q=is%3Aissue+author%3A%40me+is%3Aclosed<i class="fas fa-external-link-alt"></i></a></p>
<p>参考文章：</p>
<p><a class="link"   href="https://grafana.com/docs/grafana/latest/installation/mac/" >https://grafana.com/docs/grafana/latest/installation/mac/<i class="fas fa-external-link-alt"></i></a></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>ClickHouse</tag>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>Openresty相关</title>
    <url>/2022/07/20/Openresty%E7%9B%B8%E5%85%B3/</url>
    <content><![CDATA[<p>使用Openresty实现druid访问字段热度分析</p>
<span id="more"></span>

<h1 id="安装Openresty"><a href="#安装Openresty" class="headerlink" title="安装Openresty"></a>安装Openresty</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew install openresty/brew/openresty</span><br><span class="line"></span><br><span class="line"># 出现下面的提示安装成功</span><br><span class="line">You can find the configuration files for openresty under /opt/homebrew/etc/openresty/.</span><br><span class="line"></span><br><span class="line">To start openresty/brew/openresty now and restart at login:</span><br><span class="line">  brew services start openresty/brew/openresty</span><br><span class="line">Or, if you don&#x27;t want/need a background service you can just run:</span><br><span class="line">  openresty</span><br><span class="line">==&gt; Summary</span><br><span class="line">🍺  /opt/homebrew/Cellar/openresty/1.21.4.1_1: 307 files, 7.1MB, built in 40 seconds</span><br><span class="line">==&gt; Running `brew cleanup openresty`...</span><br><span class="line">Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.</span><br><span class="line">Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).</span><br><span class="line">==&gt; Caveats</span><br><span class="line">==&gt; openresty</span><br><span class="line">You can find the configuration files for openresty under /opt/homebrew/etc/openresty/.</span><br><span class="line"></span><br><span class="line">To start openresty/brew/openresty now and restart at login:</span><br><span class="line">  brew services start openresty/brew/openresty</span><br><span class="line">Or, if you don&#x27;t want/need a background service you can just run:</span><br><span class="line">  openresty</span><br><span class="line">  </span><br></pre></td></tr></table></figure>
<h1 id="配置Openresty环境"><a href="#配置Openresty环境" class="headerlink" title="配置Openresty环境"></a>配置Openresty环境</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">编辑 ~/.zshrc 增加以下内容</span><br><span class="line">需要注意的是如果你原先安装过nginx，那export openresty下的nginx环境的时候需要将它写到 $PATH 前面</span><br><span class="line">这样nginx才用的是openresty中的</span><br><span class="line"></span><br><span class="line"># openresty</span><br><span class="line">OPENRESTY_HOME=/opt/homebrew/Cellar/openresty/1.21.4.1_1</span><br><span class="line">export PATH=$PATH:$OPENRESTY_HOME/bin</span><br><span class="line">export PATH=$OPENRESTY_HOME/nginx/sbin:$PATH</span><br><span class="line"></span><br><span class="line">(base) ➜ which nginx</span><br><span class="line">/opt/homebrew/Cellar/openresty/1.21.4.1_1/nginx/sbin/nginx</span><br></pre></td></tr></table></figure>

<h1 id="测试openresty"><a href="#测试openresty" class="headerlink" title="测试openresty"></a>测试openresty</h1><p>创建本地项目目录，这里假设为operesty</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir /Users/liu/Documents/openresty</span><br><span class="line">mkdir /Users/liu/Documents/openresty/conf</span><br><span class="line">mkdir /Users/liu/Documents/openresty/log</span><br></pre></td></tr></table></figure>

<p>conf下创建nginx.conf文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">worker_processes 1;</span><br><span class="line">error_log log/error.log;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 8080;</span><br><span class="line">        location / &#123;</span><br><span class="line">            default_type text/html;</span><br><span class="line">            content_by_lua_block &#123;</span><br><span class="line">                ngx.say(&quot;&lt;h2&gt;hello&lt;/h2&gt;&quot;)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>使用下面的命令在后台启动nginx服务，启动后使用ps命令可以查看nginx的master和worker进程:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd openresty</span><br><span class="line">nginx -p ./ -c conf/nginx.conf</span><br><span class="line"></span><br><span class="line">ps -ef | grep nginx</span><br><span class="line">nginx: master process nginx -p ./ -c conf/nginx.conf </span><br><span class="line">nginx: worker process</span><br></pre></td></tr></table></figure>

<p>在本机上访问8080端口</p>
<img src="/2022/07/20/Openresty%E7%9B%B8%E5%85%B3/hello.png" class="" title="中二是最后的热血">

<p>测试之后停止nginx服务</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nginx -p ./ -s stop</span><br></pre></td></tr></table></figure>

<h1 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h1><h2 id="nginx-location"><a href="#nginx-location" class="headerlink" title="nginx location"></a>nginx location</h2><p>Location 块通过指定模式来与客户端请求的URI相匹配</p>
<h3 id="location匹配顺序"><a href="#location匹配顺序" class="headerlink" title="location匹配顺序"></a>location匹配顺序</h3><p>nginx有两层指令来匹配请求 URI 。第一个层次是 server 指令，它通过域名、ip 和端口来做第一层级匹配，当找到匹配的 server 后就进入此 server 的 location 匹配。</p>
<p>location 的匹配并不完全按照其在配置文件中出现的顺序来匹配，请求URI 会按如下规则进行匹配：</p>
<ol>
<li>先精准匹配 <strong><code>=</code></strong> ，精准匹配成功则会立即停止其他类型匹配；</li>
<li>没有精准匹配成功时，进行前缀匹配。先查找带有 <strong><code>^~</code></strong> 的前缀匹配，带有 <strong><code>^~</code></strong> 的前缀匹配成功则立即停止其他类型匹配，普通前缀匹配（不带参数 <strong><code>^~</code></strong> ）成功则会暂存，继续查找正则匹配；</li>
<li><strong><code>=</code></strong> 和 <strong><code>^~</code></strong> 均未匹配成功前提下，查找正则匹配 <strong><code>~</code></strong> 和 <strong><code>~\*</code></strong> 。当同时有多个正则匹配时，按其在配置文件中出现的先后顺序优先匹配，命中则立即停止其他类型匹配；</li>
<li>所有正则匹配均未成功时，返回步骤 2 中暂存的普通前缀匹配（不带参数 <strong><code>^~</code></strong> ）结果</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1. location =    <span class="comment"># 精准匹配</span></span><br><span class="line">2. location ^~   <span class="comment"># 带参前缀匹配</span></span><br><span class="line">3. location ~    <span class="comment"># 正则匹配（区分大小写）</span></span><br><span class="line">4. location ~*   <span class="comment"># 正则匹配（不区分大小写）</span></span><br><span class="line">5. location /a   <span class="comment"># 普通前缀匹配，优先级低于带参数前缀匹配。</span></span><br><span class="line">6. location /    <span class="comment"># 任何没有匹配成功的，都会匹配这里处理</span></span><br></pre></td></tr></table></figure>

<h2 id="nginx-mirror"><a href="#nginx-mirror" class="headerlink" title="nginx mirror"></a>nginx mirror</h2><p>利用 mirror 模块，业务可以将线上实时访问流量拷贝至其他环境，基于这些流量可以做一些关于流量的测试或者其他操作，这里我们将流量复制出来，然后解析其中的字段，核心配置如下:</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">...</span><br><span class="line">        location / &#123;</span><br><span class="line">        mirror /mirror;       # 将流量复制到/mirror下</span><br><span class="line">        mirror_request_body on;</span><br><span class="line"></span><br><span class="line">        proxy_pass http://druid_broker; # 将流量复制完毕之后继续传递给主服务提供查询</span><br><span class="line">        proxy_set_header   Host             $host;</span><br><span class="line">        proxy_set_header   X-Real-IP        $remote_addr;</span><br><span class="line">        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    location = /mirror &#123;</span><br><span class="line">        internal;             # 内部服务 外部访问会出现404</span><br><span class="line">        proxy_pass http://127.0.0.1:28082$request_uri;  # 镜像服务 操作镜像流量</span><br><span class="line">        proxy_pass_request_body on;</span><br><span class="line">        proxy_set_header X-Original-URI $request_uri;</span><br><span class="line">    &#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="如何引用第三方resty库"><a href="#如何引用第三方resty库" class="headerlink" title="如何引用第三方resty库"></a>如何引用第三方resty库</h2><p>在Openresy中引用第三方lua库非常简单，只需要将对应的lua文件拷贝到指定目录。</p>
<h3 id="resty-http"><a href="#resty-http" class="headerlink" title="resty.http"></a>resty.http</h3><p>我们使用openresty发起http请求的时候需要额外的resty.http包，需要将<a class="link"   href="https://github.com/ledgetech/lua-resty-http" >这个连接<i class="fas fa-external-link-alt"></i></a>下 <code>lua-resty-http/lib/resty/</code> 目录下的 http.lua 和 http_headers.lua 两个文件拷贝到openresty安装目录下<code>&#123;OPENRESTY_HOME&#125;/lualib/resty</code>目录下即可</p>
<h3 id="include-mime-types-报错"><a href="#include-mime-types-报错" class="headerlink" title="include mime.types;报错"></a>include mime.types;报错</h3><p>参考<a class="link"   href="https://blog.csdn.net/Muxi_k/article/details/104393009" >这个连接<i class="fas fa-external-link-alt"></i></a>解决了这个问题，如果我们在启动nginx的时候出现这个问题是因为，我们openresty自带的nginx中是没有<code>mime.types</code>这个文件，从github上将这个文件薅下来即可</p>
<h3 id="Nginx中upstream"><a href="#Nginx中upstream" class="headerlink" title="Nginx中upstream"></a>Nginx中upstream</h3><p>目前我理解的nginx中的upstream是为了分散请求压力，并且如果一台机器的服务挂了，nginx会自动检测到并且将需求全部转到另一台服务器，做到一定的高可用，但是如果nginx都挂了，这时候keepalived可以保证nginx的高可用(我还没用过)。</p>
<p>网上查阅nginx upstrem相关资料的时候发现除了最基本的轮询，还可以根据weight(权重)，ip_hash，fair（公平地按照后端服务器的响应时间来分配请求，响应时间短即rt小的后端服务器优先分配请求），url_hash（也可解决session问题，但需要注意，使用这种方式后，server语句中不能写入weight等其他参数）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">upstream backend &#123;</span><br><span class="line">    server 192.168.1.128:8081 weight=1; // 如果这个服务器挂掉了请求会自动传递给下面那个</span><br><span class="line">    server 192.168.1.128:8082 weight=2;</span><br><span class="line">&#125;</span><br><span class="line">server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name 0.0.0.0;</span><br><span class="line">        location / &#123;</span><br><span class="line">		    proxy_pass http://backend;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="Nginx-Openresty-监控-druid查询热度"><a href="#Nginx-Openresty-监控-druid查询热度" class="headerlink" title="Nginx + Openresty 监控 druid查询热度"></a>Nginx + Openresty 监控 druid查询热度</h3><figure class="highlight lua"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen <span class="number">80</span>;</span><br><span class="line">    client_max_body_size <span class="number">8</span>m;</span><br><span class="line">    client_body_buffer_size <span class="number">1</span>m;</span><br><span class="line">    proxy_buffers <span class="number">32</span> <span class="number">128</span>k;</span><br><span class="line">    proxy_busy_buffers_size <span class="number">1024</span>k;</span><br><span class="line">    access_log  logs/druid_broker.<span class="built_in">log</span>  main;</span><br><span class="line"></span><br><span class="line">    lua_need_request_body on;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        mirror /mirror;</span><br><span class="line">        mirror_request_body on;</span><br><span class="line"></span><br><span class="line">        proxy_pass http://druid_broker;</span><br><span class="line">        proxy_set_header   Host             $host;</span><br><span class="line">        proxy_set_header   X-Real-IP        $remote_addr;</span><br><span class="line">        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 需要 resty.http mo</span><br><span class="line">    location = /check &#123;</span><br><span class="line">        content_by_lua_block &#123;</span><br><span class="line">            <span class="keyword">local</span> httpc = <span class="built_in">require</span>(<span class="string">&quot;resty.http&quot;</span>).new()</span><br><span class="line">            <span class="keyword">local</span> res, err = httpc:request_uri(<span class="string">&quot;http://127.0.0.1:8082/druid/v2/sql/&quot;</span>, &#123;</span><br><span class="line">                method = <span class="string">&quot;POST&quot;</span>,</span><br><span class="line">                body = <span class="string">&#x27;&#123;&quot;query&quot;: &quot;SELECT CURRENT_TIMESTAMP&quot;&#125;&#x27;</span>,</span><br><span class="line">                headers = &#123;</span><br><span class="line">                    [<span class="string">&quot;Content-Type&quot;</span>] = <span class="string">&quot;application/json&quot;</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            <span class="keyword">if</span> err <span class="keyword">then</span></span><br><span class="line">                ngx.<span class="built_in">status</span> = <span class="number">500</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                ngx.<span class="built_in">status</span> = <span class="number">200</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            ngx.header.content_type = <span class="string">&#x27;application/json&#x27;</span></span><br><span class="line">            ngx.say(res.body)</span><br><span class="line">            ngx.eof()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location = /mirror &#123;   # 这块是将上面复制到/mirror中的流量发送到<span class="number">28082</span>端口，下面server会处理这个端口接收到的数据</span><br><span class="line">        internal;</span><br><span class="line">        proxy_pass http://<span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">28082</span>$request_uri;</span><br><span class="line">        proxy_pass_request_body on;</span><br><span class="line">        proxy_set_header X-Original-URI $request_uri;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span>:<span class="number">28082</span>;</span><br><span class="line"></span><br><span class="line">    lua_need_request_body on;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line"></span><br><span class="line">        content_by_lua_block &#123;</span><br><span class="line">            <span class="keyword">local</span> dataSource=<span class="string">&#x27;&#x27;</span>;</span><br><span class="line">            <span class="keyword">local</span> request_uri = ngx.var.request_uri</span><br><span class="line">            <span class="keyword">local</span> data = ngx.req.get_body_data()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">string</span>.<span class="built_in">find</span>(request_uri, <span class="string">&quot;^/druid/v2/sql&quot;</span>) <span class="keyword">then</span>  # 如果url是sql结尾的 ，那么需要进行解析 并且将字段发送到kafka中</span><br><span class="line">                data = <span class="built_in">string</span>.<span class="built_in">lower</span>(data)</span><br><span class="line">                data = <span class="built_in">string</span>.<span class="built_in">gsub</span>(data, <span class="string">&#x27;\\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                data = <span class="built_in">string</span>.<span class="built_in">gsub</span>(data, <span class="string">&#x27;/[*].*[*]/&#x27;</span>, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                fromIndex, endIndex = <span class="built_in">string</span>.<span class="built_in">find</span>(data, <span class="string">&quot;from +[%a_]+[ ]&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> fromIndex ~= <span class="literal">nil</span> <span class="keyword">then</span></span><br><span class="line">                    dataSource = <span class="built_in">string</span>.<span class="built_in">gsub</span>(<span class="built_in">string</span>.<span class="built_in">sub</span>(data, fromIndex+<span class="number">4</span>, endIndex), <span class="string">&#x27;%s&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">local</span> json = <span class="built_in">require</span> <span class="string">&quot;cjson.safe&quot;</span></span><br><span class="line">                jsonBody = json.decode(data)</span><br><span class="line">                <span class="keyword">if</span> jsonBody.dataSource ~= <span class="literal">nil</span> <span class="keyword">then</span></span><br><span class="line">                    dataSource = jsonBody.dataSource</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">local</span> producer = <span class="built_in">require</span> <span class="string">&quot;resty.kafka.producer&quot;</span></span><br><span class="line">            <span class="keyword">local</span> broker_list = &#123;</span><br><span class="line">                &#123; host = <span class="string">&quot;kfk01&quot;</span>, port = <span class="number">9092</span> &#125;,</span><br><span class="line">                &#123; host = <span class="string">&quot;kfk02&quot;</span>, port = <span class="number">9092</span> &#125;,</span><br><span class="line">                &#123; host = <span class="string">&quot;kfk03&quot;</span>, port = <span class="number">9092</span> &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> dataSource ~= <span class="string">&#x27;&#x27;</span> <span class="keyword">then</span></span><br><span class="line">                <span class="keyword">local</span> bp = producer:new(broker_list, &#123; producer_type = <span class="string">&quot;async&quot;</span>, flush_time= <span class="string">&#x27;5000&#x27;</span> &#125;)</span><br><span class="line">                <span class="keyword">local</span> ok, err = bp:send(<span class="string">&quot;druid_monitor&quot;</span>, <span class="literal">nil</span>, <span class="string">&#x27;&#123;&quot;ts&quot;:&#x27;</span>.. ngx.<span class="built_in">time</span>() ..<span class="string">&#x27;,&quot;datasource&quot;:&quot;&#x27;</span>..dataSource..<span class="string">&#x27;&quot;&#125;&#x27;</span>) # 这行将数据源发送到kafka中 topic名称为druid_monitor</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> ok <span class="keyword">then</span></span><br><span class="line">                    ngx.<span class="built_in">log</span>(ngx.ERR, <span class="string">&#x27;kafka send err:&#x27;</span>, err)</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">            ngx.say(<span class="string">&#x27;ok&#x27;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">upstream druid_broker &#123;</span><br><span class="line">   server adt-bd-c1-druid-broker01.adtiming.int:<span class="number">8082</span> max_fails=<span class="number">3</span> fail_timeout=<span class="number">60</span>;</span><br><span class="line">   server adt-bd-c1-druid-broker02.adtiming.int:<span class="number">8082</span> max_fails=<span class="number">3</span> fail_timeout=<span class="number">60</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">到此为止openresty监控druid查询就结束了。</span><br></pre></td></tr></table></figure>

<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>Openresty</tag>
      </tags>
  </entry>
  <entry>
    <title>Superset查询会丢数据问题排查</title>
    <url>/2021/10/18/Superset%E6%9F%A5%E8%AF%A2%E4%BC%9A%E4%B8%A2%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/</url>
    <content><![CDATA[<p>Superset版本 1.3.0</p>
<p>查询结果会比正确结果少两条数据</p>
<span id="more"></span>

<p>首先我先讲述我是如何发现这个问题的，准确的说是运营如何发现这个问题的。</p>
<p>数据有四列，分别是’Time’ ‘pid’ ‘eid’ ‘cnt’ ，首先我们在clickhouse客户端进行操作，其中我限定pid=’609’，然后eid进行group by 操作，然后sum(cnt)，可以看到数据有三条</p>
<img src="/2021/10/18/Superset%E6%9F%A5%E8%AF%A2%E4%BC%9A%E4%B8%A2%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/client%E7%BB%93%E6%9E%9C.png" class="" title="中二是最后的热血">

<p>我从superset进行同样的操作，需要注意的是我上面在clickhouse客户端操作的SQL就是在superset上操作view sql生成的SQL语句，所以他们肯定是相同的操作。</p>
<img src="/2021/10/18/Superset%E6%9F%A5%E8%AF%A2%E4%BC%9A%E4%B8%A2%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/superset%E7%BB%93%E6%9E%9C.png" class="" title="中二是最后的热血">

<p>可以看到superset只有一条结果返回，经过我监控query_log日志中的SQL语句发现superset请求的SQL语句确实没有什么问题，但是他在接收到结果之后进行了一些操作会导致数据少两条。</p>
<p>经过多方排查以及询问，发现最后是superset连接clickhouse使用的协议导致的，首先superset连接clickhouse使用的是开源的第三方的库叫做 <strong>clickhouse-sqlalchemy</strong> ，它支持两种连接clickhouse的协议，分别是http以及native(TCP)协议其中我们默认使用的是http协议，反映出来的url就是这么写 ：</p>
<p><code>clickhouse://bigdata:XXXXXXXXXX@127.0.0.1:8123/default</code></p>
<p>还有一种native(TCP)协议，它的url是这么写，需要注意的是必须要去掉端口号，加上端口号superset不能正常解析。</p>
<p><code>clickhouse+native://bigdata:XXXXXXXXXX@127.0.0.1/default</code></p>
<p>而第一种协议需要我们在执行底层查询的时候指定查询结果的format，否则返回的结果就是不正确的。</p>
<p><a class="link"   href="https://github.com/xzkostyan/clickhouse-sqlalchemy/issues/14#issuecomment-390755088" >https://github.com/xzkostyan/clickhouse-sqlalchemy/issues/14#issuecomment-390755088<i class="fas fa-external-link-alt"></i></a></p>
<p>改用第二种native的形式并且去掉端口号之和superset返回的数据和clickhouse客户端一致了。</p>
<p>参考连接：</p>
<p><a class="link"   href="https://github.com/apache/superset/issues/15190" >https://github.com/apache/superset/issues/15190<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://github.com/xzkostyan/clickhouse-sqlalchemy/issues/14" >https://github.com/xzkostyan/clickhouse-sqlalchemy/issues/14<i class="fas fa-external-link-alt"></i></a></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>ClickHouse</tag>
        <tag>Superset</tag>
      </tags>
  </entry>
  <entry>
    <title>StarRocks笔记</title>
    <url>/2022/05/24/StarRocks%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>好记性不如烂笔头</p>
<span id="more"></span>

<h3 id="什么是StarRocks"><a href="#什么是StarRocks" class="headerlink" title="什么是StarRocks"></a>什么是StarRocks</h3><p>StarRocks 是<strong>新一代极速全场景MPP数据库</strong>。StarRocks 的愿景是能够让用户的<strong>数据分析变得更加简单和敏捷</strong>。用户无需经过复杂的预处理，就可以用 StarRocks 来支持多种数据分析场景的极速分析。</p>
<img src="/2022/05/24/StarRocks%E7%AC%94%E8%AE%B0/%E6%9E%B6%E6%9E%84.png" class="" title="中二是最后的热血">

<h3 id="StarRocks适用哪些场景"><a href="#StarRocks适用哪些场景" class="headerlink" title="StarRocks适用哪些场景"></a>StarRocks适用哪些场景</h3><p>StarRocks 可以满足企业级用户的多种分析需求，包括 OLAP 多维分析、定制报表、实时数据分析和 Ad-hoc 数据分析等。</p>
<h3 id="StarRocks三种数据模型"><a href="#StarRocks三种数据模型" class="headerlink" title="StarRocks三种数据模型"></a>StarRocks三种数据模型</h3><p>StarRocks根据数据摄入时候的映射关系，分为了三种数据关系，分别是明细、聚合、以及更新数据关系，三种数据关系在StarRocks中分别对应着不同的数据模型:</p>
<p>明细表对应明细模型（Duplicate Key）</p>
<p>聚合表对应聚合模型（Aggregate Key）</p>
<p>更新表对应更新模型（Unique Key）和主键模型（Primary Key）</p>
<h4 id="明细模型（Duplicate-Key）"><a href="#明细模型（Duplicate-Key）" class="headerlink" title="明细模型（Duplicate Key）"></a>明细模型（Duplicate Key）</h4><ol>
<li><p>需要保留原始的数据（例如原始日志，原始操作记录等）来进行分析；</p>
</li>
<li><p>查询方式灵活, 不局限于预先定义的分析方式, 传统的预聚合方式难以命中;</p>
</li>
<li><p>数据更新不频繁。导入数据的来源一般为日志数据或者是时序数据,  以追加写为主要特点, 数据产生后就不会发生太多变化。</p>
<p><strong>注意：</strong>明细模型不会根据设置的主键覆盖数据，导入完全相同的两行数据也会认为是不同的两行插入进来，比如我们日志中经常出现含义相同的信息，但是我们一般不会希望存在覆盖的情况。</p>
</li>
</ol>
<h4 id="聚合模型（Aggregate-Key）"><a href="#聚合模型（Aggregate-Key）" class="headerlink" title="聚合模型（Aggregate Key）"></a>聚合模型（Aggregate Key）</h4><ul>
<li><p>分析网站或APP访问流量，统计用户的访问总时长、访问总次数;</p>
</li>
<li><p>广告厂商为广告主提供的广告点击总量、展示总量、消费统计等;</p>
</li>
<li><p>分析电商的全年的交易数据, 获得某指定季度或者月份的, 各人口分类(geographic)的爆款商品。</p>
<p><strong>注意：</strong></p>
<p>1.只要是指定聚合函数的字段即视为指标列，表即视为聚合表，聚合表中不带聚合函数的列即为维度列</p>
<p>2.建表语句中维度列必须在指标列前面</p>
<p>3.聚合模型是根据按照相同的指标列进行聚合，多条数据具有相同的维度列的时候，会按照聚合函数将指标列数据进行聚合，这样可以减少数据量</p>
<p>4.触发数据聚合有三种方式 : </p>
<p>(1)数据导入时，数据落盘前的聚合 </p>
<p>(2) 数据落盘后，后台的多版本异步聚合 </p>
<p>(3)数据查询时，多版本多路归并聚合</p>
</li>
</ul>
<h4 id="更新模型-Unique-Key-amp-Primary-Key"><a href="#更新模型-Unique-Key-amp-Primary-Key" class="headerlink" title="更新模型(Unique Key &amp; Primary Key)"></a>更新模型(Unique Key &amp; Primary Key)</h4><p><a class="link"   href="https://docs.starrocks.com/zh-cn/main/table_design/Data_model#%E6%9B%B4%E6%96%B0%E6%A8%A1%E5%9E%8B" >Unique Key 官网讲解<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://docs.starrocks.com/zh-cn/main/table_design/Data_model#%E4%B8%BB%E9%94%AE%E6%A8%A1%E5%9E%8B" >Primary Key官网讲解<i class="fas fa-external-link-alt"></i></a></p>
<p><strong>注意:</strong></p>
<p>即 Unique 模型完全可以用聚合模型中的 REPLACE 方式替代。</p>
<p>Primary Key模型是starrocks全新设计模型，主键模型的表要求有唯一的主键，支持对表中的行按主键进行更新和删除操作。相较更新模型，主键模型可以更好地支持<strong>实时和频繁更新</strong>等场景。</p>
<h3 id="StarRocks表设计"><a href="#StarRocks表设计" class="headerlink" title="StarRocks表设计"></a>StarRocks表设计</h3><p>StarRocks和其他OLAP数据库存储设计类似，采用的列式存储，可以提高数据查询效率，每一列的数据类型一致，这样可以提高数据的压缩率，</p>
<p>StarRocks的数据列可以分为两种，分别是维度列以及指标列，维度列用于分组和排序，指标列通过聚合函数在维度列基础上进行SUM，COUNT，BITMAP_UNION等操作。</p>
<img src="/2022/05/24/StarRocks%E7%AC%94%E8%AE%B0/SR%E8%A1%A8%E7%BB%93%E6%9E%84.png" class="" title="中二是最后的热血">

<h4 id="前缀索引"><a href="#前缀索引" class="headerlink" title="前缀索引"></a>前缀索引</h4><p>Doris 不支持在任意列上创建索引，本质上，Doris 的数据存储在类似 SSTable（Sorted String Table）的数据结构中。该结构是一种有序的数据结构，可以按照指定的列进行排序存储。在这种数据结构上，以排序列作为条件进行查找，会非常的高效。</p>
<p>比如我们建立的前缀索引为(user_id,age,message)，那么下面第一种查询要比第二种快得多</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># 命中索引</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> <span class="keyword">table</span> <span class="keyword">WHERE</span> user_id<span class="operator">=</span><span class="number">1829239</span> <span class="keyword">and</span> age<span class="operator">=</span><span class="number">20</span>；</span><br><span class="line"># 没有命中索引</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> <span class="keyword">table</span> <span class="keyword">WHERE</span> age<span class="operator">=</span><span class="number">20</span>；</span><br></pre></td></tr></table></figure>

<p>下面我们看一下索引文件的结构</p>
<img src="/2022/05/24/StarRocks%E7%AC%94%E8%AE%B0/StarRocks%E8%A1%A8%E7%B4%A2%E5%BC%95.png" class="" title="中二是最后的热血">

<p>每个部分的描述与详细功能：</p>
<p><strong>short key index表:</strong></p>
<p>稀疏索引和行号之间的对应关系，首先表中数据会按照排序键进行排序，然后每1024行会构成一个逻辑块，并且获得一个shortkey index，这个key是由排序键进行拼接而成，不会超过36个字节，我们在查询的时候首先会根据要查询的排序键值来确定所属的数据块的行号，这个行号就是数据在第多少行。</p>
<p> <strong>Per-column cardinal index:</strong> </p>
<p>列式存储，列与列之间是独立存储，所以每列都会有一个自己的行号和块号的对应关系，划分块是通过每块大概是64kb来进行划分，根据前面找到的行号找到对应的数据块地址</p>
<p><strong>Per-column data blocks:</strong></p>
<p>表中每一列数据按64KB分块存储，根据前面找到的数据块地址就可以找到对应的块内容了。</p>
<p><strong>注意:</strong></p>
<p>这种使用范围查找的需要有前提条件，就是查询时, 如果指定了维度列的<strong>等值条件</strong>或者<strong>范围条件</strong>, 并且这些条件中维度列<strong>可构成表维度列的前缀</strong>, 则可以利用数据的有序性</p>
<p>使用索引范围查找的条件:</p>
<p>（1）维度列的等值或者范围条件，像 in (…) 这种是不能索引</p>
<p>（2）条件中的维度列可以构成前缀</p>
<p>Eg: 对于表table1: (event_day, siteid, citycode, username)➜(pv); 当查询条件为event_day &gt; 2020-09-18 and siteid = 2, 则可以使用范围查找; 如果指定条件为citycode = 4 and username in [“Andy”, “Boby”, “Christian”, “StarRocks”], 则无法使用范围查找，或者查询条件是 where code = 4这种也是不能适用范围查找，没有命中。</p>
<h3 id="RollUp以及物化视图"><a href="#RollUp以及物化视图" class="headerlink" title="RollUp以及物化视图"></a>RollUp以及物化视图</h3><h4 id="Rollup"><a href="#Rollup" class="headerlink" title="Rollup"></a>Rollup</h4><p>Rollup 可以理解为 Table 的一个物化索引结构。<strong>物化</strong> 是因为其数据在<strong>物理上独立存储</strong>，而 <strong>索引</strong> 的意思是，Rollup可以<strong>调整列顺序</strong>以增加前缀索引的命中率，也可以减少key列以增加数据的聚合度。</p>
<p>数据上Rollup意味着在多维分析中“上卷操作”，上卷意味着在数据立方体上进行一个维度的合并，与之对应的还有“钻取”操作(Drill-down)，钻取意味着在某些维度上进一步打开扩展维度，详细可以参考<a class="link"   href="https://www.zhihu.com/question/19955124" >这里<i class="fas fa-external-link-alt"></i></a></p>
<h5 id="Aggregate-Key-amp-Roll-up"><a href="#Aggregate-Key-amp-Roll-up" class="headerlink" title="Aggregate Key &amp; Roll up"></a>Aggregate Key &amp; Roll up</h5><p>Rollup在聚合模型上可以发挥很好地作用，基于Base表创建Rollup可以获得更好的聚合数据效果以及调整key顺序，可以提高Rollup命中率</p>
<h5 id="Unique-Key-amp-Roll-up"><a href="#Unique-Key-amp-Roll-up" class="headerlink" title="Unique Key &amp; Roll up"></a>Unique Key &amp; Roll up</h5><p>更新模型中的Unique Key相当于聚合模型中聚合函数被固定为Replace，所以每次在Unique上创建Rollup的时候必须选中所有的维度列，即在Unique Key模型上创建Rollup只能调整维度key的顺序，可以获得更高的Rollup命中率，但是因为Rollup数据是需要独立存储的，这样的话数据是否直接全部被存储了两遍？还需要后面验证下。</p>
<h5 id="Duplicate-Key-amp-Roll-up"><a href="#Duplicate-Key-amp-Roll-up" class="headerlink" title="Duplicate Key &amp; Roll up"></a>Duplicate Key &amp; Roll up</h5><p>Duplicate Key模型和Unique Key模型类似，同样因为 Duplicate 模型没有聚合的语意。所以该模型中的 ROLLUP，已经失去了“上卷”这一层含义。而仅仅是作为调整列顺序，以命中前缀索引的作用。我们将在<a class="link"   href="https://doris.apache.org/zh-CN/data-table/index/prefix-index.html" >前缀索引<i class="fas fa-external-link-alt"></i></a>详细介绍前缀索引，以及如何使用ROLLUP改变前缀索引，以获得更好的查询效率。</p>
<h4 id="物化视图"><a href="#物化视图" class="headerlink" title="物化视图"></a>物化视图</h4><p>说到物化视图就一定要和Rollup做一个区分，物化视图相当于Rollup的一个超集，和物化视图相比Rollup 具有一定的局限性：</p>
<p>1.Rollup不能做明细模型的聚合，只能起到更改前缀索引顺序来加快查询速度，因为明细模型Duplicate Key表本身就没有聚合的含义</p>
<p>2.Rollup的聚合函数必须和Base table保持一致，物化视图的聚合函数可以和base表不同</p>
<p>3.物化视图是不支持Unique Key模型，因为物化视图实际上是预聚合进行计算，比如sum，count，聚合完毕之后我们就丢失了明细信息，这时候如果你去更新明细信息，物化视图就没法知道该如何做出改变，比如两条明细数据分别是1、5，我们建立物化视图求和，这时候这个值是6，然后我们更新数据，将1更改成2，但是rollup手里只有一个6，他知道其中一个值被变成2了，但是仅此而已，所以物化视图是不支持更新模型。</p>
<h5 id="更新策略"><a href="#更新策略" class="headerlink" title="更新策略"></a>更新策略</h5><p>为保证物化视图表和 Base 表的数据一致性, Doris 会将导入，删除等对 base 表的操作都同步到物化视图表中。并且通过增量更新的方式来提升更新效率。通过事务方式来保证原子性。</p>
<p>比如如果用户通过 INSERT 命令插入数据到 base 表中，则这条数据会同步插入到物化视图中。当 base 表和物化视图表均写入成功后，INSERT 命令才会成功返回。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>StarRocks</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用JMX_Expoter+Prometheus+Grafana监控Hadoop集群&amp;Hbase集群</title>
    <url>/2021/11/03/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JMX-Prometheus-Grafana%E7%9B%91%E6%8E%A7Hadoop%E9%9B%86%E7%BE%A4-Hbase%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<p>因为目前CDH以及HDP后续要合并闭源，公司打算花时间自研一个类似的平台，我也对集群监控这块下了点功夫。</p>
<span id="more"></span>

<h2 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h2><p>​    对于一个集群管理平台，首当其冲的就是其中的监控如何实现，毕竟很多时候我们打开它只是因为邮箱里收到了报警:-)，那么我们应该如何获取Hadoop等集群的信息呢？这时候需要简单了解一个知识点了：JMX。</p>
<p>​    我们简单介绍一下Java的JMX是什么，JMX全程叫做Java Management Extensions，翻译过来就是Java内存管理，最常用到的就是对于 JVM 的监测和管理，比如 JVM 内存、CPU 使用率、线程数、垃圾收集情况等等。另外，还可以用作日志级别的动态修改，比如 log4j 就支持 JMX 方式动态修改线上服务的日志级别。</p>
<img src="/2021/11/03/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JMX-Prometheus-Grafana%E7%9B%91%E6%8E%A7Hadoop%E9%9B%86%E7%BE%A4-Hbase%E9%9B%86%E7%BE%A4/JMX%E6%9E%B6%E6%9E%84.png" class="" title="中二是最后的热血">

<p>总而言之，就是Java自己开发的用于监控JVM指标的一个工具，可以提供给一些界面JConsole，VisualVM使用，详细信息可以参考<a class="link"   href="https://juejin.cn/post/6856949531003748365" >这里!<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="获取集群的JMX信息"><a href="#获取集群的JMX信息" class="headerlink" title="获取集群的JMX信息"></a>获取集群的JMX信息</h2><p>​    Hadoop，Hbase集群都提供了便捷获取集群JMX信息的途径，具体通过在访问地址后面加上<code>/jmx</code>来实现，比如我们访问hdfs的NameNode页面的时候地址是<code>localhost:50070</code>，那么在后面加上一个<code>/jmx</code>即为<code>localhost:50070/jmx</code>，访问即可得到类似下面的信息。</p>
<img src="/2021/11/03/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JMX-Prometheus-Grafana%E7%9B%91%E6%8E%A7Hadoop%E9%9B%86%E7%BE%A4-Hbase%E9%9B%86%E7%BE%A4/jmx%E7%9B%91%E6%8E%A7%E9%A1%B5%E9%9D%A2.png" class="" title="中二是最后的热血">

<p>​    上面图中具体指标信息可以在Hadoop官方文档对应的<a class="link"   href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/Metrics.html" >metrics章节<i class="fas fa-external-link-alt"></i></a>中找到，其中包括了Namenode以及Datanode相关信息，同理如果我们在8088端口后面加上<code>/jmx</code>即可获得关于Yarn相关指标信息。</p>
<p>​    现在我们监控的信息有了，接下来如果我们想将数据在监控折线图中展现出来的话就需要一个时序数据库，因为监控指标数据必须存在对应的时间才有意义，目前比较常见的时序数据库+界面组合是普罗米修斯（时序数据库）+ Grafana（界面展示），那么目前的问题就转换成了如何将Hadoop集群中的JMX信息传递给普罗米修斯，简单调研就可以发现，普罗米修斯自己开发了一款插件支持将java程序对应的jmx信息传递到自己的时序数据库中，<a class="link"   href="https://github.com/prometheus/jmx_exporter" >插件地址<i class="fas fa-external-link-alt"></i></a>。</p>
<p>  根据自己Java环境版本下载插件，将插件放置到自己选定的位置，现在插件有了，就差如何在集群中使用插件了，我们开始着手修改集群中的配置。</p>
<h2 id="Hadoop集群配置Jmx-expoter"><a href="#Hadoop集群配置Jmx-expoter" class="headerlink" title="Hadoop集群配置Jmx_expoter"></a>Hadoop集群配置Jmx_expoter</h2><p>在hadoop-env.sh中最后添加以下代码，注意将其中的路径修改成自己系统中的路径，这段代码的主要作用是添加我们下载的jar包，以及给jar包传递配置文件，以及指定该服务要占用的端口，这里的对应位置的配置文件<code>prometheus_config.yml</code>测试的时候可以直接创建一个空文件即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">if ! grep -q &lt;&lt;&lt;&quot;$HDFS_NAMENODE_OPTS&quot; jmx_prometheus_javaagent; then</span><br><span class="line">HDFS_NAMENODE_OPTS=&quot;$HDFS_NAMENODE_OPTS -javaagent:/usr/local/Cellar/hadoop/3.3.1/jmx_prometheus_javaagent-0.16.1.jar=27001:/usr/local/Cellar/hadoop/3.3.1/libexec/etc/hadoop/prometheus_config.yml&quot;</span><br><span class="line">fi</span><br><span class="line">if ! grep -q &lt;&lt;&lt;&quot;$HDFS_DATANODE_OPTS&quot; jmx_prometheus_javaagent; then</span><br><span class="line">HDFS_DATANODE_OPTS=&quot;$HDFS_DATANODE_OPTS -javaagent:/usr/local/Cellar/hadoop/3.3.1/jmx_prometheus_javaagent-0.16.1.jar=27002:/usr/local/Cellar/hadoop/3.3.1/libexec/etc/hadoop/prometheus_config.yml&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p><strong>注意事项</strong> ：</p>
<p>1.上面的代码不能直接写成类似以下模式，因为不能在<code>$HADOOP_OPTS</code>中出现<code>multiple -javaagent opts</code>，就是不能直接出现多个-javaagent选项，必须要换一种写法，将<code>-javaagent</code>写在if else代码中可以避免这个问题，详细可以参考这个<a class="link"   href="https://stackoverflow.com/questions/47121498/configuring-prometheus-jmx-exporter-for-hadoop2/59837822#59837822" >stackoverflow回答<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">写成这种模式会报错</span></span><br><span class="line">export HADOOP_NAMENODE_OPTS=&quot;$HADOOP_NAMENODE_OPTS -javaagent:/home/ec2-user/jmx_exporter/jmx_prometheus_javaagent-0.10.jar=9102:/home/ec2-user/jmx_exporter/prometheus_config.yml&quot;</span><br><span class="line">export HADOOP_DATANODE_OPTS=&quot;$HADOOP_DATANODE_OPTS -javaagent:/home/ec2-user/jmx_exporter/jmx_prometheus_javaagent-0.10.jar=9102:/home/ec2-user/jmx_exporter/prometheus_config.yml&quot;</span><br></pre></td></tr></table></figure>

<p>2.同一台机器的每一个JMX服务端口必须区分开</p>
<p>比如上面namenode的jmx服务所占用的端口为27001，datanode的jmx服务所占用的端口为27002，如果使用了相同的端口，那么在启动hdfs服务(./start-dfs.sh)的时候会报如下所示的错。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Starting namenodes on [localhost]</span><br><span class="line">Starting datanodes</span><br><span class="line">localhost: /usr/local/Cellar/hadoop/3.3.1/libexec/bin/../libexec/hadoop-functions.sh: line 1821: 11125 Abort trap: 6           hadoop_start_daemon &quot;$&#123;daemonname&#125;&quot; &quot;$class&quot; &quot;$&#123;pidfile&#125;&quot; &quot;$@&quot; &gt;&gt; &quot;$&#123;outfile&#125;&quot; 2&gt;&amp;1 &lt; /dev/null</span><br><span class="line">localhost: ERROR: Cannot set priority of datanode process 11125</span><br><span class="line">localhost: ERROR: Cannot disconnect datanode process 11125</span><br></pre></td></tr></table></figure>



<p>这样配置完成之后，Hadoop的jmx信息就被采集到指定的端口中了，接下来我们可以在网页上测试一下我们的采集数据，访问地址就是前面配置的端口 <code>localhost:27001</code></p>
<img src="/2021/11/03/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JMX-Prometheus-Grafana%E7%9B%91%E6%8E%A7Hadoop%E9%9B%86%E7%BE%A4-Hbase%E9%9B%86%E7%BE%A4/27001.png" class="" title="中二是最后的热血">    

<p>同理我们的Yarn相关信息采集也要在yarn-env.sh中配置上面类似的代码，其中同样要注意区分端口号，并且不要同时出现两个-javaagent，将两个javaagent放到不同的if else中。</p>
<h2 id="Hbase集群配置"><a href="#Hbase集群配置" class="headerlink" title="Hbase集群配置"></a>Hbase集群配置</h2><p>因为我是启动的单机版本的Hbase，所以我只配置<code>HBASE_MASTER_OPTS</code>和<code>HBASE_JMX_BASE</code>选项，如果是集群模式可能还需要配置<code>HBASE_REGIONSERVER_OPTS</code>，将下面内容替换成自己的文件路径然后添加到hbase-env.sh的尾部。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HBASE_JMX_BASE=&quot;-Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false&quot;</span><br><span class="line">export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=20101 -javaagent:$HBASE_HOME/lib/jmx_prometheus_javaagent-0.16.1.jar=27000:$HBASE_HOME/conf/hbase_jmx_config.yaml&quot;</span><br></pre></td></tr></table></figure>

<p>这样我们就可以通过指定端口访问各个集群的JMX信息了，下一步就是配置Prometheus将数据导入到时序数据库中。</p>
<h2 id="配置Prometheus"><a href="#配置Prometheus" class="headerlink" title="配置Prometheus"></a>配置Prometheus</h2><p>打开Prometheus配置文件，并且在后面增加关于Hadoop集群的NameNode，DataNode以及Hbase的jmx数据配置，增加如下代码，重启普罗米修斯服务。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">- job_name: &quot;hbase&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&quot;localhost:27000&quot;]</span><br><span class="line">      labels:</span><br><span class="line">          instance: localhost</span><br><span class="line">  - job_name: &quot;hadoop namenode&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&quot;localhost:27001&quot;]</span><br><span class="line">      labels:</span><br><span class="line">          instance: localhost</span><br><span class="line">  - job_name: &quot;hadoop datanode&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&quot;localhost:27002&quot;]</span><br><span class="line">      labels:</span><br><span class="line">          instance: localhost</span><br></pre></td></tr></table></figure>

<p>我们打开Prometheus的页面查看对应的target，查看我们配置的任务，如果出现下面几个选项并且是绿色的说明是正常的，打开采集的结果网址会发现其中Prometheus采集的指标名称相比于原先集群50070/jmx的指标名称是经过处理的，比如Prometheus中有一个指标叫做<code>hadoop_namenode_memnonheapmaxm</code>他在50070/jmx中是名称是<code>memnonheapmaxm</code>，然后前面加上service name等，其中的匹配规则应该在插件的配置文件<code>prometheus_config.yml</code>中设置的，详细可以看<a class="link"   href="https://github.com/prometheus/jmx_exporter" >插件地址<i class="fas fa-external-link-alt"></i></a>。</p>
<img src="/2021/11/03/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8JMX-Prometheus-Grafana%E7%9B%91%E6%8E%A7Hadoop%E9%9B%86%E7%BE%A4-Hbase%E9%9B%86%E7%BE%A4/prometheus.png" class="" title="中二是最后的热血">

<p>最后选出来我们需要的指标之和在Grafana中展示出来即可，具体方法这里不再展示，可以参考<a class="link"   href="https://www.daimajiaoliu.com/daima/4796909c8100406" >这篇教程<i class="fas fa-external-link-alt"></i></a></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>JMX_Expoter</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title>如何备份hexo博客</title>
    <url>/2022/03/29/%E5%A6%82%E4%BD%95%E5%A4%87%E4%BB%BDhexo%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h1 id="备忘"><a href="#备忘" class="headerlink" title="备忘"></a>备忘</h1><p>1.需要将整个博客文件夹和github上一个新项目关联起来 命令类似git set-url…<br>2.需要删除你使用的主题主文件夹中的.git目录 否则会被看做是一个子module<br>3.git push origin master推送上去<br>4.git pull origin master拉下来 并且还需要用npm下载一些包</p>
<p>npm install hexo-cli<br>npm install<br>npm install hexo-deployer-git</p>
<p>不用再执行hexo init命令</p>
<p>参考:<br><a class="link"   href="https://lrscy.github.io/2018/01/26/Hexo-Github-Backup/" >https://lrscy.github.io/2018/01/26/Hexo-Github-Backup/<i class="fas fa-external-link-alt"></i></a><br><a class="link"   href="https://finisky.github.io/2020/09/06/hexobackuptogithub/" >https://finisky.github.io/2020/09/06/hexobackuptogithub/<i class="fas fa-external-link-alt"></i></a></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title>使用grafana监控预警表格类型数据</title>
    <url>/2021/10/20/%E4%BD%BF%E7%94%A8grafana%E7%9B%91%E6%8E%A7%E9%A2%84%E8%AD%A6%E8%A1%A8%E6%A0%BC%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>监控clickhouse服务，我们使用的是grafana+grafana-clickhouse插件，grafana只能报警graph类型数据，不能报警类似Table样式的数据，反应出来就是panel有没有alter这个Tab</p>
<span id="more"></span>

<img src="/2021/10/20/%E4%BD%BF%E7%94%A8grafana%E7%9B%91%E6%8E%A7%E9%A2%84%E8%AD%A6%E8%A1%A8%E6%A0%BC%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE/alter%E5%9B%BE%E6%A0%87.png" class="" title="中二是最后的热血">

<p>对于graph类型数据比如CPU使用率，或者系统IO使用率的情况很容易就可以进行预警操作，但是对于一些表格数据，比如磁盘使用情况，如下图这种表格类型的数据就不能很好地支持预警，这时候需要我们对系统表简单的改造或者改写我们的查询SQL。</p>
<img src="/2021/10/20/%E4%BD%BF%E7%94%A8grafana%E7%9B%91%E6%8E%A7%E9%A2%84%E8%AD%A6%E8%A1%A8%E6%A0%BC%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE/%E6%B2%A1%E6%9C%89alter.png" class="" title="中二是最后的热血">

<h2 id="用’数据点’来预警"><a href="#用’数据点’来预警" class="headerlink" title="用’数据点’来预警"></a>用’数据点’来预警</h2><p>想要数据成graph或者Time series类型数据展示，我们肯定需要数据中存在一个时间字段，最好是连续的，首先我们使用的是grafana-clickhouse插件，查询的System.parts系统表来获取各个表大小然后求和，我们对parts表进行desc操作，查看其中的时间字段，发现有四个分别是‘modification_time’，‘remove_time’，‘min_time’，‘max_time’，四个时间字段分别的含义如下</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">remove_time (DateTime) – 数据片段变为非激活状态的时间。</span><br><span class="line">min_time (DateTime) – 数据片段中日期和时间键的最小值。</span><br><span class="line">max_time(DateTime) – 数据片段中日期和时间键的最小值。</span><br><span class="line">modification_time (DateTime) – 修改包含数据片段的目录的时间。这通常对应于创建数据部分的时间。</span><br></pre></td></tr></table></figure>

<p>有点蒙，没事我们直接选十条数据简单看一看</p>
<img src="/2021/10/20/%E4%BD%BF%E7%94%A8grafana%E7%9B%91%E6%8E%A7%E9%A2%84%E8%AD%A6%E8%A1%A8%E6%A0%BC%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE/%E6%97%B6%E9%97%B4%E7%BB%93%E6%9E%9C.png" class="" title="中二是最后的热血">

<p>可以看出<code>modification_time</code>就是我们想要的时间字段，代表了最后一次修改的时间，这个是最合适的了，当然更合适的是连续时间字段，但是这里没有。有了时间字段我们需要做的就是写查询SQL了，我们在grafana中写下如下的查询SQL。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    <span class="built_in">max</span>(modification_time) <span class="keyword">as</span> t,</span><br><span class="line">    (<span class="built_in">sum</span>(if(active, bytes, <span class="number">0</span>))) <span class="keyword">AS</span> size_on_disk</span><br><span class="line"><span class="keyword">FROM</span> monitor.parts</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> size_on_disk <span class="keyword">desc</span></span><br></pre></td></tr></table></figure>

<p>时间我使用的是最大的修改时间，这样的话就是一个时间点，这也是最大的缺点，呈现出来的结果是这样的</p>
<img src="/2021/10/20/%E4%BD%BF%E7%94%A8grafana%E7%9B%91%E6%8E%A7%E9%A2%84%E8%AD%A6%E8%A1%A8%E6%A0%BC%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE/%E7%9B%91%E6%8E%A7%E7%BB%93%E6%9E%9C.png" class="" title="中二是最后的热血">

<p>这样设计的思想是<strong>这个graph只是为了预警的作用，如果整体磁盘容量超过了我们设置的限制，比如80%就会报警，起到一个通知的作用，然后上线grafana去查看各个表的详细容量情况，再做出一些操作，比如删除某些分区数据或者扩容等</strong></p>
<h2 id="改变查询的系统表"><a href="#改变查询的系统表" class="headerlink" title="改变查询的系统表"></a>改变查询的系统表</h2><p>第二种方法是修改我们查询的系统表，首先我们查询的是system.parts表，其中因为没有合适的连续时间字段导致我们不能很好地创建关于CK磁盘使用情况的时序数据，那换一种思维，我们自己来手动创建一个满足条件的表呢？需要满足的条件有以下几个。<br>（1）有连续的时间字段<br>（2）每个时间字段有对应的CK占用磁盘大小<br>连续的时间字段应该怎么搞定呢，我这里使用的是Linux自带的Crontab，每隔30s通过<code>clickhouse-client</code>去查询一次CK占用磁盘的总大小，然后和当前时间字段一起插入到一个新表即可，下面是我运行一段时间的效果，可以看到这次是连续的了，如果怕产生的数据条数太多可以给数据设置上TTL，这样就完成了我们想要的效果，同样也可以进行预警了。</p>
<img src="/2021/10/20/%E4%BD%BF%E7%94%A8grafana%E7%9B%91%E6%8E%A7%E9%A2%84%E8%AD%A6%E8%A1%A8%E6%A0%BC%E7%B1%BB%E5%9E%8B%E6%95%B0%E6%8D%AE/%E8%BF%9E%E7%BB%AD%E7%9B%91%E6%8E%A7.png" class="" title="中二是最后的热血">

<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>ClickHouse</tag>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>学习clickhouse详细知识</title>
    <url>/2021/10/08/%E5%AD%A6%E4%B9%A0clickhouse%E8%AF%A6%E7%BB%86%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p>ClickHouse基础应用与内部原理</p>
<span id="more"></span>

<h1 id="MergeTree原理解析"><a href="#MergeTree原理解析" class="headerlink" title="MergeTree原理解析"></a>MergeTree原理解析</h1><p>MergeTree是家族中最基础的表引擎，提供了主键索引、数据分区、数据副本和数据采样等基本能力，而家族中其他的表引擎则在MergeTree的基础之上各有所长。</p>
<img src="/2021/10/08/%E5%AD%A6%E4%B9%A0clickhouse%E8%AF%A6%E7%BB%86%E7%9F%A5%E8%AF%86/yinqing.png" class="" title="中二是最后的热血">

<img src="/2021/10/08/%E5%AD%A6%E4%B9%A0clickhouse%E8%AF%A6%E7%BB%86%E7%9F%A5%E8%AF%86/mtreejianbiao.png" class="" title="中二是最后的热血">
<p>MergeTree建表语句中重要的参数：</p>
<p>（1）PARTITION BY [选填]：分区键，用于指定表数据以何种标准进行分区。分区键既可以是单个列字段，也可以通过元组的形式使用多个列字段，同时它也支持使用列表达式。如果不声明分区键，则ClickHouse会生成一个名为all的分区。合理使用数据分区，可以有效减少查询时数据文件的扫描范围。</p>
<p>（2）ORDER BY [必填]：排序键，用于指定在一个数据片段内，数据以何种标准排序。默认情况下主键（PRIMARY KEY）与排序键相同。排序键既可以是单个列字段，例如ORDER BY CounterID，也可以通过元组的形式使用多个列字段，例如ORDERBY（CounterID,EventDate）。当使用多个列字段排序时，以ORDERBY（CounterID,EventDate）为例，在单个数据片段内，数据首先会以CounterID排序，相同CounterID的数据再按EventDate排序。</p>
<p>（3）PRIMARY KEY [选填]：主键，顾名思义，声明后会依照主键字段生成一级索引，用于加速表查询。默认情况下，主键与排序键(ORDER BY)相同，所以通常直接使用ORDER BY代为指定主键，无须刻意通过PRIMARY KEY声明。所以在一般情况下，在单个数据片段内，数据与一级索引以相同的规则升序排列。与其他数据库不同，MergeTree主键允许存在重复数据（ReplacingMergeTree可以去重）。</p>
<h2 id="数据存储结构"><a href="#数据存储结构" class="headerlink" title="数据存储结构"></a>数据存储结构</h2><p>​    MergeTree中的主键一般通过order by指定，主键可以是单个字段也可以是多个字段，根据主键建立索引，这里涉及到两个参数<code>index_granularity</code>以及<code>enable_mixed_granularity_parts</code>，前者是指定我们创建稀疏索引时候的索引间隔，默认是8192，后者是我们选择是否开启根据写入数据体量大小的自适应索引的参数，默认是开启的，两者支持同时开启并同时生效，详细参考这篇<a class="link"   href="https://blog.csdn.net/nazeniwaresakini/article/details/109143116" >文章<i class="fas fa-external-link-alt"></i></a>，设置<code>index_granularity_bytes=0</code>即为关闭自适应索引</p>
<p> 数据存储结构主要文件有以下几个，.idx文件是一级索引文件，.bin是数据文件，.mrk是数据标记文件，首先根据idx稀疏索引文件找到要读取的数据段，因为idx文件和mrk文件是一一对应的，找到下一步找到mrk文件中压缩后偏移量和压缩前偏移量</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>ClickHouse</tag>
      </tags>
  </entry>
  <entry>
    <title>谓词下推</title>
    <url>/2022/06/07/%E8%B0%93%E8%AF%8D%E4%B8%8B%E6%8E%A8/</url>
    <content><![CDATA[<p>学习谓词下推相关笔记</p>
<span id="more"></span>

<h1 id="谓词下推"><a href="#谓词下推" class="headerlink" title="谓词下推"></a>谓词下推</h1><h2 id="谓词"><a href="#谓词" class="headerlink" title="谓词"></a>谓词</h2><p>​    概念：计算机科学中谓词解释，谓词不是像语言中的某一种词性，比如英语中的动词或者名词，谓词是一个返回值为真、假、未知(True/False/Unknow)的函数，一般我们以元素列表的形式应用于给定的谓词函数，并且根据函数返回值决定是否返回该元素，这个函数就叫做谓词。</p>
<p>​    SQL中哪些常见的谓词：Like、Between、Is Null、Is Not Null 等等</p>
<h2 id="下推"><a href="#下推" class="headerlink" title="下推"></a>下推</h2><p>​    概念：<strong>将过滤表达式尽可能移动至靠近数据源的位置，以使真正执行时能直接跳过无关的数据。</strong></p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># A,B 两表进行<span class="keyword">join</span>操作 并且有过滤条件 A.a <span class="operator">&gt;</span> <span class="number">10</span> <span class="keyword">and</span> B.b <span class="operator">&lt;</span> <span class="number">100</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> A <span class="keyword">Join</span> B <span class="keyword">on</span> A.id <span class="operator">=</span> B.id <span class="keyword">where</span> A.a <span class="operator">&gt;</span> <span class="number">10</span> <span class="keyword">and</span> B.b <span class="operator">&lt;</span> <span class="number">100</span>;</span><br></pre></td></tr></table></figure>

<p>​    执行的时候会先将过滤条件尽可能的前提，比如到对表进行TableScan的时候进行，在提取每一行数据的时候会进行一个判断该表是否满足条件，如果不满足直接不会读取，最后再进行Join操作，这样可以大大降低数据量，比如上面例子我们执行的时候优化筛选条件 A.a &gt; 10 and B.b &lt; 100 放到扫描表的时候进行，可以提高性能。</p>
<p>​    像Parquet这种特殊的格式对谓词下推做了特殊的处理 <a class="link"   href="https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/cdh_ig_predicate_pushdown_parquet.html" >Parquet谓词下推参考连接<i class="fas fa-external-link-alt"></i></a></p>
<p>​    这块值得展开学习：</p>
<ol>
<li>Parquet谓词下推如何优化</li>
<li>行存储数据库以及列存储数据库谓词下推的方法不同。</li>
</ol>
<h2 id="Outer-Join中的谓词下推"><a href="#Outer-Join中的谓词下推" class="headerlink" title="Outer Join中的谓词下推"></a>Outer Join中的谓词下推</h2><p>​    如果阅读英文文档没有问题最好阅读一遍<a class="link"   href="https://cwiki.apache.org/confluence/display/Hive/OuterJoinBehavior" >官方文档<i class="fas fa-external-link-alt"></i></a></p>
<p>​    在弄清楚Hive如何处理谓词下推之前需要搞清楚四个概念：</p>
<p>​    <strong>Preserved Row table</strong>：Outer Join操作中必须返回所有行的表，例如left join的时候左表就是Preserved Row table，相当于固定住了一个表的内容</p>
<p>​    <strong>Null Supplying table</strong>：Outer Join操作中<strong>不用</strong>必须返回所有行的表，比如left join的时候左表是全部都要返回，当右表不满足on条件的时候他就会返回null，所以形象的称为Null Supplying Table</p>
<p>​    <strong>During Join predicate</strong>：指join…on…中的谓词</p>
<p>​    <strong>After Join predicate</strong>：指在join操作后，where中的谓词</p>
<p>了解上面四个概念之后，开始看一下Hive的谓词下推原理概念(Predicate Pushdown PPD)</p>
<p>Hive中的谓词下推主要有两个原理，下面我们讲解这两个原理</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Hive Predicate Pushdown</span></span><br><span class="line">During Join predicates cannot be pushed past Preserved Row tables.</span><br><span class="line">After Join predicates cannot be pushed past Null Supplying tables.</span><br></pre></td></tr></table></figure>

<ol>
<li><p><strong>During Join predicates</strong> cannot be pushed past <strong>Preserved Row tables</strong>.</p>
<p>在join..on…条件中Preserved Row tables表的谓词不能下推，举一个例子就是left join的时候on中的左表的条件不能下推。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># E.eid<span class="operator">=</span><span class="string">&#x27;HZ001&#x27;</span>不能下推 因为此时左表需要全部保留</span><br><span class="line"><span class="keyword">select</span> ename,dept_name <span class="keyword">from</span> E <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> D <span class="keyword">on</span> ( E.dept_id <span class="operator">=</span> D.dept_id <span class="keyword">and</span> E.eid<span class="operator">=</span><span class="string">&#x27;HZ001&#x27;</span>);</span><br></pre></td></tr></table></figure></li>
<li><p><strong>After Join predicates</strong> cannot be pushed past <strong>Null Supplying tables</strong>.</p>
<p>在After Join predicate中 Null Supplying tables表的谓词是不能下推的，这个也比较好理解，还是以left join举例，左关联的时候左表全部保留，右表数据是否保留应该取决于on中的关联条件是否满足，其次取决于where中右表条件，而不能先使用where中右表的条件过滤右表数据。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># D.dept_id<span class="operator">=</span><span class="string">&#x27;D001&#x27;</span> 不能下推 因为此时右表数据应取决于<span class="keyword">on</span>中关联</span><br><span class="line"><span class="keyword">select</span> ename,dept_name <span class="keyword">from</span> E <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> D <span class="keyword">on</span> E.dept_id <span class="operator">=</span> D.dept_id <span class="keyword">where</span> D.dept_id<span class="operator">=</span><span class="string">&#x27;D001&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
<li><p>Full Outer Join形式是任何条件都不能下推谓词，因为需要左右两表全都保留数据</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"># <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> 任何条件都不能下推 下面四个都不能下推</span><br><span class="line"><span class="keyword">select</span> ename,dept_name <span class="keyword">from</span> E <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> D <span class="keyword">on</span> ( E.dept_id <span class="operator">=</span> D.dept_id <span class="keyword">and</span> E.eid<span class="operator">=</span><span class="string">&#x27;HZ001&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> ename,dept_name <span class="keyword">from</span> E <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> D <span class="keyword">on</span> E.dept_id <span class="operator">=</span> D.dept_id <span class="keyword">where</span> E.eid<span class="operator">=</span><span class="string">&#x27;HZ001&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> ename,dept_name <span class="keyword">from</span> E <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> D <span class="keyword">on</span> ( E.dept_id <span class="operator">=</span> D.dept_id <span class="keyword">and</span> D.dept_id<span class="operator">=</span><span class="string">&#x27;D001&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> ename,dept_name <span class="keyword">from</span> E <span class="keyword">full</span> <span class="keyword">outer</span> <span class="keyword">join</span> D <span class="keyword">on</span> E.dept_id <span class="operator">=</span> D.dept_id <span class="keyword">where</span> D.dept_id<span class="operator">=</span><span class="string">&#x27;D001&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Inner-Join中的谓词下推"><a href="#Inner-Join中的谓词下推" class="headerlink" title="Inner Join中的谓词下推"></a>Inner Join中的谓词下推</h2><p>​    对于inner join情况，因为左右表都不是Preserved Row table 或者Null Supplying table，都需要被条件过滤，此时During Join predicate条件的谓词全都可以前提，也就是join on中条件，同时After Join predicate，也就是where中的条件也可以被前置。</p>
<p>参考文献：</p>
<p><a class="link"   href="https://blog.csdn.net/strongyoung88/article/details/81156271" >https://blog.csdn.net/strongyoung88/article/details/81156271<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://cwiki.apache.org/confluence/display/Hive/OuterJoinBehavior" >https://cwiki.apache.org/confluence/display/Hive/OuterJoinBehavior<i class="fas fa-external-link-alt"></i></a></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>Sql</tag>
      </tags>
  </entry>
  <entry>
    <title>数据湖基础知识以及Mac安装Iceberg教程</title>
    <url>/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/</url>
    <content><![CDATA[<p><strong>计算机科学领域的任何问题都可以通过增加一个间接地中间层来解决</strong></p>
<span id="more"></span>

<img src="/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/%E6%95%B0%E6%8D%AE%E6%B9%96.png" class="" title="中二是最后的热血">



<p>​    关于数据湖为什么会出现我觉得有一句话概括的非常好，</p>
<p><code>大数据领域发展至今已经经历了相当长时间的发展和探索，虽然大数据技术的出现和迭代降低了用户处理海量数据的门槛，但是有一个问题不能忽视，数据格式对不同引擎适配的对接</code></p>
<p>这句话是什么意思呢？我们在使用不同的引擎进行计算时，需要将数据根据引擎进行适配。这是相当棘手的问题，比如说我用Spark进行计算，但是底层的文件可能是TXT存储的，可能是Hive ORC存储的，或者是经过HDFS的一些压缩算法处理的数据，那么我Spark就需要支持读取TXT文件，读取ORC文件，读取snappy文件 and so on.为了将计算引擎和数据剥离开来，结合的不那么紧密，大牛们出现了一种新的解决方案：</p>
<p><code>介于上层计算引擎和底层存储格式之间的一个中间层。</code></p>
<p>这个中间层不是数据存储的方式，只是定义了数据的元数据组织方式，并且向引擎层面提供统一的类似传统数据库</p>
<p>中”表”的语义。它的底层仍然是Parquet、ORC等存储格式。重新应证了那句话。</p>
<p><strong>计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决</strong>。</p>
<p><code>基于此，Netflix开发了Iceberg数据湖（更主要的是基于Hive的数据湖方案不能满足需求），目前已经是Apache的顶级项目，除此之外还有Hudi，delta数据湖方案。</code></p>
<img src="/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/%E6%95%B0%E6%8D%AE%E6%B9%96%E6%A6%82%E5%BF%B5.png" class="" title="中二是最后的热血">



<p>​    我们可以看到数据湖是一个在计算平台和存储平台中间的一个类似中间件的东西，用于解决多种数据源，多种计算引擎</p>
<p>之间的连接关系，数据湖中的数据可以包括来自于关系型数据库中的结构化数据（行和列）、半结构化数据（如CSV、日</p>
<p>志、XML、JSON）、非结构化数据（如email、文档、PDF等）和二进制数据（如图像、音频、视频）。除此之外引申出</p>
<p>来的还有关于元数据的管理，源数据的多种Schema种类样式都要满足。概括一下成熟的数据湖应该满足以下需求：</p>
<p>（1）需要支持比较丰富的数据 Schema 的组织；</p>
<p>（2）它在注入的过程中要支撑<strong>实时的数据查询</strong>，所以<strong>需要 ACID</strong> 的保证，确保不会读到一些还没写完的中间状态的脏数据；</p>
<p>（3）例如日志这些有可能临时需要改个格式，或者加一列。类似这种情况，需要避免像传统的数仓一样，可能要把所有的数据重新提出来写一遍，重新注入到存储；而是需要一个轻量级的解决方案来达成需求。</p>
<h3 id="Lambda架构"><a href="#Lambda架构" class="headerlink" title="Lambda架构"></a>Lambda架构</h3><p>​        在继续了解数据湖之前我们先来了解一下相关大数据架构，主要分为两种架构，分别是lambda &amp; kappa，我们先来</p>
<p>说lambda架构。lambda是一种比较传统的大数据架构，广泛应用于现在的大数据数仓平台中，其中主要的特点就是流批</p>
<p>分离，比如我们批处理数据会在每天夜间12点运行一次，计算昨天一天的数据然后覆盖到druid或者clickhouse中，保证数</p>
<p>据的准确性，但是对于今天的实时数据查询来说我们就只能通过实时程序来计算好之后存到druid或者clickhouse中，因为</p>
<p>离线层采用分布式存储系统，这样哪怕程序崩溃也不大可能会损伤到数据。</p>
<img src="/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/lambda%E6%9E%B6%E6%9E%84.png" class="" title="中二是最后的热血">



<p>Lambda架构优缺点如下：</p>
<p>优点：</p>
<p>（1）离线层采用分布式存储管理历史信息，即使系统崩溃也不太可能损伤到数据</p>
<p>（2）有效平衡了速度与可靠性</p>
<p>（3）容灾而兼具弹性的数据处理架构</p>
<p>缺点：</p>
<p>（1）Lambad架构维护成本很高。很显然，这种架构下数据存在两份、schema不统一、 数据处理逻辑不统一，整个数仓系统维护成本很高</p>
<p>（2）Lambda中的实时数仓中一般使用kafka当做中间件，但是kafka不支持OLAP操作，大多数业务希望可以在DWD、DWS层进行即席查询，kafka无法满足</p>
<p>（3）Lambda实时数仓kafka仅支持append不支持update/delete</p>
<h3 id="Kappa架构"><a href="#Kappa架构" class="headerlink" title="Kappa架构"></a>Kappa架构</h3><p>​        Kappa 架构不能被简单视作 Lambda 架构的替代品，相反，它是在离线层对满足业务需求不是必须实时的一个备选</p>
<p>项。该架构适合实时处理不同事件，下图展示了其整体结构</p>
<img src="/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/kappa%E6%9E%B6%E6%9E%84.png" class="" title="中二是最后的热血">



<p>其中中间的消息件一般采用kafka，因为其多分区，顺序append data，可水平拓展的特性迎合了关键需求。一般实时架构</p>
<p>选用的是Flink+Kafka架构，如果数据量极大的情况，一般kafka的过期时间设置的都比较短，比如一天甚至几小时，但是</p>
<p>如果数据出现峰值情况，导致数据积压验证的话，可能导致kafka中数据不能被及时消费出来在Kafka中过期导致数据丢</p>
<p>失。</p>
<p>Kappa架构优缺点：</p>
<p>优点：</p>
<p>（1）无需离线层</p>
<p>（2）只有当代码变更时才需要重新处理</p>
<p>（3）可以使用固定内存进行部署</p>
<p>（4）可用于具备水平扩展性的系统</p>
<p>（5）机器学习是实时的，所以所需资源更少</p>
<p>缺点：</p>
<p>（1）缺少离线层可能导致数据处理或数据库更新时发生错误，所以往往需要异常管理器来调解矛盾，恢复数据。</p>
<p>（2）数据链路更加复杂，如果我们需要对DWD层的实时数仓数据进行数据分析的时候就需要将DWD层的Kafka中的数据</p>
<p>写入到ClickHouse或者hive中，增加了链路复杂性</p>
<p>（3）Kappa架构是严重依赖于消息队列的，消息队列本身的准确性严格依赖它上游数据的顺序，但是，消息队列越多，</p>
<p>发生乱序的可能性越大。</p>
<p>上面分别讲述了两种架构的优缺点，这时候有人就提出了，有没有一种技术能够保证数据高效回溯能力，架构稳定，支持</p>
<p>数据更新，支持数据的流批读写，做到分钟级别的数据接入。</p>
<p>于是乎，各大厂商针对上面的需求，分别推出了自己的数据湖方案，现在基本呈现三足鼎立的局势，Delta，Hudi，以及Iceberg。</p>
<h2 id="Iceberg"><a href="#Iceberg" class="headerlink" title="Iceberg"></a>Iceberg</h2><p>​    上面我们了解了关于一个数据湖应该具有的特性以及lambda架构和Kappa架构，下面我们来看看Netflix发起的数据湖项</p>
<p>目Iceberg，Iceberg的表数据架构图：</p>
<img src="/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/Iceberg%E8%A1%A8%E6%95%B0%E6%8D%AE%E7%BB%84%E7%BB%87%E6%9E%B6%E6%9E%84.png" class="" title="中二是最后的热血">



<p>有几点需要注意一下：</p>
<p>（1）新快照不会覆盖旧快照，旧的快照中依然保存了早期数据的Manifest File，新旧快照共同组成了表的快照Metadata</p>
<p>的一部分。</p>
<p>（2）数据文件datafile支持多种格式如parque,orc,avro等格式。</p>
<p>（3）有了快照，读数据的时候只能读到快照所能引用到的数据，还在写的数据不会被快照引用到，也就不会读到脏数</p>
<p>据。多个快照会共享以前的数据文件，通过共享这些 Manifest File 来共享之前的数据。</p>
<p>（4）Manifest List再往上是快照元数据（快照Metadata），记录了当前或者历史上表格 Scheme 的变化、分区的配置、</p>
<p>所有快照 Manifest File 路径、以及当前快照是哪一个。同时，Iceberg 提供命名空间以及表格的抽象，做完整的数据组织</p>
<p>管理。</p>
<p>有了Iceberg这种神器之后，新一代的数仓基于Flink以及Iceberg的设计横空出世</p>
<img src="/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/%E6%96%B0%E6%95%B0%E4%BB%93%E6%9E%B6%E6%9E%84.png" class="" title="中二是最后的热血">



<p>这样无论是流处理还是批处理，我们的存储都是进入到了Iceberg数据湖中，不用再分开做流批处理的两套代码，以及不用</p>
<p>大量依靠kafka这种消息中间件，下面我们总结一下这个新架构的优点。</p>
<p>（1）解决Kafka存储数据量有限的问题，kafka使用成本较高，数据湖所有数据都是基于HDFS实现的文件管理系统，数据</p>
<p>量可以巨大基本不用担心</p>
<p>（2）DWD层，ODS层依然支持OLAP查询，原先的实时数仓如果想查询这两层的数据需要通过Flink将数据导入到OLAP查</p>
<p>询平台（ClickHouse、Druid、Hive）</p>
<p>（3）批流存储都是基于Iceberg以及HDFS之后，就可以完全复用一套数据质量管理体系，数据血缘关系同样也只做一套</p>
<p>即可</p>
<p>（4）依靠数据湖架构设计的实时数仓可以看做是一种Kappa架构的升级，kappa的优点在数据湖数仓中依然存在，</p>
<p>schema统一，数据处理逻辑统一，开发人员不用再维护两套代码。</p>
<p>上面基于Iceberg构建的架构看起来就像是把Lambda架构中的Kafka和HDFS结合到了一起，并且我们知道Iceberg也是可</p>
<p>以基于HDFS上存储的，那么他究竟是在HDFS上做了哪些操作让其可以作为实时数仓的存储了呢？</p>
<p>（1）<strong>支持流式写入-增量拉取</strong>，目前主要基于Flink即可实现流式写入，但是这里有个问题，因为频繁的写文件导致小文件</p>
<p>可能会增多，数据湖需要在这方面做出处理，还有就是实时处理程序需要知道<strong>哪些文件是新增的哪些是旧的</strong>，每次只需要</p>
<p>处理新增的文件即可，这个也是离线数仓做不到实时的关键原因之一，离线数仓的做法就是处理完一整批数据之和给下游</p>
<p>说我这批数据处理完了，你可以使用了。那Iceberg如何实现读写增量拉取的呢？这个需要我们上面提到的表数据组织架</p>
<p>构。</p>
<img src="/2021/10/20/%E6%95%B0%E6%8D%AE%E6%B9%96%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5%E4%B8%8E%E7%90%86%E8%AE%BA%EF%BC%8CIceberg/Iceberg%E8%AF%BB%E5%86%99API.png" class="" title="中二是最后的热血">



<p>​        其中的s0表示第一次产生的快照，s1表示第二次产生的快照，同样新快照是包含了旧快照中的数据的，根据快照中的</p>
<p>manifest文件Iceberg可以确定哪些文件是旧文件，哪些是新文件，有快照了我们就可以根据快照中的内容判断文件是新增</p>
<p>的还是原先就存在的。</p>
<p>（2）解决小文件过多的问题，目前iceberg的spark代码中有原生的合并小文件代码，flink的合并代码社区还在积极开发</p>
<p>中，详细可以看一下<a class="link"   href="https://github.com/apache/iceberg/pull/3213" >issue<i class="fas fa-external-link-alt"></i></a></p>
<h2 id="Mac安装Iceberg"><a href="#Mac安装Iceberg" class="headerlink" title="Mac安装Iceberg"></a>Mac安装Iceberg</h2><p>Versions:<br>Hive 3.1.2  (hive –version)<br>Hadoop 3.3.1  (hadoop version)<br>Flink Version: 1.11.1 (flink –version)</p>
<p><strong>注意事项</strong></p>
<p>（1）👆上面的版本是我自己本机的安装，可能其他版本也可以，但是Flink最好是选用1.11版本的，因为Iceberg官网有这</p>
<p>样一句话</p>
<p><code>Apache Iceberg supports both [Apache Flink](https://flink.apache.org/)‘s DataStream API and Table API to write records into an Iceberg table. Currently, we only integrate Iceberg with Apache Flink 1.11.x.</code></p>
<p>（2）Mac使用brew安装指定版本的flink需要将flink项目clone到本地然后找到flink的提交记录，然后安装指定版本对应的</p>
<p>commit id，详细参考<a class="link"   href="https://blog.timeline229.com/homebrew-set-software-elder-version/" >这里!<i class="fas fa-external-link-alt"></i></a>，但是需要说明的是，我这样安装同样是报错的，所以最终采用了下载版本的对应压缩包直接</p>
<p>解压在本地目录使用.</p>
<p>（3）环境变量必须要配置，我当时没有配置是运行失败了的，配上肯定是保险的，因为Mac程序员一般会用homebrew安</p>
<p>装软件，会发现不用像原先Linux，或者windows需要配置环境变量才能使用</p>
<blockquote>
<p>Mac使用Homebrew安装软件的时候，软件包的二进制文件会被创建软连接然后放到/usr/local/bin中，而Mac开机时，会自动读取该文件，使用某个命令时会根据链接文件找到命令的实际位置并执行。</p>
</blockquote>
<p>需要配置的环境变量如下所示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">注意：需要将地址换成你自己的安装地址</span></span><br><span class="line"><span class="meta">#</span><span class="bash">添加完成之后记得进行 <span class="built_in">source</span> 操作，让修改马上生效</span></span><br><span class="line">export HADOOP_HOME=/usr/local/Cellar/hadoop/3.3.1/libexec</span><br><span class="line">export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_291.jdk/Contents/Home</span><br><span class="line"><span class="meta">#</span><span class="bash">需要注意的是hadoop classpath会自动将相关包的路径打出来，不用一个个手动敲</span></span><br><span class="line">export HADOOP_CLASSPATH=hadoop classpath</span><br><span class="line">export PATH=$HADOOP_CLASSPATH:$HADOOP_HOME:$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>



<p>（4）我们后面创建HiveCatlog创建修改Hive表数据则需要连接Hive的元数据，那么就意味着需要启动Hive metastore服</p>
<p>务，我们平常对Hive metastore元数据服务没有什么感觉，下面有一段关于Hive Metastore的简短介绍，更详细信息可以</p>
<p>参考<a class="link"   href="https://zhuanlan.zhihu.com/p/100585524" >!知乎回答<i class="fas fa-external-link-alt"></i></a> &amp; <a class="link"   href="https://docs.cloudera.com/runtime/7.2.7/hive-hms-overview/topics/hive-hms-introduction.html" >!cloudera官网介绍<i class="fas fa-external-link-alt"></i></a></p>
<blockquote>
<p>Hive Metastore是Hive用来管理库表元数据的一个服务，有了它上层的服务不用再跟裸的文件数据打交道，而是可以基于结构化的库表信息构建计算框架。现在除了Hive之外很多计算框架都支持以Hive Metastore为元数据中心来查询底层Hadoop生态的数据，比如Drill, Presto, Spark等等。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">我们在Mac上启动Hive Metastore服务命令</span><br><span class="line">hive --service metastore</span><br></pre></td></tr></table></figure>



<p>了解了上面的注意事项之后我们开始下载Iceberg的jar包，<a class="link"   href="https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-flink-runtime/0.11.1/" >下载地址<i class="fas fa-external-link-alt"></i></a></p>
<p>下载好之后我们将Iceberg的Jar包(iceberg-flink-runtime-0.11.1.jar)和用来连接hive的jar包(flink-sql-connector-hive-</p>
<p>3.1.2_2.11-1.11.0.jar)放到flink目录中的lib目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">进入flink-sql，进入前记得打开Hive元数据服务</span></span><br><span class="line">./sql-client.sh embedded \\n    </span><br><span class="line">-j /Users/liu/Documents/flink-1.11.1/lib/iceberg-flink-runtime-0.11.1.jar \\n    </span><br><span class="line">-j /Users/liu/Documents/flink-1.11.1/lib/flink-sql-connector-hive-3.1.2_2.11-1.11.0.jar \\n    shell</span><br></pre></td></tr></table></figure>



<p>创建Hive的Catalog，即可以建立已HDFS为基础的数据湖表</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">Flink <span class="keyword">SQL</span><span class="operator">&gt;</span> <span class="keyword">CREATE</span> CATALOG hive_catalog <span class="keyword">WITH</span> (</span><br><span class="line"><span class="operator">&gt;</span>   <span class="string">&#x27;type&#x27;</span><span class="operator">=</span><span class="string">&#x27;iceberg&#x27;</span>,</span><br><span class="line"><span class="operator">&gt;</span>   <span class="string">&#x27;catalog-type&#x27;</span><span class="operator">=</span><span class="string">&#x27;hive&#x27;</span>,</span><br><span class="line"><span class="operator">&gt;</span>   <span class="string">&#x27;uri&#x27;</span><span class="operator">=</span><span class="string">&#x27;thrift://localhost:9083&#x27;</span>,</span><br><span class="line"><span class="operator">&gt;</span>   <span class="string">&#x27;clients&#x27;</span><span class="operator">=</span><span class="string">&#x27;5&#x27;</span>,</span><br><span class="line"><span class="operator">&gt;</span>   <span class="string">&#x27;property-version&#x27;</span><span class="operator">=</span><span class="string">&#x27;1&#x27;</span>,</span><br><span class="line"><span class="operator">&gt;</span>   <span class="string">&#x27;warehouse&#x27;</span><span class="operator">=</span><span class="string">&#x27;hdfs://localhost:9000/user/hive/warehouse&#x27;</span></span><br><span class="line"><span class="operator">&gt;</span> );</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>​        本文主要记录数据湖的基础知识，以及如何在Mac上安装Iceberg，其中的注意事项是我出现过的问题，如果有什么疑问或者不理解的地方欢迎随时交流。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>data Lake</tag>
        <tag>Iceberg</tag>
      </tags>
  </entry>
  <entry>
    <title>Grafana</title>
    <url>/2022/07/12/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86Grafana%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<p>配置grafana的免密反向代理 访问dashboard &amp; panel</p>
<span id="more"></span>

<h3 id="grafana免密"><a href="#grafana免密" class="headerlink" title="grafana免密"></a>grafana免密</h3><p>什么叫做免密反向代理？我们在grafana中分享出来的panel的link，是下面样式的</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://localhost:3000/gf/d/173XYzd7z/clickhouse-performance-monitor?orgId=1&amp;from=1657692410493&amp;to=1657694210493&amp;viewPanel=30</span><br></pre></td></tr></table></figure>

<p>如果我们在别的机器上打开就需要我们输入账号和密码，如果我做了一个监控页面，内嵌了grafana的页面，我点开我的监控，还需要手动输入grafana的账号密码，这就比较难受，有没有什么办法不用输入呢？</p>
<p>这就需要 grafana 的 <a class="link"   href="https://grafana.com/docs/grafana/v7.5/http_api/create-api-tokens-for-org/" >API tokens<i class="fas fa-external-link-alt"></i></a>，这个API token 官方给出的使用方法是我们可以通过命令行来操作dashboard 或者 alert预警之类的，但是我们只需要用来认证即可。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 按照文档在本机上生成key的过程</span></span><br><span class="line">curl -X POST -H &quot;Content-Type: application/json&quot; -d &#x27;&#123;&quot;name&quot;:&quot;apiorg&quot;&#125;&#x27; http://admin:admin@localhost:3000/api/orgs</span><br><span class="line">&#123;&quot;message&quot;:&quot;Organization created&quot;,&quot;orgId&quot;:2&#125;%</span><br><span class="line"></span><br><span class="line">curl -X POST -H &quot;Content-Type: application/json&quot; -d &#x27;&#123;&quot;loginOrEmail&quot;:&quot;admin&quot;, &quot;role&quot;: &quot;Admin&quot;&#125;&#x27; http://admin:admin@localhost:3000/api/orgs/2/users</span><br><span class="line"></span><br><span class="line">curl -X POST http://admin:admin@localhost:3000/api/user/using/1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 上面就是我们生成的key，注意只出现一次，记录下来</span></span><br><span class="line">curl -X POST -H &quot;Content-Type: application/json&quot; -d &#x27;&#123;&quot;name&quot;:&quot;apikeycurl&quot;, &quot;role&quot;: &quot;Admin&quot;&#125;&#x27; http://admin:admin@localhost:3000/api/auth/keys</span><br><span class="line">&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;apikeycurl&quot;,&quot;key&quot;:&quot;eyJrIjoidTJPdm1mbDRBZHlzaVBTNHV1cmZiMjZ6N3l3ZG9VbTQiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoyfQ==&quot;&#125;%</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过key访问我们的share link</span></span><br><span class="line">curl -H &quot;Authorization: Bearer eyJrIjoidTJPdm1mbDRBZHlzaVBTNHV1cmZiMjZ6N3l3ZG9VbTQiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoyfQ==&quot; http://localhost:3000/gf/d/173XYzd7z/clickhouse-performance-monitor?</span><br></pre></td></tr></table></figure>

<h3 id="grafana打开反向代理配置"><a href="#grafana打开反向代理配置" class="headerlink" title="grafana打开反向代理配置"></a>grafana打开反向代理配置</h3><p>参考 <a class="link"   href="https://grafana.com/tutorials/run-grafana-behind-a-proxy/" >官方文档<i class="fas fa-external-link-alt"></i></a> 设置我们在grafan中的代理配置</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> vim conf/defaults.ini</span></span><br><span class="line">domain = localhost</span><br><span class="line">root_url = %(protocol)s://%(domain)s:%(http_port)s/gf/</span><br><span class="line">serve_from_sub_path = true</span><br></pre></td></tr></table></figure>

<h3 id="反向代理的两种方式"><a href="#反向代理的两种方式" class="headerlink" title="反向代理的两种方式"></a>反向代理的两种方式</h3><h5 id="nginx配置反向代理"><a href="#nginx配置反向代理" class="headerlink" title="nginx配置反向代理"></a>nginx配置反向代理</h5><p>安装nginx这里不再赘述，详细可以参考<a class="link"   href="https://juejin.cn/post/6986190222241464350" >这里<i class="fas fa-external-link-alt"></i></a></p>
<p>我的nginx使用的homebrew安装，首先查看nginx相关信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(base) ➜  brew info nginx</span><br><span class="line">nginx: stable 1.21.3 (bottled), HEAD</span><br><span class="line">HTTP(S) server and reverse proxy, and IMAP/POP3 proxy server</span><br><span class="line">https://nginx.org/</span><br><span class="line">/opt/homebrew/Cellar/nginx/1.21.3 (26 files, 2.2MB) *</span><br><span class="line">  Poured from bottle on 2022-06-16 at 17:13:49</span><br><span class="line">From: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/nginx.rb</span><br><span class="line">License: BSD-2-Clause</span><br><span class="line">==&gt; Dependencies</span><br><span class="line">Required: openssl@1.1 ✔, pcre ✔</span><br><span class="line">==&gt; Options</span><br><span class="line">--HEAD</span><br><span class="line">	Install HEAD version</span><br><span class="line">==&gt; Caveats</span><br><span class="line">Docroot is: /opt/homebrew/var/www</span><br><span class="line"></span><br><span class="line">The default port has been set in /opt/homebrew/etc/nginx/nginx.conf to 8080 so that</span><br><span class="line">nginx can run without sudo.</span><br><span class="line"></span><br><span class="line">nginx will load all files in /opt/homebrew/etc/nginx/servers/.</span><br><span class="line"></span><br><span class="line">To restart nginx after an upgrade:</span><br><span class="line">  brew services restart nginx</span><br><span class="line">Or, if you don&#x27;t want/need a background service you can just run:</span><br><span class="line">  /opt/homebrew/opt/nginx/bin/nginx -g daemon off;</span><br><span class="line">==&gt; Analytics</span><br><span class="line">install: 38,896 (30 days), 91,488 (90 days), 455,183 (365 days)</span><br><span class="line">install-on-request: 38,823 (30 days), 91,297 (90 days), 454,267 (365 days)</span><br><span class="line">build-error: 48 (30 days)</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">The default port has been set in /opt/homebrew/etc/nginx/nginx.conf to 8080 so that</span><br><span class="line">nginx can run without sudo.</span><br><span class="line">这行我们可以了解到默认nginx配置文件是/opt/homebrew/etc/nginx/nginx.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">修改我们的nginx.conf文件</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">user  nobody;</span></span><br><span class="line">worker_processes  1;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">error_log  logs/error.log;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">error_log  logs/error.log  notice;</span></span><br><span class="line"><span class="meta">#</span><span class="bash">error_log  logs/error.log  info;</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash">pid        logs/nginx.pid;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen       8080;</span><br><span class="line">        server_name  localhost;</span><br><span class="line">        location / &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    # 主要是下面的配置</span><br><span class="line">    server &#123;</span><br><span class="line">      listen 8888;   # 源端口</span><br><span class="line">      root /opt/homebrew/var/www;  # 根 主页目录 保持默认即可</span><br><span class="line">      location /gf &#123;</span><br><span class="line">        proxy_pass http://localhost:3000; # 目标域名以及端口</span><br><span class="line">        proxy_set_header Authorization &quot;Bearer  eyJrIjoiRWZOVElUTHRwYkt0VVgwVkNpMmppTElsUUgxOXpxZUIiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==&quot;; # 申请的grafana 组的秘钥</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    include servers/*;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面就是我们的nginx配置，主要是其中的server配置，注意秘钥配置中间都是空格没有冒号。</p>
<p>到此为止nginx配置反向代理就完成了，接下来我们看怎么用go语言来实现一个grafana反向代理</p>
<h5 id="go实现反向代理"><a href="#go实现反向代理" class="headerlink" title="go实现反向代理"></a>go实现反向代理</h5><p>加入你不想在grafana服务器上再部署一个nginx服务器，那么我们尝试用go来实现一个反向代理服务。</p>
<p>参考<a class="link"   href="https://h1z3y3.me/posts/simple-and-powerful-reverse-proxy-in-golang/" >这篇文章<i class="fas fa-external-link-alt"></i></a>，我们实现了一个简单的go服务反向代理，下面是主要的代码</p>
<figure class="highlight go"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;log&quot;</span></span><br><span class="line">	<span class="string">&quot;net/http&quot;</span></span><br><span class="line">	<span class="string">&quot;net/http/httputil&quot;</span></span><br><span class="line">	<span class="string">&quot;net/url&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// NewProxy takes target host and creates a reverse proxy</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewProxy</span><span class="params">(targetHost <span class="keyword">string</span>)</span> <span class="params">(*httputil.ReverseProxy, error)</span></span> &#123;</span><br><span class="line">	url, err := url.Parse(targetHost)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	proxy := httputil.NewSingleHostReverseProxy(url)</span><br><span class="line"></span><br><span class="line">	originalDirector := proxy.Director</span><br><span class="line">	proxy.Director = <span class="function"><span class="keyword">func</span><span class="params">(req *http.Request)</span></span> &#123;</span><br><span class="line">		originalDirector(req)</span><br><span class="line">		modifyRequest(req)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">//proxy.ModifyResponse = modifyResponse()</span></span><br><span class="line">	proxy.ErrorHandler = errorHandler()</span><br><span class="line">	<span class="keyword">return</span> proxy, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">modifyRequest</span><span class="params">(req *http.Request)</span></span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 需要将请求链接中的Host改为grafana的Host</span></span><br><span class="line">	</span><br><span class="line">  req.URL.Host = <span class="string">&quot;localhost:3000&quot;</span></span><br><span class="line">	req.Host = <span class="string">&quot;localhost:3000&quot;</span></span><br><span class="line">	fmt.Printf(<span class="string">&quot;url: %s \n&quot;</span>, req.URL) </span><br><span class="line">	fmt.Printf(<span class="string">&quot;host: %s \n&quot;</span>, req.Host)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将grafana api key 设置进请求的header中</span></span><br><span class="line">	req.Header.Set(<span class="string">&quot;Authorization&quot;</span>, <span class="string">&quot; Bearer eyJrIjoiRWZOVElUTHRwYkt0VVgwVkNpMmppTElsUUgxOXpxZUIiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==&quot;</span>)</span><br><span class="line">	req.Header.Set(<span class="string">&quot;Access-Control-Allow-Origin&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line">	req.Header.Set(<span class="string">&quot;Access-Control-Allow-Methods&quot;</span>, <span class="string">&quot;*&quot;</span>)</span><br><span class="line">	req.Header.Set(<span class="string">&quot;Access-Control-Allow-Credentials&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">errorHandler</span><span class="params">()</span> <span class="title">func</span><span class="params">(http.ResponseWriter, *http.Request, error)</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="function"><span class="keyword">func</span><span class="params">(w http.ResponseWriter, req *http.Request, err error)</span></span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Got error while modifying response: %v \n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ProxyRequestHandler handles the http request using proxy</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ProxyRequestHandler</span><span class="params">(proxy *httputil.ReverseProxy)</span> <span class="title">func</span><span class="params">(http.ResponseWriter, *http.Request)</span></span> &#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="function"><span class="keyword">func</span><span class="params">(w http.ResponseWriter, r *http.Request)</span></span> &#123;</span><br><span class="line">		proxy.ServeHTTP(w, r)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	<span class="comment">// initialize a reverse proxy and pass the actual backend server url here</span></span><br><span class="line">	proxy, err := NewProxy(<span class="string">&quot;http://localhost:3000&quot;</span>)</span><br><span class="line">	<span class="comment">//proxy, err := NewProxy(&quot;http://localhost:3000&quot;)</span></span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		<span class="built_in">panic</span>(err)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// handle all requests to your server using the proxy</span></span><br><span class="line">	http.HandleFunc(<span class="string">&quot;/&quot;</span>, ProxyRequestHandler(proxy))</span><br><span class="line">	log.Fatal(http.ListenAndServe(<span class="string">&quot;:8888&quot;</span>, <span class="literal">nil</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样我们访问localhost:8888的时候就会自动跳转到localhost:3000，到此为止两种免密grafana配置就完成了。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>Grafana</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS CheckPoint 流程源码分析</title>
    <url>/2022/10/11/HDFS-CheckPoint-%E6%B5%81%E7%A8%8B%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>Talk is cheap show me the code!</p>
<span id="more"></span>

<blockquote>
<p>HDFS 在非 HA 模式的集群下，NameNode 和 DataNode 是一个主从的架构。在这样的主从架构之下只有一台 NameNode。一台 NameNode 的好处是无需因为元数据的同步而考虑数据的一致性问题。但是，只有一台 NameNode 也会有很多的坏处，因为，单台 NameNode 的情况下会出现所有单机都会出现的问题。最简单的问题就是，当这一台 NameNode 挂掉后，整个集群将不可用了。</p>
<p> 为了解决单台 NameNode 挂掉不可用的问题，HDFS 在 2.x 版本的时候引入了 HDFS 集群的 HA 模式，也就是有了 NameNode 的主备架构。在 2.x 的版本中，HDFS 支持一主一备的架构，在 3.x 的版本中最多支持 5 个，官方推荐使用 3 个。</p>
</blockquote>
<p>上面我们知道了Hadoop集群为了解决单点故障问题，在2.x的时候引用了HA架构，之后3.x的时候增加了standy nn的数量最大为5个，在我们的集群HA开启和关闭的两种情况下我们的hdfs checkpoint也是有很大区别的，下面我们分别来从源码角度分析一下。</p>
<h2 id="HA关闭情况下HDFS-Checkpoint流程"><a href="#HA关闭情况下HDFS-Checkpoint流程" class="headerlink" title="HA关闭情况下HDFS Checkpoint流程"></a>HA关闭情况下HDFS Checkpoint流程</h2><p>首先在我们开始了解checkpoint之前，我们需要了解几个关于Hadoop元数据的基础知识 </p>
<p><strong>txid</strong></p>
<p>Hadoop为每一次操作都赋予了一个独一无二的transactionId，简称为txid，如何保证txid顺序递增，<strong>加锁</strong>，在hdfs这种高并发的系统中肯定不能普通的加锁释放操作，性能达不到需求，<a class="link"   href="https://www.modb.pro/db/122703" >hdfs如何保证txid顺序递增?<i class="fas fa-external-link-alt"></i></a></p>
<p><strong>fsimage</strong></p>
<p>fsimage文件是Hadoop系统元数据的永久性检查点，包含了系统中所有文件的目录和文件inode序列化信息</p>
<p><strong>edits</strong></p>
<p>edits日志文件存放hadoop所有操作的日志信息，操作首先会被记录到edits文件中，定时合并为fsimage文件，在没有开启HA的情况下是由secondary nn来进行合并操作，开启HA的情况下是JournalNode节点来进行合并以及同步。</p>
<p>上面三个知识点在了解完毕之后我们来看关闭HA状况下的checkpoint流程。</p>
<p>Secondary NameNode是一个周期性唤醒的守护进程，定时的来触发checkpoint操作，checkpoint条件有两个:</p>
<p>(1)超过了配置的检查点时间时长</p>
<p>(2)没有合并的操作次数超过了配置值(dfs.namenode.checkpoint.txns)默认为100w</p>
<p>打开Hadoop的SecondaryNameNode.java文件，这个文件就是2nn的启动进程，其中的main方法注释写明了在什么条件下会启动checkpoint。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// SecondaryNameNode can be started in 2 modes:</span></span><br><span class="line"><span class="comment">// 手动执行checkpoint或者获取未执行checkpoint的事务数量</span></span><br><span class="line"><span class="comment">// 1. run a command (i.e. checkpoint or geteditsize) then terminate</span></span><br><span class="line"><span class="comment">// 作为一个守护进程，开启InfoServer和CheckpointThread定期执行checkpoint</span></span><br><span class="line"><span class="comment">// 2. run as a daemon when &#123;@link #parseArgs&#125; yields no commands</span></span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> (opts != <span class="keyword">null</span> &amp;&amp; opts.getCommand() != <span class="keyword">null</span>) &#123;</span><br><span class="line">        <span class="comment">// mode 1</span></span><br><span class="line">        <span class="keyword">int</span> ret = secondary.processStartupCommand(opts);</span><br><span class="line">        terminate(ret);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// mode 2</span></span><br><span class="line">        secondary.startInfoServer();</span><br><span class="line">        secondary.startCheckpointThread();</span><br><span class="line">        secondary.join();</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<p>我们主要讲解第二种情况：</p>
<p>其中startInfoServer()主要功能是建立一个http server和namenode保持通信，而startCheckpointThread()作用是启动一个SecondaryNameNode线程，我们SecondaryNameNode类本身就是实现了Runnable的，所以可以被当做线程启动。</p>
<p>run方法中调用doWork(); 下面是SecondaryNameNode.doWork()主要内容</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doWork</span><span class="params">()</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// We may have lost our ticket since last checkpoint, log in again, just in case</span></span><br><span class="line">        <span class="keyword">if</span>(UserGroupInformation.isSecurityEnabled())</span><br><span class="line">          UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> monotonicNow = Time.monotonicNow();</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> now = Time.now();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 根据操作次数和checkpoint间隔时间判断是否进行checkpoint</span></span><br><span class="line">        <span class="keyword">if</span> (shouldCheckpointBasedOnCount() ||</span><br><span class="line">            monotonicNow &gt;= lastCheckpointTime + <span class="number">1000</span> * checkpointConf.getPeriod()) &#123;</span><br><span class="line">          <span class="comment">//checkpoint主要流程</span></span><br><span class="line">          doCheckpoint();</span><br><span class="line">          <span class="comment">// 上传完毕之后更新2nn对象上的lastcheckpoint时间</span></span><br><span class="line">          lastCheckpointTime = monotonicNow;</span><br><span class="line">          lastCheckpointWallclockTime = now;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>doCheckpoint方法首先就是调用namenode.rollEditLog()来滚动nn中正在写的edit文件，然后返回一个包含nn的元数据相关信息的CheckpointSignature</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">doCheckpoint</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    checkpointImage.ensureCurrentDirExists();</span><br><span class="line">    NNStorage dstStorage = checkpointImage.getStorage();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Tell the namenode to start logging transactions in a new edit file</span></span><br><span class="line">    <span class="comment">// Returns a token that would be used to upload the merged image.</span></span><br><span class="line">    <span class="comment">//namenode.rollEditLog()</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">//通知nn进行滚动edit文件，向新文件中写日志，并且返回一个CheckpointSignature对象，该对象中包含了curSegmentTxId和mostRecentCheckpointTxId等信息</span></span><br><span class="line">  	<span class="comment">//curSegmentTxId是滚动生成的新文件的起始txid</span></span><br><span class="line">    <span class="comment">//mostRecentCheckpointTxId是nn上的fsimage最后的txid</span></span><br><span class="line">    CheckpointSignature sig = namenode.rollEditLog();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> loadImage = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">boolean</span> isFreshCheckpointer = (checkpointImage.getNamespaceID() == <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">boolean</span> isSameCluster =</span><br><span class="line">        (dstStorage.versionSupportsFederation(NameNodeLayoutVersion.FEATURES)</span><br><span class="line">            &amp;&amp; sig.isSameCluster(checkpointImage)) ||</span><br><span class="line">        (!dstStorage.versionSupportsFederation(NameNodeLayoutVersion.FEATURES)</span><br><span class="line">            &amp;&amp; sig.namespaceIdMatches(checkpointImage));</span><br><span class="line">    <span class="keyword">if</span> (isFreshCheckpointer ||</span><br><span class="line">        (isSameCluster &amp;&amp;</span><br><span class="line">         !sig.storageVersionMatches(checkpointImage.getStorage()))) &#123;</span><br><span class="line">      <span class="comment">// if we&#x27;re a fresh 2NN, or if we&#x27;re on the same cluster and our storage</span></span><br><span class="line">      <span class="comment">// needs an upgrade, just take the storage info from the server.</span></span><br><span class="line">      <span class="comment">// 如果我们是第一次启动2nn，或者我们的2nn节点 needs an upgrade</span></span><br><span class="line">      dstStorage.setStorageInfo(sig);</span><br><span class="line">      dstStorage.setClusterID(sig.getClusterID());</span><br><span class="line">      dstStorage.setBlockPoolID(sig.getBlockpoolID());</span><br><span class="line">      loadImage = <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    sig.validateStorageInfo(checkpointImage);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// error simulation code for junit test</span></span><br><span class="line">    CheckpointFaultInjector.getInstance().afterSecondaryCallsRollEditLog();</span><br><span class="line">		<span class="comment">// 获取nn上的元数据目录信息</span></span><br><span class="line">    RemoteEditLogManifest manifest =</span><br><span class="line">      namenode.getEditLogManifest(sig.mostRecentCheckpointTxId + <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Fetch fsimage and edits. Reload the image if previous merge failed.</span></span><br><span class="line">    <span class="comment">// 下载fsimage文件和edit文件，这个downloadCheckpointFiles方法中会根据txid进行判断文件顺序是否正确，还有是否需要从nn的fsimage上拉取文件等操作，下面有展开说明</span></span><br><span class="line">    loadImage |= downloadCheckpointFiles(</span><br><span class="line">        fsName, checkpointImage, sig, manifest) |</span><br><span class="line">        checkpointImage.hasMergeError();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将fsimage和edit文件进行合并 如果中间缺少哪个文件可以</span></span><br><span class="line">      <span class="comment">// 通过reloadFromImageFile()方法单独从nn上下载</span></span><br><span class="line"></span><br><span class="line">      doMerge(sig, manifest, loadImage, checkpointImage, namesystem);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IOException ioe) &#123;</span><br><span class="line">      <span class="comment">// A merge error occurred. The in-memory file system state may be</span></span><br><span class="line">      <span class="comment">// inconsistent, so the image and edits need to be reloaded.</span></span><br><span class="line">      checkpointImage.setMergeError();</span><br><span class="line">      <span class="keyword">throw</span> ioe;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Clear any error since merge was successful.</span></span><br><span class="line">    checkpointImage.clearMergeError();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Upload the new image into the NameNode. Then tell the Namenode</span></span><br><span class="line">    <span class="comment">// to make this new uploaded image as the most current image.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获得2nn上最新的txid 然后上传最新的fsimage给nn</span></span><br><span class="line">    <span class="keyword">long</span> txid = checkpointImage.getLastAppliedTxId();</span><br><span class="line">    TransferFsImage.uploadImageFromStorage(fsName, conf, dstStorage,</span><br><span class="line">        NameNodeFile.IMAGE, txid);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// error simulation code for junit test</span></span><br><span class="line">    CheckpointFaultInjector.getInstance().afterSecondaryUploadsNewImage();</span><br><span class="line"></span><br><span class="line">    LOG.warn(<span class="string">&quot;Checkpoint done. New Image Size: &quot;</span></span><br><span class="line">             + dstStorage.getFsImageName(txid).length());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (legacyOivImageDir != <span class="keyword">null</span> &amp;&amp; !legacyOivImageDir.isEmpty()) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        checkpointImage.saveLegacyOIVImage(namesystem, legacyOivImageDir,</span><br><span class="line">            <span class="keyword">new</span> Canceler());</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">        LOG.warn(<span class="string">&quot;Failed to write legacy OIV image: &quot;</span>, e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> loadImage;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>namenode.rollEditLog()需要注意namenode是interface，调用的实际上是NameNodeRpcServer.rollEditLog()方法和nn交互，滚动edit文件，下面我们来看一下NameNodeRpcServer.rollEditLog()方法主要作用:</p>
<p>1.检查namenode 是否是安全模式 HA有没有开启等 处于安全模式或者HA开启则抛出异常<br>2.getFSImage().rollEditLog()调用FSImage.rollEditLog()滚动生成一个新的edit文件 返回一个CheckpointSignature对象 其中包含了curSegmentTxId(当前nn正在写的文件的txid) 等等一系列元数据相关信息</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// namenode.rollEditLog()-&gt;NameNodeRpcServer.rollEditLog()-&gt;FSNamesystem.rollEditLog():</span></span><br><span class="line"><span class="function">CheckpointSignature <span class="title">rollEditLog</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  String operationName = <span class="string">&quot;rollEditLog&quot;</span>;</span><br><span class="line">  CheckpointSignature result = <span class="keyword">null</span>;</span><br><span class="line">  checkSuperuserPrivilege(operationName);</span><br><span class="line">  checkOperation(OperationCategory.JOURNAL);</span><br><span class="line">  writeLock();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    checkOperation(OperationCategory.JOURNAL);</span><br><span class="line">    <span class="comment">// 检查namenode 是否是安全模式 HA有没有开启等</span></span><br><span class="line">    checkNameNodeSafeMode(<span class="string">&quot;Log not rolled&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span> (Server.isRpcInvocation()) &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;Roll Edit Log from &quot;</span> + Server.getRemoteAddress());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 这个方法会滚动edit日志文件</span></span><br><span class="line">    result = getFSImage().rollEditLog(getEffectiveLayoutVersion());</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    writeUnlock(operationName, getLockReportInfoSupplier(<span class="keyword">null</span>));</span><br><span class="line">  &#125;</span><br><span class="line">  logAuditEvent(<span class="keyword">true</span>, operationName, <span class="keyword">null</span>);</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Fsimage.rollEditLog(): nn 滚动生成一个新的edit文件</span></span><br><span class="line"><span class="function">CheckpointSignature <span class="title">rollEditLog</span><span class="params">(<span class="keyword">int</span> layoutVersion)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    </span><br><span class="line">  	<span class="comment">// nn 滚动生成一个新的edits文件 并且txid+1 赋值给curSegmentTxId</span></span><br><span class="line">    </span><br><span class="line">  	getEditLog().rollEditLog(layoutVersion);</span><br><span class="line">    <span class="comment">// Record this log segment ID in all of the storage directories, so</span></span><br><span class="line">    <span class="comment">// we won&#x27;t miss this log segment on a restart if the edits directories</span></span><br><span class="line">    <span class="comment">// go missing.</span></span><br><span class="line">    storage.writeTransactionIdFileToStorage(getEditLog().getCurSegmentTxId());</span><br><span class="line">    <span class="comment">//Update NameDirSize Metric</span></span><br><span class="line">    getStorage().updateNameDirSize();</span><br><span class="line">  </span><br><span class="line">	  <span class="comment">//创建并返回一个CheckpointSignature 其中包含了curSegmentTxId和mostRecentCheckpointTxId等信息</span></span><br><span class="line">  	<span class="comment">//curSegmentTxId是滚动生成的新文件的起始txid</span></span><br><span class="line">    <span class="comment">//mostRecentCheckpointTxId是nn上的fsimage最后的txid</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> CheckpointSignature(<span class="keyword">this</span>);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// secondary namenode从namenode上下载edits文件和fsimage方法</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">downloadCheckpointFiles</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">final</span> URL nnHostPort,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">final</span> FSImage dstImage,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">final</span> CheckpointSignature sig,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">final</span> RemoteEditLogManifest manifest</span></span></span><br><span class="line"><span class="params"><span class="function">)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Sanity check manifest - these could happen if, eg, someone on the</span></span><br><span class="line">  <span class="comment">// NN side accidentally rmed the storage directories</span></span><br><span class="line">  <span class="keyword">if</span> (manifest.getLogs().isEmpty()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Found no edit logs to download on NN since txid &quot;</span></span><br><span class="line">        + sig.mostRecentCheckpointTxId);</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">// nn中的fsimage中最新的txid+1是我们想开始拉取的id</span></span><br><span class="line">  <span class="keyword">long</span> expectedTxId = sig.mostRecentCheckpointTxId + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 检查nn中的第一个edit文件的开始的txId和我们想拉取的expectedTxId是否相匹配</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (manifest.getLogs().get(<span class="number">0</span>).getStartTxId() != expectedTxId) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Bad edit log manifest (expected txid = &quot;</span> +</span><br><span class="line">        expectedTxId + <span class="string">&quot;: &quot;</span> + manifest);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">      Boolean b = UserGroupInformation.getCurrentUser().doAs(</span><br><span class="line">          <span class="keyword">new</span> PrivilegedExceptionAction&lt;Boolean&gt;() &#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> Boolean <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">          dstImage.getStorage().cTime = sig.cTime;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// 检查nn上fsimage的txid和2nn上fsimage的txid是否相等，相等的话2nn就不用从nn上拉取fsimage文件</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (sig.mostRecentCheckpointTxId ==</span><br><span class="line">              dstImage.getStorage().getMostRecentCheckpointTxId()) &#123;</span><br><span class="line">            LOG.info(<span class="string">&quot;Image has not changed. Will not download image.&quot;</span>);</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LOG.info(<span class="string">&quot;Image has changed. Downloading updated image from NN.&quot;</span>);</span><br><span class="line">            MD5Hash downloadedHash = TransferFsImage.downloadImageToStorage(</span><br><span class="line">                nnHostPort, sig.mostRecentCheckpointTxId,</span><br><span class="line">                dstImage.getStorage(), <span class="keyword">true</span>, <span class="keyword">false</span>);</span><br><span class="line">            dstImage.saveDigestAndRenameCheckpointImage(NameNodeFile.IMAGE,</span><br><span class="line">                sig.mostRecentCheckpointTxId, downloadedHash);</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// get edits file</span></span><br><span class="line">          <span class="comment">// 将所有edits文件下载下来</span></span><br><span class="line">          <span class="keyword">for</span> (RemoteEditLog log : manifest.getLogs()) &#123;</span><br><span class="line">            TransferFsImage.downloadEditsToStorage(</span><br><span class="line">                nnHostPort, log, dstImage.getStorage());</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// true if we haven&#x27;t loaded all the transactions represented by the</span></span><br><span class="line">          <span class="comment">// downloaded fsimage.</span></span><br><span class="line">          <span class="comment">// lastAppliedTxId 这个指标是2nn查看自己最新的txid，包括了fsimage和edit文件中最新的</span></span><br><span class="line">          <span class="comment">// 上面拉取完成之后2nn上最新的txid和nn上的fsimage中最新的id相比较肯定是要大的 如果是小的话就是文件没有拉下来</span></span><br><span class="line">          <span class="keyword">return</span> dstImage.getLastAppliedTxId() &lt; sig.mostRecentCheckpointTxId;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      <span class="keyword">return</span> b.booleanValue();</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="HA开启的情况下HDFS的checkpoint流程"><a href="#HA开启的情况下HDFS的checkpoint流程" class="headerlink" title="HA开启的情况下HDFS的checkpoint流程"></a>HA开启的情况下HDFS的checkpoint流程</h2><p>如果开启了HA，那么我们的NameNode就分为Active和standby两种情况，如果active nn出现问题那么就需要及时的切换为standby模式下的nn，这就需要active和standby的namenode节点元数据信息完全一致，那么是如何保证一致性的呢？</p>
<p>NameNode存储的元数据有两种，一种是由hdfs client提交命令生成的元数据，比如创建文件夹等操作，另一种是datanode的block信息，<strong>而为了保证快速切换，datanode会将自己所有的块信息发送给active namenode 和 standby namenodes</strong>，这样需要同步的信息就只剩下了由HDFS client产生的元数据变化，那么这个如何操作呢？下面我们来分析一下有几种方案。</p>
<p>1.<strong>同步阻塞式</strong></p>
<p>HDFS client同步提交操作给active namenode之后再由其同步给standby namenode，这样确实可以保证两边的元数据完全一致，但是带来的影响也非常明显，如果standby namenode节点出现问题导致同步没有完成一直等待，超时后告诉用户是因为备用节点出现问题，<strong>备用节点</strong>出现问题导致可用性降低这个肯定是我们不能接受的现象，那么另一种方法呢？</p>
<p>2.<strong>异步阻塞式</strong></p>
<p>HDFS client同步提交操作给active namenode再由其同步给standby namenode，然后stand by nn接收到命令之后直接返回一个成功，这样确实不会影响用户体验性，但是如果处理过成功一些问题导致失败，那么就不能保证active namenode和stanyby namenode之间的数据一致性。</p>
<p>一个不能保证一致性，一个不能保证可用性，还有什么办法呢？还真的有，那就是引入一个<strong>中间层</strong>，由这个中间层来进行和nn之间的交互，这个中间层就是JournalNode Cluster，active nn向JournalNode中同步元数据信息，然后standby nn从JournalNode中同步过来元数据即可，这样就算standby nn出现问题也可以恢复后继续从JournalNode中拉取元数据，听起来和直接由active nn向stand by nn同步数据类似，那如果JournalNode出现问题应该怎么办呢？JournalNode是存在奇数个(3,5,7)的，只要active nn成功向 <code>(JournalNode数-1)/2</code>数量的JournalNode写入之后，JournalNode就会返回一个成功信息给active nn。</p>
<blockquote>
<p>参考：<br><a class="link"   href="https://cloud.tencent.com/developer/article/1812781#:~:text=Paxos%20%E5%8D%8F%E8%AE%AE%E6%98%AF%E5%B0%91%E6%95%B0%E5%9C%A8,Chubby%E3%80%81Megastore%20%E4%BB%A5%E5%8F%8ASpanner%20%E7%AD%89%E3%80%82" >Paxos协议<i class="fas fa-external-link-alt"></i></a><br><a class="link"   href="https://zhangli1.gitbooks.io/dummies-for-blockchain/content/di-1-zhang-bi-te-bi-3001-qu-kuai-lian-3001-zhi-neng-he-yue/bi-te-bi/yun-gong-shi/fen-bu-shi-xi-tong-yu-qu-zhong-xin-hua.html" >分布式系统和去中心化区别<i class="fas fa-external-link-alt"></i></a></p>
</blockquote>
<p>采取了Paxos协议的JournalNode Cluster保证了高可用性，如果某一个JournalNode挂掉，那么马上JournalNode集群会选举一个新的<code>leader</code>出来，这个机制在Hadoop中被称为QJM(HA using Quorum Journal Manager)架构，也是当前Hadoop默认的HA架构。</p>
<blockquote>
<p>Hadoop HA QJM 架构</p>
<p>Active NameNode（ANN）：在HDFS集群中，对外提供读写服务的唯一Master节点。ANN将客户端请求过来的写操作通过EditLog写入共享存储系统（即JournalNode Cluster），为Standby NameNode及时同步数据提供支持；</p>
<p>Standby NameNode（SBN）：与ANN相互形成热备，SBN及时从共享存储系统中读取EditLog数据并更新内存，以保证当前状态尽可能与ANN同步。当前在整个HDFS集群中最多一台处于Active状态，最多一台处于Standby状态；</p>
<p>JournalNode Cluster（JNs）：ANN与SBN之间共享Editlog的一致性存储系统，是HDFS NameNode高可用的核心组件。借助JournalNode集群ANN可以尽可能及时同步元数据到SBN。其中ANN采用Push模式将EditLog写入JN，SBN通过Pull模式从JN拉取数据，整个过程中JN不主动进行数据交换；</p>
<p>ZKFailoverController（ZKFC）：ZKFailoverController以独立进程运行，对NameNode主备切换进行控制，正常情况ANN和SBN分别对应各自ZKFC进程。ZKFC主要功能：NameNode健康状况检测；借助Zookeeper实现NameNode自动选主；操作NameNode进行主从切换；</p>
</blockquote>
<p>启动方式不同，2nn是一个单独的main方法去启动，而standbynn是一个随着namenode启动随之被调用的程序，但是其中的很多内部方法都非常类似，比如同样都是调用doCheckpoint()方法开始checkpoint，在之前也是要根据操作次数和checkpoint间隔时间判断是否进行checkpoint，需要注意的是，我们可以配置多个fsimage的目录</p>
<blockquote>
<property>
         <name>dfs.namenode.name.dir</name>
         <value>/data1/data/hadoop/namenode,/data2/data/hadoop/namenode</value>
 </property>
</blockquote>
<p>doCheckpoint 中 img.saveNamespace()-&gt;FSImage.saveFSImageInAllDirs() 最终会将所有fsimage保存在指定目录中</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doCheckpoint</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException, IOException </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> canceler != <span class="keyword">null</span>;</span><br><span class="line">  <span class="keyword">final</span> <span class="keyword">long</span> txid;</span><br><span class="line">  <span class="keyword">final</span> NameNodeFile imageType;</span><br><span class="line">  <span class="comment">// Acquire cpLock to make sure no one is modifying the name system.</span></span><br><span class="line">  <span class="comment">// It does not need the full namesystem write lock, since the only thing</span></span><br><span class="line">  <span class="comment">// that modifies namesystem on standby node is edit log replaying.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 这里获取cplock hdfs有两种锁 一种是fslock会将整个fsSystem锁住，另一种是专门给standby nn 进行ckeckpoint使用的cplock,这个锁不会影响块信息和元数据之间的映射关系，所以不用将整个文件系统锁住，应该只是锁住了active nn中的editlog文件,让他的txid不会再增长。</span></span><br><span class="line">  <span class="comment">// Prevent reading of name system while being modified. The full</span></span><br><span class="line">  <span class="comment">// name system lock will be acquired to further block even the block</span></span><br><span class="line">  <span class="comment">// state updates.</span></span><br><span class="line">  namesystem.cpLockInterruptibly();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">assert</span> namesystem.getEditLog().isOpenForRead() :</span><br><span class="line">      <span class="string">&quot;Standby Checkpointer should only attempt a checkpoint when &quot;</span> +</span><br><span class="line">      <span class="string">&quot;NN is in standby mode, but the edit logs are in an unexpected state&quot;</span>;</span><br><span class="line"></span><br><span class="line">    FSImage img = namesystem.getFSImage();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获得上次进行checkpoint生成的fsimage中最大的txID</span></span><br><span class="line">    <span class="keyword">long</span> prevCheckpointTxId = img.getStorage().getMostRecentCheckpointTxId();</span><br><span class="line">    <span class="comment">// 获得正在写的editlog中最新的txID</span></span><br><span class="line">    <span class="keyword">long</span> thisCheckpointTxId = img.getCorrectLastAppliedOrWrittenTxId();</span><br><span class="line">    <span class="keyword">assert</span> thisCheckpointTxId &gt;= prevCheckpointTxId;</span><br><span class="line">    <span class="keyword">if</span> (thisCheckpointTxId == prevCheckpointTxId) &#123;</span><br><span class="line">      LOG.info(<span class="string">&quot;A checkpoint was triggered but the Standby Node has not &quot;</span> +</span><br><span class="line">          <span class="string">&quot;received any transactions since the last checkpoint at txid &#123;&#125;. &quot;</span> +</span><br><span class="line">          <span class="string">&quot;Skipping...&quot;</span>, thisCheckpointTxId);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (namesystem.isRollingUpgrade()</span><br><span class="line">        &amp;&amp; !namesystem.getFSImage().hasRollbackFSImage()) &#123;</span><br><span class="line">      <span class="comment">// 如果我们是要进行 rolling upgrade 滚动升级，则生成的fsimage叫做“fsimage_rollback”开头</span></span><br><span class="line">      <span class="comment">// 就设置imageType为fsimage_rollback否则就设置为正常的&quot;fsimage&quot;开头</span></span><br><span class="line">      imageType = NameNodeFile.IMAGE_ROLLBACK;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      imageType = NameNodeFile.IMAGE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Save the contents of the FS image to a new image file in each of the current storage directories.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// saveNamespace 这个方法将会在指定的目录生成image文件</span></span><br><span class="line">    img.saveNamespace(namesystem, imageType, canceler);</span><br><span class="line">    txid = img.getStorage().getMostRecentCheckpointTxId();</span><br><span class="line">    <span class="keyword">assert</span> txid == thisCheckpointTxId : <span class="string">&quot;expected to save checkpoint at txid=&quot;</span> +</span><br><span class="line">        thisCheckpointTxId + <span class="string">&quot; but instead saved at txid=&quot;</span> + txid;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Save the legacy OIV image, if the output dir is defined.</span></span><br><span class="line">    String outputDir = checkpointConf.getLegacyOivImageDir();</span><br><span class="line">    <span class="keyword">if</span> (outputDir != <span class="keyword">null</span> &amp;&amp; !outputDir.isEmpty()) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        img.saveLegacyOIVImage(namesystem, outputDir, canceler);</span><br><span class="line">      &#125; <span class="keyword">catch</span> (IOException ioe) &#123;</span><br><span class="line">        LOG.warn(<span class="string">&quot;Exception encountered while saving legacy OIV image; &quot;</span></span><br><span class="line">                + <span class="string">&quot;continuing with other checkpointing steps&quot;</span>, ioe);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    namesystem.cpUnlock();</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Save the contents of the FS image to a new image file in each of the</span></span><br><span class="line"><span class="comment"> * current storage directories.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">saveNamespace</span><span class="params">(FSNamesystem source, NameNodeFile nnf,</span></span></span><br><span class="line"><span class="params"><span class="function">    Canceler canceler)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">assert</span> editLog != <span class="keyword">null</span> : <span class="string">&quot;editLog must be initialized&quot;</span>;</span><br><span class="line">  LOG.info(<span class="string">&quot;Save namespace ...&quot;</span>);</span><br><span class="line">  storage.attemptRestoreRemovedStorage();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> editLogWasOpen = editLog.isSegmentOpen();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (editLogWasOpen) &#123;</span><br><span class="line">    editLog.endCurrentLogSegment(<span class="keyword">true</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 获得最新写入的txID</span></span><br><span class="line">  <span class="keyword">long</span> imageTxId = getCorrectLastAppliedOrWrittenTxId();</span><br><span class="line">  <span class="keyword">if</span> (!addToCheckpointing(imageTxId)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(</span><br><span class="line">        <span class="string">&quot;FS image is being downloaded from another NN at txid &quot;</span> + imageTxId);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// 这个方法会将文件全部合并为fsimage到指定目录中</span></span><br><span class="line">      saveFSImageInAllDirs(source, nnf, imageTxId, canceler);</span><br><span class="line">      <span class="keyword">if</span> (!source.isRollingUpgrade()) &#123;</span><br><span class="line">        updateStorageVersion();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (editLogWasOpen) &#123;</span><br><span class="line">        editLog.startLogSegmentAndWriteHeaderTxn(imageTxId + <span class="number">1</span>,</span><br><span class="line">            source.getEffectiveLayoutVersion());</span><br><span class="line">        <span class="comment">// Take this opportunity to note the current transaction.</span></span><br><span class="line">        <span class="comment">// Even if the namespace save was cancelled, this marker</span></span><br><span class="line">        <span class="comment">// is only used to determine what transaction ID is required</span></span><br><span class="line">        <span class="comment">// for startup. So, it doesn&#x27;t hurt to update it unnecessarily.</span></span><br><span class="line">        storage.writeTransactionIdFileToStorage(imageTxId + <span class="number">1</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    removeFromCheckpointing(imageTxId);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//Update NameDirSize Metric</span></span><br><span class="line">  getStorage().updateNameDirSize();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (exitAfterSave.get()) &#123;</span><br><span class="line">    LOG.error(<span class="string">&quot;NameNode process will exit now... The saved FsImage &quot;</span> +</span><br><span class="line">        nnf + <span class="string">&quot; is potentially corrupted.&quot;</span>);</span><br><span class="line">    ExitUtil.terminate(-<span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@see</span> #saveFSImageInAllDirs(FSNamesystem, NameNodeFile, long, Canceler)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">saveFSImageInAllDirs</span><span class="params">(FSNamesystem source, <span class="keyword">long</span> txid)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!addToCheckpointing(txid)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException((<span class="string">&quot;FS image is being downloaded from another NN&quot;</span>));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    saveFSImageInAllDirs(source, NameNodeFile.IMAGE, txid, <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    removeFromCheckpointing(txid);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <tags>
        <tag>Hadoop 源码</tag>
      </tags>
  </entry>
  <entry>
    <title>多线程编程基础</title>
    <url>/2021/08/26/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<p>本文记载了关于多线程编程的一些基础知识，不断补充</p>
<span id="more"></span>

<h2 id="多线程编程基础"><a href="#多线程编程基础" class="headerlink" title="多线程编程基础"></a>多线程编程基础</h2><h3 id="cpu-高速缓存"><a href="#cpu-高速缓存" class="headerlink" title="cpu 高速缓存"></a>cpu 高速缓存</h3><p>为了解决 cpu 和主存之间的速率差，CPU 中的高速缓存运营而生，程序运算时会将数据复制一份到 CPU 的高速缓存中，当 CPU 计算时直接从高速缓存中取数据，然后计算完成之后将数据写回高速缓存中，隔一段时间刷新一次高速缓存中内容到主存</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">val i = 0</span><br><span class="line">i = i + 1</span><br></pre></td></tr></table></figure>

<p>　当线程执行这个语句时，会先从主存当中读取 i 的值，然后复制一份到高速缓存当中，然后 CPU 执行指令对 i 进行加 1 操作，然后将数据写入高速缓存，最后将高速缓存中 i 最新的值刷新到主存当中。</p>
<p> 但是多线程执行时候会带来一个问题叫做“缓存一致性问题”，什么叫做缓存一致性问题呢？</p>
<p> 首先我们上面提高了 CPU 中是有高速缓存这个概念，那么这里的缓存一致性中的缓存是否就是指的 CPU 中的缓存呢？答案是：是的，当多线程执行上面的 i=i+1 语句的时候，我们期待的结果是 n 个线程执行那么 i 就应该是原先的值 +n，但是事实真是如此吗？</p>
<h3 id="缓存一致性"><a href="#缓存一致性" class="headerlink" title="缓存一致性"></a>缓存一致性</h3><p>多个线程同时将变量 i 拷贝到 CPU 中，这时候他们拷贝的变量都是 i 的初始值 0 ，线程 A 经过计算得到 i=1，然后将 i 重新写回高速缓存中，高速缓存再将 i 重新刷新回主存中，这时候的 i 应该是 2，同理线程 B 进行了上述操作，你会发现，这时候的 i 依旧是 1，不是我们想象的 2，通常称这种被多个线程同时访问的变量叫做共享变量。</p>
<p>缓存一致性导致原因：一个变量在多个 CPU 中同时存在缓存，就有可能导致程序的修改没有起作用或者被覆盖</p>
<p> 原因找到了，我们来针对原因想象一下可能会有那些方法呢？首先因为线程 A 和线程 B 都会去将 i 值 copy 进自己的高速缓存，线程 A 修改了数值，但是线程 B 不知道，后面重复写入的时候导致出现问题，有人可能就会说应该线程 A 访问这个变量的时候线程 B 不能访问，这样就保证了线程 A 的修改一定是生效的。没错，这种就是加锁的思想，但是这种思想的缺点也很明显，线程 A 访问变量加锁，线程 B 等等线程都会被卡住，只能等待 A 释放共享变量之后才能访问，效率十分低下。</p>
<h3 id="如何解决缓存一致性问题"><a href="#如何解决缓存一致性问题" class="headerlink" title="如何解决缓存一致性问题"></a>如何解决缓存一致性问题</h3><p> 我们重新回头看一下缓存一致性的原因，如果不能在线程访问变量的时候加锁，那我们能不能在后面的操作添加以下限制呢？这时候就有人提出来了一种叫做缓存一致性协议：如果线程 A 修改了共享变量的数值那么就会通知其他线程他们缓存的共享变量是无效的，这时候如果其他线程想要使用共享变量的数值就必须去通知线程 A 将修改后的数值重写会主存，并且从主存中重新读取共享变量的值</p>
<p>上面就是解决缓存一致性的两种常见的方法：（1）加锁（2）缓存一致性协议</p>
<p>加锁的缺点我们上面已经说过了，那么缓存一致性的缺点是什么呢？乍一看仿佛方法很完美，但是不要忘掉了 CPU 是一个高速不断运行的环境，缓存一致性协议需要做的事情很多，例如线程 A 去通知所有其它使用了该共享变量的线程 你们的这个变量的缓存无效 如果用的话必须说一声，我给你们更新下再用，然后各自线程将该缓存变量的状态更改为无效并且返回”晓得了”指令，CPU 都会等待所有的缓存响应完成，详细内容贴在后面的参考文章</p>
<h3 id="并发编程中常见的三个概念"><a href="#并发编程中常见的三个概念" class="headerlink" title="并发编程中常见的三个概念"></a>并发编程中常见的三个概念</h3><h4 id="原子性"><a href="#原子性" class="headerlink" title="原子性"></a>原子性</h4><p>首先第一个原子性，没错就是我们常说的原子性，一个操作要不就不执行，要不就全部执行，不能被打断，最经典的就是银行转账：</p>
<p>账户 A 向账户 B 转账 1000 元，账户 A 中扣钱和账户 B 中数额增长必须是一起生效</p>
<p>那么我们程序中有哪些时候会用到原子性这个概念呢？举个例子就是 赋值操作时候</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">i = 0</span><br></pre></td></tr></table></figure>

<p>在 64 位机器 上我们假设先给前 32 位赋值，然后再给后 32 位赋值，如果进行到一半，一个线程读取了 i 的值，那么他读取的肯定就会出问题，这个就是原子性的意义。</p>
<h4 id="可见性"><a href="#可见性" class="headerlink" title="可见性"></a>可见性</h4><p>用我们上面的例子举例就是线程 A 对共享变量做出改变之后，其他线程可以立即知晓</p>
<h4 id="有序性"><a href="#有序性" class="headerlink" title="有序性"></a>有序性</h4><p>什么叫做有序性呢？你思考过这样一个问题没有，就是我们写下来的一行行的代码真的是按照顺序执行下来的吗？如果你是写编译器的人，你考虑到有时候有一些语句没有前后的依赖关系，而你又很想提高程序的运行效率，那应该怎么办呢？没错就是改变程序的运行顺序，至于为什么改变执行顺序就能提高效率这个另说，和底层指令集息息相关，反正他们之间没有相互依赖关系，这样最终的结果不会变，并且我们的执行效率变高了，但是如果在多线程情况下呢？</p>
<p>我们看下面的代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 线程 1:</span><br><span class="line">var app = new Application();   // 语句 1</span><br><span class="line">if_init = true;             // 语句 2</span><br><span class="line"> </span><br><span class="line">// 线程 2:</span><br><span class="line">while(!if_init)&#123;</span><br><span class="line">  sleep()</span><br><span class="line">&#125;</span><br><span class="line">doSomethingwithconfig(app);</span><br></pre></td></tr></table></figure>

<p>上面的代码意思是，创建 app 对象，然后将 if_init 重置成 true，表明现在可以使用 app 了，但是编译器认为的是语句 1 和语句 2 之间没有任何关系，那么他就会去将两个语句重排序，如果语句 2 先执行，语句 1 还没有执行，这时候线程 2 执行 while()代码，发现 if_inie 为 true-&gt; 跳出循环，下面的函数使用到了 app 这个变量，极有可能导致程序出错</p>
<h3 id="JVM-对多线程的-native-操作"><a href="#JVM-对多线程的-native-操作" class="headerlink" title="JVM 对多线程的 native 操作"></a>JVM 对多线程的 native 操作</h3><p> 首先我们需要知道的是 JVM 没有对程序使用 CPU 中的高速缓存器进行限制，那么他就会出现我们上面提到过的问题，比如高速缓存和内存之间更新不及时导致的缓存一致性问题，以及指令重排导致的程序执行和预期想象的不一样。</p>
<p> Java 内存模型规定所有的变量都是存在主存当中（类似于前面说的物理内存），每个线程都有自己的工作内存（类似于前面的高速缓存）。线程对变量的所有操作都必须在工作内存中进行，而不能直接对主存进行操作。并且每个线程不能访问其他线程的工作内存。</p>
<h4 id="JVM-原子性"><a href="#JVM-原子性" class="headerlink" title="JVM 原子性"></a>JVM 原子性</h4><p> java 规定给一个变量赋值以及读取是原子性操作，这些操作是不可中断的，也就是我们上面说过的比如给 64 位数赋值，前 32 位和后 32 位要一起赋值，如果没有成功那么前后 32 位都没有成功，撤销操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 下面这些语句哪些是原子性呢？</span><br><span class="line">x = 10;         // 语句 1</span><br><span class="line">y = x;         // 语句 2</span><br><span class="line">x++;           // 语句 3</span><br><span class="line">x = x + 1;     // 语句 4</span><br></pre></td></tr></table></figure>

<p>语句 1 是原子性</p>
<p>语句 2 不是：先读取 x 的值，再去给 y 赋值</p>
<p>语句 3 不是：先读取 x 的值再加 1，再赋值给 x</p>
<p>语句 4：先读取 x 的值再加 1，再重新写回</p>
<p>所以上面只有第一个操作是原子性的，剩下三个其实都是原子性操作的组合</p>
<h4 id="JVM-可见性如何"><a href="#JVM-可见性如何" class="headerlink" title="JVM 可见性如何"></a>JVM 可见性如何</h4><p>JAVA 通过关键字 volatile 关键字来实现可见性，被 volatile 关键字修饰的共享变量将会在被修改后立刻更新到主存中，并且其他线程如果想读取这个被修改过的共享变量就必须去主存中重新加载一遍。</p>
<p> 另外我们上面提到过的 synchronized 关键字和 Lock 关键字都可以保证可见性，但是这种可见性是表现出来的可见性，这两个关键字限制了同时只能有一个线程去访问，修改完成之后将变量重写回主存中之后才会解锁</p>
<h4 id="JVM-有序性如何"><a href="#JVM-有序性如何" class="headerlink" title="JVM 有序性如何"></a>JVM 有序性如何</h4><p> 我们提到过 JVM 是允许指令重排的，这个意思就是允许编译器对指令执行顺序进行重新排序以获得更好的性能，但是会带来的问题就是会影响程序的执行效果，那么 JVM 如何来保证多线程中的有序性呢？主要通过三个关键字 volatile 以及 synchronized 和 Lock，后两个很容易理解，因为规定了只能由一个线程访问所以相当于将多线程的问题直接转变成在单线程的情况下运行，自然就不会出现那样的问题，那么第一个关键字主要是通过什么呢？这个我们后面再说。其中 JVM 还规定了一个排序原则：happens-before 原则，这个是 JVM 推断语句变换顺序的时候会不会最终效果一样的准则，如果两个操作不能通过 happens-before 来进行推导那么就认为两个语句是可以进行重排序的。</p>
<p> 注意 happens-before 原则适用于推导语句的前后顺序关系，如果语句不能从 happens-beofre 中推导出来，那么就认为这两个语句是无序的，不要搞混了</p>
<p>下面就来具体介绍下 happens-before 原则（先行发生原则）：</p>
<ul>
<li>程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作</li>
<li>锁定规则：一个 unLock 操作先行发生于后面对同一个锁额 lock 操作</li>
<li>volatile 变量规则：对一个变量的写操作先行发生于后面对这个变量的读操作</li>
<li>传递规则：如果操作 A 先行发生于操作 B，而操作 B 又先行发生于操作 C，则可以得出操作 A 先行发生于操作 C</li>
<li>线程启动规则：Thread 对象的 start()方法先行发生于此线程的每个一个动作</li>
<li>线程中断规则：对线程 interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生</li>
<li>线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过 Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行</li>
<li>对象终结规则：一个对象的初始化完成先行发生于他的 finalize()方法的开始</li>
</ul>
<p>关于 happens-before 解析详看这篇文章：<a class="link"   href="https://segmentfault.com/a/1190000011458941" >https://segmentfault.com/a/1190000011458941<i class="fas fa-external-link-alt"></i></a></p>
<p>后面有时间了我也会详细的看一看，然后总结一下，哭 好忙~</p>
<h3 id="讲解关于-Volatile-关键字"><a href="#讲解关于-Volatile-关键字" class="headerlink" title="讲解关于 Volatile 关键字"></a>讲解关于 Volatile 关键字</h3><p> 还是针对上面三个多线程中讨论的地方：可见性，原子性，有序性，我们来讨论一下 Volatile 关键字如何操作这三方面的</p>
<h4 id="Volatile-可见性？"><a href="#Volatile-可见性？" class="headerlink" title="Volatile 可见性？"></a>Volatile 可见性？</h4><p> Volatile 可以保证可见性嘛？答案是肯定的，被 Volatile 修饰过的变量在被线程 A 修改过后必须马上写回到主存中，所以效果上来看，Volatile 这个关键字是保证了变量的可见性的，这个操作和我们前面提到的缓存一致性协议有着千丝万缕的关系，可以说 Volatile 底层就是缓存一致性协议的实现，具体可以看这篇文章(好文真多)：</p>
<p><a class="link"   href="https://blog.csdn.net/mashaokang1314/article/details/96571818" >https://blog.csdn.net/mashaokang1314/article/details/96571818<i class="fas fa-external-link-alt"></i></a></p>
<p>Volatile 实现内存可见性是通过 store 和 load 指令完成的；也就是对 volatile 变量执行写操作时，会在写操作后加入一条 store 指令，即强迫线程将最新的值刷新到主内存中；而在读操作时，会加入一条 load 指令，即强迫从主内存中读入变量的值。写操作之后紧跟着写回主存的操作，读必须从主存中读取，这个就是 volatile 保证可见性的方法。</p>
<h4 id="Volatile-原子性？"><a href="#Volatile-原子性？" class="headerlink" title="Volatile 原子性？"></a>Volatile 原子性？</h4><p> 上面我们知道了 Volatile 是具有可见性的，那我们来查看一下下面的代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 十个线程各自循环 1000 次对一个 volatile 修饰的共享变量来进行自增</span><br><span class="line">public class Test &#123;</span><br><span class="line">    public volatile int inc = 0;</span><br><span class="line">     </span><br><span class="line">    public void increase() &#123;</span><br><span class="line">        inc++;</span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        final Test test = new Test();</span><br><span class="line">        for(int i=0;i&lt;10;i++)&#123;</span><br><span class="line">            new Thread()&#123;</span><br><span class="line">                public void run() &#123;</span><br><span class="line">                    for(int j=0;j&lt;1000;j++)</span><br><span class="line">                        test.increase();</span><br><span class="line">                &#125;;</span><br><span class="line">            &#125;.start();</span><br><span class="line">        &#125;</span><br><span class="line">         </span><br><span class="line">        while(Thread.activeCount()&gt;1)  // 保证前面的线程都执行完</span><br><span class="line">            Thread.yield();</span><br><span class="line">        System.out.println(test.inc);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p> 上面的代码我们想当然的会觉得，既然已经被 Volatile 修饰了，那其中一个线程修改完成写回之后其他线程马上知道了，并且重新从主存中读取，那么最终的结果应该是 10*1000 = 10000，但是实验发现结果不是这样的，我们发现每次每次输出的都要比 10000 小，这是因为什么呢？</p>
<p> 首先我们应该考虑一下我们给 test 这个共享变量 +1 的操作都有哪些，我们提到过自增变量是不具备原子性的，他需要先取回自己变量的值，+1，写回，这是三个原子性操作，而 volatile 保证的是如果这个共享变量被修改了，那么所有的线程中这个编程无效，重新读取，如果线程 A 只是读取了共享变量 test，然后线程 A 去忙别的了，这时候线程 B 又读取了共享变量 test 进行 +1 操作，然后将变量写回，这时候线程 A 已经读取了变量 test，紧接着他也进行 +1 然后写回，那么就是两个程序执行完毕之后 test 仅仅加了一次，这时候如果有疑惑建议回顾一下上面 volatile 如何保证可见性。</p>
<p> 那如何保证原子性呢？使用 synchronized，Lock，AtomicInteger，操作都可以具体不展开了，因为我还需要查~</p>
<h4 id="Volatile-顺序性"><a href="#Volatile-顺序性" class="headerlink" title="Volatile 顺序性"></a>Volatile 顺序性</h4><p> Volatile 可以保证程序的部分顺序执行，保证“部分”顺序执行是什么意思呢？下面举一个例子：</p>
<p><a class="link"   href="https://www.cnblogs.com/dolphin0520/p/3920373.html" >参考链接<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//x、y 为非 volatile 变量</span><br><span class="line">//flag 为 volatile 变量</span><br><span class="line"> </span><br><span class="line">x = 2;        // 语句 1</span><br><span class="line">y = 0;        // 语句 2</span><br><span class="line">flag = true;  // 语句 3</span><br><span class="line">x = 4;         // 语句 4</span><br><span class="line">y = -1;       // 语句 5</span><br></pre></td></tr></table></figure>

<p>flag 是被 volatile 修饰的变量，那他会带来什么效果呢？</p>
<p>（1）flag 之前的代码全部执行了，flag 后面的代码还没有开始执行</p>
<p>（2）语句 1 和语句 2 的顺序不能确定，但是语句 1，2 肯定不能放在 flag 后面执行，同理语句 3，4 的顺序不能确定，但是不能放在 flag 之前执行</p>
<h3 id="什么时候使用-Volatile？"><a href="#什么时候使用-Volatile？" class="headerlink" title="什么时候使用 Volatile？"></a>什么时候使用 Volatile？</h3><p> 我们知道多线程经常用的关键字就是 synchronized 以及 volatile，那么他们各自都有什么作用呢？synchronized 是防止多个线程同时执行一段代码，相当于给这段代码加锁了，不断线程过来请求访问这段代码，如果加锁了就只能等，所有 Volatile 在效率上是优于 synchronized 的，但是 Volatile 是不能保证操作的原子性的，所以有时候必须结合 synchronized 一起来用，那什么时候可以只用 Volatile 这个关键字呢？</p>
<p>（1）对变量的写操作不依赖当前的变量值</p>
<p>（2）该变量没有包含在具有其他变量的不变式中</p>
<p>上面两个条件就是针对 Volatile 不能保证原子性，所以就不能随便改变共享变量的数值，哪怕是被 Volatile 修饰的，下面举例什么时候使用 Volatile</p>
<h4 id="更改状态标记量："><a href="#更改状态标记量：" class="headerlink" title="更改状态标记量："></a>更改状态标记量：</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">volatile boolean flag = false;</span><br><span class="line"> </span><br><span class="line">while(!flag)&#123;</span><br><span class="line">    doSomething();</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">public void setFlag() &#123;</span><br><span class="line">    flag = true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面这段代码中的 setFlag 方法将 flag 置为 true，哪怕出现了原子性问题，线程 A 读取了然后停一会，期间线程 B 读取了重写会主存中为 true，这时候线程 A 再写一遍 true 也是无所谓的</p>
<h4 id="double-check-volatile"><a href="#double-check-volatile" class="headerlink" title="double check+volatile:"></a>double check+volatile:</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Singleton&#123;</span><br><span class="line">    private volatile static Singleton instance = null;</span><br><span class="line">     </span><br><span class="line">    private Singleton() &#123;</span><br><span class="line">         </span><br><span class="line">    &#125;</span><br><span class="line">     </span><br><span class="line">    public static Singleton getInstance() &#123;</span><br><span class="line">        if(instance==null) &#123;</span><br><span class="line">            synchronized (Singleton.class) &#123;</span><br><span class="line">                if(instance==null)</span><br><span class="line">                    instance = new Singleton();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面是为了实现单例模式使用了 double check，因为 synchronized 如果直接修饰整个 getInstance 方法会导致效率变低，我们只需要初始化的时候进行加锁即可，就是下面的代码：</p>
<figure class="highlight plaintext"><figcaption><span>a</span></figcaption><table><tr><td class="code"><pre><span class="line">public static Singleton getInstance() &#123;</span><br><span class="line">        if(instance==null) &#123;</span><br><span class="line">            synchronized (Singleton.class) &#123;</span><br><span class="line">                if(instance==null)</span><br><span class="line">                    instance = new Singleton();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    return uniqueSingleton;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="双重检查锁（double-checked-locking）"><a href="#双重检查锁（double-checked-locking）" class="headerlink" title="双重检查锁（double checked locking）"></a>双重检查锁（double checked locking）</h5><p>先判断对象是否已经被初始化，再决定要不要加锁，双重检查流程如下：</p>
<ol>
<li>检查变量是否被初始化(不去获得锁)，如果已被初始化则立即返回。</li>
<li>获取锁。</li>
<li>再次检查变量是否已经被初始化，如果还没被初始化就初始化一个对象。</li>
</ol>
<p>执行双重检查是因为，如果多个线程同时了通过了第一次检查，并且其中一个线程首先通过了第二次检查并实例化了对象，那么剩余通过了第一次检查的线程就不会再去实例化对象。</p>
<p>这样，除了初始化的时候会出现加锁的情况，后续的所有调用都会避免加锁而直接返回，解决了性能消耗的问题。否则每一次调用 getInstance()方法都会加锁，性能低下。</p>
<h5 id="隐患"><a href="#隐患" class="headerlink" title="隐患"></a>隐患</h5><p>上述写法看似解决了问题，但是有个很大的隐患。实例化对象的那行代码（标记为 error 的那行），实际上可以分解成以下三个步骤：</p>
<ol>
<li>分配内存空间</li>
<li>初始化对象</li>
<li>将对象指向刚分配的内存空间</li>
</ol>
<p>但是有些编译器为了性能的原因，可能会将第二步和第三步进行 <strong>重排序</strong>，顺序就成了：</p>
<ol>
<li>分配内存空间</li>
<li>将对象指向刚分配的内存空间</li>
<li>初始化对象</li>
</ol>
<p>现在考虑重排序后，两个线程发生了以下调用：</p>
<table>
<thead>
<tr>
<th align="left">Time</th>
<th align="left">Thread A</th>
<th align="left">Thread B</th>
</tr>
</thead>
<tbody><tr>
<td align="left">T1</td>
<td align="left">检查到 Singleton 为空</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">T2</td>
<td align="left">获取锁</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">T3</td>
<td align="left">再次检查到 Singleton 为空</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">T4</td>
<td align="left">为 Singleton 分配内存空间</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">T5</td>
<td align="left">将 Singleton 指向内存空间</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">T6</td>
<td align="left"></td>
<td align="left">检查到 Singleton 不为空</td>
</tr>
<tr>
<td align="left">T7</td>
<td align="left"></td>
<td align="left">访问 Singleton（此时对象还未完成初始化）</td>
</tr>
<tr>
<td align="left">T8</td>
<td align="left">初始化 Singleton</td>
<td align="left"></td>
</tr>
</tbody></table>
<p>在这种情况下，T7 时刻线程 B 对 Singleton 的访问，访问的是一个 <strong>初始化未完成</strong> 的对象。</p>
<p>为了解决上述问题，需要在 Singleton 前加入关键字<code>volatile</code>。使用了 volatile 关键字后，重排序被禁止，所有的写（write）操作都将发生在读（read）操作之前。</p>
<p>至此，双重检查锁就可以完美工作了。</p>
<p>参考：</p>
<p><a class="link"   href="https://www.cnblogs.com/dolphin0520/p/3920373.html" >https://www.cnblogs.com/dolphin0520/p/3920373.html<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://cloud.tencent.com/developer/article/1548942" >https://cloud.tencent.com/developer/article/1548942<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   href="https://www.cnblogs.com/xz816111/p/8470048.html" >https://www.cnblogs.com/xz816111/p/8470048.html<i class="fas fa-external-link-alt"></i></a></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
  </entry>
</search>
